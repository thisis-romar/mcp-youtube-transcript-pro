WEBVTT

00:00:48.560 --> 00:00:52.960
Today I'm speaking with Andrej Karpathy.
Andrej, why do you say that this will be

00:00:52.960 --> 00:00:58.480
the decade of agents and not the year of agents?
First of all, thank you for having me here. I'm

00:00:58.479 --> 00:01:03.007
excited to be here. The quote you've just 
mentioned, "It's the decade of agents,"

00:01:03.008 --> 00:01:09.519
is actually a reaction to a pre-existing quote.
I'm not actually sure who said this but they were

00:01:09.519 --> 00:01:14.719
alluding to this being the year of agents with 
respect to LLMs and how they were going to evolve.

00:01:16.239 --> 00:01:20.479
I was triggered by that because there's some 
over-prediction going on in the industry.

00:01:21.280 --> 00:01:25.519
In my mind, this is more accurately 
described as the decade of agents.

00:01:25.519 --> 00:01:28.479
We have some very early agents that 
are extremely impressive and that I

00:01:28.480 --> 00:01:34.240
use daily—Claude and Codex and so on—but I 
still feel there's so much work to be done.

00:01:35.439 --> 00:01:38.239
My reaction is we'll be working 
with these things for a decade.

00:01:38.239 --> 00:01:41.439
They're going to get better, 
and it's going to be wonderful.

00:01:42.000 --> 00:01:46.879
I was just reacting to the 
timelines of the implication.

00:01:46.879 --> 00:01:50.719
What do you think will take a decade to 
accomplish? What are the bottlenecks?

00:01:51.840 --> 00:01:56.240
Actually making it work. When you're talking 
about an agent, or what the labs have in mind

00:01:56.239 --> 00:01:59.920
and maybe what I have in mind as well, you 
should think of it almost like an employee or

00:01:59.920 --> 00:02:04.079
an intern that you would hire to work with you.
For example, you work with some employees here.

00:02:04.719 --> 00:02:08.719
When would you prefer to have an agent like 
Claude or Codex do that work? Currently,

00:02:08.719 --> 00:02:11.759
of course they can't. What would it 
take for them to be able to do that?

00:02:11.759 --> 00:02:13.840
Why don't you do it today?
The reason you don't do it

00:02:13.840 --> 00:02:17.360
today is because they just don't work.
They don't have enough intelligence,

00:02:17.360 --> 00:02:20.880
they're not multimodal enough, they 
can't do computer use and all this stuff.

00:02:20.879 --> 00:02:24.159
They don't do a lot of the things you've 
alluded to earlier. They don't have

00:02:24.159 --> 00:02:27.120
continual learning. You can't just tell 
them something and they'll remember it.

00:02:27.120 --> 00:02:29.840
They're cognitively lacking 
and it's just not working.

00:02:30.560 --> 00:02:32.800
It will take about a decade to 
work through all of those issues.

00:02:32.800 --> 00:02:41.200
Interesting. As a professional podcaster 
and a viewer of AI from afar, it's easy

00:02:42.080 --> 00:02:47.120
for me to identify what's lacking: continual 
learning is lacking, or multimodality is lacking.

00:02:47.120 --> 00:02:52.719
But I don't really have a good way 
of trying to put a timeline on it.

00:02:52.719 --> 00:02:57.919
If somebody asks how long continual learning 
will take, I have no prior about whether

00:02:57.919 --> 00:03:01.439
this is a project that should take 5 
years, 10 years, or 50 years. Why a

00:03:01.439 --> 00:03:07.680
decade? Why not one year? Why not 50 years?
This is where you get into a bit of my own

00:03:07.680 --> 00:03:14.000
intuition, and doing a bit of an extrapolation 
with respect to my own experience in the field.

00:03:14.639 --> 00:03:19.919
I've been in AI for almost two decades.
It's going to be 15 years or so, not that long.

00:03:19.919 --> 00:03:23.039
You had Richard Sutton here, 
who was around for much longer.

00:03:23.039 --> 00:03:28.000
I do have about 15 years of experience of people 
making predictions, of seeing how they turned out.

00:03:28.000 --> 00:03:30.479
Also I was in the industry for 
a while, I was in research,

00:03:30.479 --> 00:03:32.399
and I've worked in the industry for a while.

00:03:32.400 --> 00:03:36.319
I have a general intuition 
that I have left from that.

00:03:37.759 --> 00:03:43.840
I feel like the problems are tractable, they're 
surmountable, but they're still difficult.

00:03:43.840 --> 00:03:47.039
If I just average it out, it 
just feels like a decade to me.

00:03:47.039 --> 00:03:50.879
This is quite interesting. I want 
to hear not only the history,

00:03:50.879 --> 00:03:58.079
but what people in the room felt was about to 
happen at various different breakthrough moments.

00:03:58.080 --> 00:04:03.040
What were the ways in which their feelings were 
either overly pessimistic or overly optimistic?

00:04:03.599 --> 00:04:07.120
Should we just go through each of them one by one?
That's a giant question because you're talking

00:04:07.120 --> 00:04:10.879
about 15 years of stuff that happened.
AI is so wonderful because there have been

00:04:10.879 --> 00:04:16.240
a number of seismic shifts where the entire 
field has suddenly looked a different way.

00:04:17.040 --> 00:04:21.040
I've maybe lived through two or three of those.
I still think there will continue to be

00:04:21.040 --> 00:04:25.200
some because they come with 
almost surprising regularity.

00:04:25.199 --> 00:04:29.519
When my career began, when I started to work on 
deep learning, when I became interested in deep

00:04:29.519 --> 00:04:34.000
learning, this was by chance of being right next 
to Geoff Hinton at the University of Toronto.

00:04:34.000 --> 00:04:37.040
Geoff Hinton, of course, is 
the godfather figure of AI.

00:04:37.040 --> 00:04:40.160
He was training all these neural networks.
I thought it was incredible and interesting.

00:04:40.160 --> 00:04:43.680
This was not the main thing that 
everyone in AI was doing by far.

00:04:43.680 --> 00:04:49.439
This was a niche little subject on the side.
That's maybe the first dramatic seismic shift

00:04:49.439 --> 00:04:54.000
that came with the AlexNet and so on.
AlexNet reoriented everyone, and everyone

00:04:54.000 --> 00:04:59.600
started to train neural networks, but it 
was still very per-task, per specific task.

00:04:59.600 --> 00:05:04.400
Maybe I have an image classifier or I have a 
neural machine translator or something like that.

00:05:04.399 --> 00:05:10.959
People became very slowly interested in agents.
People started to think, "Okay, maybe we have a

00:05:10.959 --> 00:05:14.239
check mark next to the visual cortex or something 
like that, but what about the other parts of the

00:05:14.240 --> 00:05:19.040
brain, and how can we get a full agent or a 
full entity that can interact in the world?"

00:05:19.600 --> 00:05:26.000
The Atari deep reinforcement learning shift 
in 2013 or so was part of that early effort

00:05:26.000 --> 00:05:30.160
of agents, in my mind, because it was an 
attempt to try to get agents that not just

00:05:30.160 --> 00:05:34.080
perceive the world, but also take actions and 
interact and get rewards from environments.

00:05:34.079 --> 00:05:38.479
At the time, this was Atari games.
I feel that was a misstep.

00:05:39.680 --> 00:05:45.600
It was a misstep that even the early OpenAI that 
I was a part of adopted because at that time,

00:05:45.600 --> 00:05:50.160
the zeitgeist was reinforcement learning 
environments, games, game playing,

00:05:50.160 --> 00:05:54.080
beat games, get lots of different types of 
games, and OpenAI was doing a lot of that.

00:05:54.079 --> 00:06:01.439
That was another prominent part of AI where maybe 
for two or three or four years, everyone was doing

00:06:01.439 --> 00:06:06.800
reinforcement learning on games.
That was all a bit of a misstep.

00:06:06.800 --> 00:06:10.000
What I was trying to do at OpenAI is 
I was always a bit suspicious of games

00:06:10.000 --> 00:06:13.920
as being this thing that would lead to AGI.
Because in my mind, you want something like

00:06:13.920 --> 00:06:17.600
an accountant or something that's 
interacting with the real world.

00:06:17.600 --> 00:06:23.840
I just didn't see how games add up to it.
My project at OpenAI, for example, was within the

00:06:23.839 --> 00:06:30.079
scope of the Universe project, on an agent that 
was using keyboard and mouse to operate web pages.

00:06:30.079 --> 00:06:33.839
I really wanted to have something that 
interacts with the actual digital world

00:06:33.839 --> 00:06:37.279
that can do knowledge work.
It just so turns out that this

00:06:37.279 --> 00:06:41.199
was extremely early, way too early, so early 
that we shouldn't have been working on that.

00:06:42.079 --> 00:06:47.199
Because if you're just stumbling your way around 
and keyboard mashing and mouse clicking and trying

00:06:47.199 --> 00:06:52.079
to get rewards in these environments, your 
reward is too sparse and you just won't learn.

00:06:52.079 --> 00:06:54.879
You're going to burn a forest 
computing, and you're never

00:06:54.879 --> 00:06:58.560
going to get something off the ground.
What you're missing is this power of

00:06:58.560 --> 00:07:02.319
representation in the neural network.
For example, today people are training

00:07:02.319 --> 00:07:05.759
those computer-using agents, but they're 
doing it on top of a large language model.

00:07:05.759 --> 00:07:08.719
You have to get the language model first, 
you have to get the representations first,

00:07:08.720 --> 00:07:11.760
and you have to do that by all the 
pre-training and all the LLM stuff.

00:07:11.759 --> 00:07:17.920
I feel maybe loosely speaking, people 
kept trying to get the full thing too

00:07:17.920 --> 00:07:22.800
early a few times, where people really try 
to go after agents too early, I would say.

00:07:22.800 --> 00:07:26.240
That was Atari and Universe 
and even my own experience.

00:07:26.240 --> 00:07:29.680
You actually have to do some things 
first before you get to those agents.

00:07:30.560 --> 00:07:36.079
Now the agents are a lot more competent, but maybe 
we're still missing some parts of that stack.

00:07:36.079 --> 00:07:40.240
I would say those are the three major 
buckets of what people were doing:

00:07:40.240 --> 00:07:44.560
training neural nets per-tasks, 
trying the first round of agents,

00:07:44.560 --> 00:07:48.639
and then maybe the LLMs and seeking the 
representation power of the neural networks

00:07:48.639 --> 00:07:53.680
before you tack on everything else on top.
Interesting. If I were to steelman the Sutton

00:07:54.319 --> 00:07:57.839
perspective, it would be that humans 
can just take on everything at once,

00:07:57.839 --> 00:08:01.919
or even animals can take on everything at once.
Animals are maybe a better example because they

00:08:01.920 --> 00:08:05.360
don't even have the scaffold of language.
They just get thrown out into the world,

00:08:05.360 --> 00:08:08.720
and they just have to make sense 
of everything without any labels.

00:08:10.000 --> 00:08:14.319
The vision for AGI then should just be 
something which looks at sensory data,

00:08:14.319 --> 00:08:18.800
looks at the computer screen, and it just 
figures out what's going on from scratch.

00:08:18.800 --> 00:08:22.240
If a human were put in a similar situation and 
had to be trained from scratch… This is like a

00:08:22.240 --> 00:08:26.000
human growing up or an animal growing up.
Why shouldn't that be the vision for AI,

00:08:26.000 --> 00:08:29.920
rather than this thing where we're 
doing millions of years of training?

00:08:29.920 --> 00:08:35.600
That's a really good question. Sutton was 
on your podcast and I saw the podcast and I

00:08:35.600 --> 00:08:40.000
had a write-up about that podcast that 
gets into a bit of how I see things.

00:08:41.600 --> 00:08:46.240
I'm very careful to make analogies to 
animals because they came about by a

00:08:46.240 --> 00:08:49.840
very different optimization process.
Animals are evolved, and they come

00:08:49.840 --> 00:08:55.840
with a huge amount of hardware that's built in.
For example, my example in the post was the zebra.

00:08:55.840 --> 00:08:59.280
A zebra gets born, and a few minutes later 
it's running around and following its mother.

00:08:59.279 --> 00:09:03.919
That's an extremely complicated thing to do. 
That's not reinforcement learning. That's

00:09:03.919 --> 00:09:08.159
something that's baked in. Evolution obviously 
has some way of encoding the weights of our

00:09:08.159 --> 00:09:12.639
neural nets in ATCGs, and I have no idea 
how that works, but it apparently works.

00:09:14.639 --> 00:09:20.399
Brains just came from a very different process, 
and I'm very hesitant to take inspiration from it

00:09:20.399 --> 00:09:25.759
because we're not actually running that process.
In my post, I said we're not building animals.

00:09:25.759 --> 00:09:30.240
We're building ghosts or spirits or 
whatever people want to call it, because

00:09:31.919 --> 00:09:37.519
we're not doing training by evolution.
We're doing training by imitation of humans

00:09:37.519 --> 00:09:42.799
and the data that they've put on the Internet.
You end up with these ethereal spirit entities

00:09:42.799 --> 00:09:45.359
because they're fully digital 
and they're mimicking humans.

00:09:45.360 --> 00:09:48.720
It's a different kind of intelligence.
If you imagine a space of intelligences,

00:09:48.720 --> 00:09:53.279
we're starting off at a different point almost. 
We're not really building animals. But it's also

00:09:53.279 --> 00:09:56.799
possible to make them a bit more animal-like 
over time, and I think we should be doing that.

00:09:58.480 --> 00:10:04.879
One more point. I do feel Sutton has a very...
His framework is, "We want to build animals."

00:10:04.879 --> 00:10:07.679
I think that would be wonderful if we can 
get that to work. That would be amazing.

00:10:07.679 --> 00:10:13.120
If there were a single algorithm that you 
can just run on the Internet and it learns

00:10:13.120 --> 00:10:18.799
everything, that would be incredible.
I'm not sure that it exists and that's

00:10:18.799 --> 00:10:23.439
certainly not what animals do, because 
animals have this outer loop of evolution.

00:10:24.399 --> 00:10:28.399
A lot of what looks like learning is 
more like maturation of the brain.

00:10:28.399 --> 00:10:32.879
I think there's very little 
reinforcement learning for animals.

00:10:32.879 --> 00:10:37.120
A lot of the reinforcement learning is more 
like motor tasks; it's not intelligence tasks.

00:10:37.120 --> 00:10:40.480
So I actually kind of think humans 
don’t really use RL, roughly speaking.

00:10:41.120 --> 00:10:42.960
Can you repeat the last sentence?
A lot of that intelligence is

00:10:42.960 --> 00:10:45.600
not motor task…it's what, sorry?
A lot of the reinforcement learning, in my

00:10:45.600 --> 00:10:51.680
perspective, would be things that are a lot more 
motor-like, simple tasks like throwing a hoop.

00:10:53.440 --> 00:10:57.440
But I don't think that humans use reinforcement 
learning for a lot of intelligence tasks

00:10:57.440 --> 00:11:01.520
like problem-solving and so on.
That doesn't mean we shouldn't

00:11:01.519 --> 00:11:05.919
do that for research, but I just feel 
like that's what animals do or don't.

00:11:05.919 --> 00:11:09.599
I'm going to take a second to digest that 
because there are a lot of different ideas.

00:11:09.600 --> 00:11:14.240
Here’s one clarifying question I can 
ask to understand the perspective.

00:11:15.279 --> 00:11:18.639
You suggest that evolution is doing 
the kind of thing that pre-training

00:11:18.639 --> 00:11:24.000
does in the sense of building something 
which can then understand the world.

00:11:24.000 --> 00:11:28.320
The difference is that evolution 
has to be titrated in the case

00:11:28.320 --> 00:11:37.360
of humans through three gigabytes of DNA.
That's very unlike the weights of a model.

00:11:37.360 --> 00:11:42.800
Literally, the weights of the model are 
a brain, which obviously does not exist

00:11:42.799 --> 00:11:46.559
in the sperm and the egg.
So it has to be grown.

00:11:46.559 --> 00:11:50.639
Also, the information for every single 
synapse in the brain simply cannot exist

00:11:50.639 --> 00:11:55.600
in the three gigabytes that exist in the DNA.
Evolution seems closer to finding the algorithm

00:11:55.600 --> 00:12:00.639
which then does the lifetime learning.
Now, maybe the lifetime learning is

00:12:00.639 --> 00:12:04.639
not analogous to RL, to your point.
Is that compatible with the thing you

00:12:04.639 --> 00:12:06.960
were saying, or would you disagree with that?
I think so. I would agree with you that there's

00:12:06.960 --> 00:12:09.040
some miraculous compression 
going on because obviously,

00:12:09.039 --> 00:12:14.000
the weights of the neural net are not stored 
in ATCGs. There's some dramatic compression.

00:12:14.000 --> 00:12:18.879
There are some learning algorithms encoded that 
take over and do some of the learning online.

00:12:18.879 --> 00:12:23.600
I definitely agree with you on that.
I would say I'm a lot more practically minded.

00:12:23.600 --> 00:12:26.080
I don't come at it from the 
perspective of, let's build animals.

00:12:26.080 --> 00:12:28.960
I come from it from the perspective 
of, let's build useful things.

00:12:28.960 --> 00:12:31.920
I have a hard hat on, and I'm just 
observing that we're not going to do

00:12:31.919 --> 00:12:36.959
evolution, because I don't know how to do that.
But it does turn out we can build these ghosts,

00:12:36.960 --> 00:12:43.120
spirit-like entities, by imitating internet 
documents. This works. It's a way to bring you

00:12:43.120 --> 00:12:48.399
up to something that has a lot of built-in 
knowledge and intelligence in some way,

00:12:48.399 --> 00:12:51.919
similar to maybe what evolution has done.
That's why I call pre-training

00:12:51.919 --> 00:12:56.159
this crappy evolution.
It's the practically possible version

00:12:56.159 --> 00:13:01.199
with our technology and what we have available 
to us to get to a starting point where we can do

00:13:01.200 --> 00:13:05.360
things like reinforcement learning and so on.
Just to steelman the other perspective,

00:13:05.360 --> 00:13:09.200
after doing this Sutton interview and thinking 
about it a bit, he has an important point here.

00:13:09.200 --> 00:13:14.080
Evolution does not give us the knowledge, really.
It gives us the algorithm to find the knowledge,

00:13:14.080 --> 00:13:19.440
and that seems different from pre-training.
Perhaps the perspective is that pre-training helps

00:13:19.440 --> 00:13:23.280
build the kind of entity which can learn better.
It teaches meta-learning, and therefore

00:13:23.279 --> 00:13:28.319
it is similar to finding an algorithm.
But if it's "Evolution gives us knowledge,

00:13:28.320 --> 00:13:31.440
pre-training gives us knowledge," 
that analogy seems to break down.

00:13:31.440 --> 00:13:35.040
It's subtle and I think you're right to 
push back on it, but basically the thing

00:13:35.039 --> 00:13:38.879
that pre-training is doing, you're getting 
the next-token predictor over the internet,

00:13:38.879 --> 00:13:43.679
and you're training that into a neural net.
It's doing two things that are unrelated.

00:13:43.679 --> 00:13:46.079
Number one, it's picking up all 
this knowledge, as I call it.

00:13:46.080 --> 00:13:51.120
Number two, it's actually becoming intelligent.
By observing the algorithmic patterns in the

00:13:51.120 --> 00:13:55.679
internet, it boots up all these little circuits 
and algorithms inside the neural net to do things

00:13:55.679 --> 00:14:00.799
like in-context learning and all this stuff.
You don't need or want the knowledge.

00:14:00.799 --> 00:14:05.279
I think that's probably holding back the neural 
networks overall because it's getting them to rely

00:14:05.279 --> 00:14:09.759
on the knowledge a little too much sometimes.
For example, I feel agents, one thing they're

00:14:09.759 --> 00:14:13.439
not very good at, is going off the data 
manifold of what exists on the internet.

00:14:13.440 --> 00:14:17.360
If they had less knowledge or less 
memory, maybe they would be better.

00:14:17.919 --> 00:14:21.679
What I think we have to do going forward—and 
this would be part of the research paradigms—is

00:14:23.679 --> 00:14:28.719
figure out ways to remove some of the knowledge 
and to keep what I call this cognitive core.

00:14:28.720 --> 00:14:34.080
It's this intelligent entity that is stripped from 
knowledge but contains the algorithms and contains

00:14:34.080 --> 00:14:39.360
the magic of intelligence and problem-solving 
and the strategies of it and all this stuff.

00:14:39.360 --> 00:14:44.080
There's so much interesting stuff there. Let's 
start with in-context learning. This is an

00:14:44.080 --> 00:14:48.800
obvious point, but I think it's worth just 
saying it explicitly and meditating on it.

00:14:48.799 --> 00:14:53.839
The situation in which these models seem the most 
intelligent—in which I talk to them and I'm like,

00:14:53.840 --> 00:14:58.240
"Wow, there's really something on the other end 
that's responding to me thinking about things—is

00:14:58.240 --> 00:15:01.840
if it makes a mistake it's like, "Oh wait, that's 
the wrong way to think about it. I'm backing up."

00:15:01.840 --> 00:15:04.480
All that is happening in context.
That's where I feel like the real

00:15:04.480 --> 00:15:10.560
intelligence is that you can visibly see.
That in-context learning process is

00:15:10.559 --> 00:15:16.319
developed by gradient descent on pre-training.
It spontaneously meta-learns in-context learning,

00:15:16.320 --> 00:15:21.600
but the in-context learning itself is not 
gradient descent, in the same way that our

00:15:21.600 --> 00:15:26.560
lifetime intelligence as humans to be able 
to do things is conditioned by evolution

00:15:26.559 --> 00:15:30.879
but our learning during our lifetime is 
happening through some other process.

00:15:30.879 --> 00:15:34.480
I don't fully agree with that, but 
you should continue your thought.

00:15:34.480 --> 00:15:36.879
Well, I'm very curious to understand 
how that analogy breaks down.

00:15:36.879 --> 00:15:40.639
I'm hesitant to say that in-context 
learning is not doing gradient descent.

00:15:41.759 --> 00:15:47.279
It's not doing explicit gradient descent.
In-context learning is pattern completion

00:15:47.279 --> 00:15:50.319
within a token window.
It just turns out that there's

00:15:50.320 --> 00:15:53.760
a huge amount of patterns on the internet.
You're right, the model learns to complete

00:15:53.759 --> 00:15:58.000
the pattern, and that's inside the weights.
The weights of the neural network are trying

00:15:58.000 --> 00:16:02.080
to discover patterns and complete the pattern.
There's some adaptation that happens inside

00:16:02.080 --> 00:16:06.160
the neural network, which is magical 
and just falls out from the internet

00:16:06.159 --> 00:16:10.559
just because there's a lot of patterns.
I will say that there have been some papers

00:16:10.559 --> 00:16:14.000
that I thought were interesting that look at 
the mechanisms behind in-context learning.

00:16:14.000 --> 00:16:17.679
I do think it's possible that in-context 
learning runs a small gradient descent loop

00:16:17.679 --> 00:16:22.319
internally in the layers of the neural network.
I recall one paper in particular where they were

00:16:22.320 --> 00:16:30.640
doing linear regression using in-context learning.
Your inputs into the neural network are XY pairs,

00:16:30.639 --> 00:16:35.600
XY, XY, XY that happen to be on the line.
Then you do X and you expect Y.

00:16:35.600 --> 00:16:40.639
The neural network, when you train it 
in this way, does linear regression.

00:16:41.519 --> 00:16:46.240
Normally when you would run linear regression, you 
have a small gradient descent optimizer that looks

00:16:46.240 --> 00:16:50.639
at XY, looks at an error, calculates the gradient 
of the weights and does the update a few times.

00:16:50.639 --> 00:16:54.799
It just turns out that when they looked at the 
weights of that in-context learning algorithm,

00:16:54.799 --> 00:16:59.120
they found some analogies to 
gradient descent mechanics.

00:16:59.120 --> 00:17:03.919
In fact, I think the paper was even stronger 
because they hardcoded the weights of a neural

00:17:03.919 --> 00:17:10.000
network to do gradient descent through 
attention and all the internals of the

00:17:10.000 --> 00:17:14.559
neural network. That's just my only pushback. 
Who knows how in-context learning works,

00:17:14.559 --> 00:17:19.919
but I think that it's probably doing a bit of some 
funky gradient descent internally. I think that

00:17:19.920 --> 00:17:24.800
that's possible. I was only pushing back on your 
saying that it's not doing in-context learning.

00:17:24.799 --> 00:17:28.240
Who knows what it's doing, but it's probably maybe 
doing something similar to it, but we don't know.

00:17:28.240 --> 00:17:34.480
So then it's worth thinking okay, if 
in-context learning and pre-training

00:17:34.480 --> 00:17:39.279
are both implementing something like gradient 
descent, why does it feel like with in-context

00:17:39.279 --> 00:17:44.639
learning we're getting to this continual 
learning, real intelligence-like thing?

00:17:44.640 --> 00:17:49.920
Whereas you don't get the analogous feeling just 
from pre-training. You could argue that. If it's

00:17:49.920 --> 00:17:53.039
the same algorithm, what could be different?
One way you could think about it is,

00:17:53.039 --> 00:18:00.240
how much information does the model store 
per information it receives from training?

00:18:00.240 --> 00:18:03.839
If you look at pre-training, if 
you look at Llama 3 for example,

00:18:03.839 --> 00:18:10.000
I think it's trained on 15 trillion tokens.
If you look at the 70B model, that would

00:18:10.000 --> 00:18:15.279
be the equivalent of 0.07 bits per 
token that it sees in pre-training,

00:18:15.279 --> 00:18:18.799
in terms of the information in the weights 
of the model compared to the tokens it reads.

00:18:18.799 --> 00:18:22.799
Whereas if you look at the KV cache 
and how it grows per additional token

00:18:22.799 --> 00:18:29.279
in in-context learning, it's like 320 kilobytes.
So that's a 35 million-fold difference in how much

00:18:29.279 --> 00:18:34.879
information per token is assimilated by the model.
I wonder if that's relevant at all.

00:18:34.880 --> 00:18:39.760
I kind of agree. The way I usually put this is 
that anything that happens during the training of

00:18:39.759 --> 00:18:45.599
the neural network, the knowledge is only a hazy 
recollection of what happened in training time.

00:18:45.599 --> 00:18:48.879
That's because the compression is dramatic.
You're taking 15 trillion tokens and you're

00:18:48.880 --> 00:18:51.680
compressing it to just your final neural 
network of a few billion parameters.

00:18:51.680 --> 00:18:53.840
Obviously it's a massive 
amount of compression going on.

00:18:54.559 --> 00:18:57.919
So I refer to it as a hazy 
recollection of the internet documents.

00:18:57.920 --> 00:19:00.720
Whereas anything that happens in the 
context window of the neural network—you're

00:19:00.720 --> 00:19:04.240
plugging in all the tokens and building up 
all those KV cache representations—is very

00:19:04.240 --> 00:19:08.000
directly accessible to the neural net.
So I compare the KV cache and the stuff

00:19:08.000 --> 00:19:11.599
that happens at test time to 
more like a working memory.

00:19:11.599 --> 00:19:16.240
All the stuff that's in the context window is 
very directly accessible to the neural net.

00:19:16.240 --> 00:19:21.120
There's always these almost surprising 
analogies between LLMs and humans.

00:19:21.119 --> 00:19:25.279
I find them surprising because we're not 
trying to build a human brain directly.

00:19:25.279 --> 00:19:27.279
We're just finding that this 
works and we're doing it.

00:19:27.279 --> 00:19:30.720
But I do think that anything 
that's in the weights, it's a

00:19:30.720 --> 00:19:36.079
hazy recollection of what you read a year ago.
Anything that you give it as a context at test

00:19:36.079 --> 00:19:40.079
time is directly in the working memory.
That's a very powerful analogy to

00:19:40.079 --> 00:19:42.159
think through things.
When you, for example,

00:19:42.160 --> 00:19:46.000
go to an LLM and you ask it about some book 
and what happened in it, like Nick Lane's

00:19:46.000 --> 00:19:49.680
book or something like that, the LLM will often 
give you some stuff which is roughly correct.

00:19:49.680 --> 00:19:53.600
But if you give it the full chapter and ask it 
questions, you're going to get much better results

00:19:53.599 --> 00:19:56.240
because it's now loaded in the 
working memory of the model.

00:19:56.240 --> 00:20:00.319
So a very long way of saying 
I agree and that's why.

00:20:00.319 --> 00:20:02.879
Stepping back, what is the part 
about human intelligence that we

00:20:03.759 --> 00:20:08.400
have most failed to replicate with these models?

00:20:12.400 --> 00:20:17.120
Just a lot of it. So maybe one way to think 
about it, I don't know if this is the best way,

00:20:17.119 --> 00:20:22.639
but I almost feel like — again, making these 
analogies imperfect as they are — we've stumbled

00:20:22.640 --> 00:20:27.360
by with the transformer neural network, 
which is extremely powerful, very general.

00:20:27.359 --> 00:20:31.359
You can train transformers on audio, or 
video, or text, or whatever you want,

00:20:31.359 --> 00:20:35.279
and it just learns patterns and they're 
very powerful, and it works really well.

00:20:35.279 --> 00:20:38.960
That to me almost indicates that this 
is some piece of cortical tissue.

00:20:38.960 --> 00:20:42.640
It's something like that, because the 
cortex is famously very plastic as well.

00:20:42.640 --> 00:20:48.080
You can rewire parts of brains.
There were the slightly gruesome experiments

00:20:48.079 --> 00:20:54.159
with rewiring the visual cortex to the auditory 
cortex, and this animal learned fine, et cetera.

00:20:54.160 --> 00:20:58.560
So I think that this is cortical tissue.
I think when we're doing reasoning and

00:20:58.559 --> 00:21:04.000
planning inside the neural networks, doing 
reasoning traces for thinking models,

00:21:04.000 --> 00:21:11.599
that's kind of like the prefrontal cortex.
Maybe those are like little checkmarks,

00:21:11.599 --> 00:21:15.519
but I still think there are many brain 
parts and nuclei that are not explored.

00:21:15.519 --> 00:21:18.480
For example, there's a basal ganglia doing a 
bit of reinforcement learning when we fine-tune

00:21:18.480 --> 00:21:23.599
the models on reinforcement learning. But where's 
the hippocampus? Not obvious what that would be.

00:21:23.599 --> 00:21:26.399
Some parts are probably not important.
Maybe the cerebellum is not important

00:21:26.400 --> 00:21:29.120
to cognition, its thoughts, so 
maybe we can skip some of it.

00:21:29.119 --> 00:21:32.719
But I still think there's, for example, the 
amygdala, all the emotions and instincts.

00:21:33.440 --> 00:21:36.799
There's probably a bunch of other nuclei 
in the brain that are very ancient that

00:21:36.799 --> 00:21:41.279
I don't think we've really replicated.
I don't know that we should be pursuing the

00:21:41.279 --> 00:21:45.519
building of an analog of a human brain.
I'm an engineer mostly at heart.

00:21:48.160 --> 00:21:52.160
Maybe another way to answer the question is that 
you're not going to hire this thing as an intern.

00:21:52.160 --> 00:21:55.519
It's missing a lot of it because it comes with 
a lot of these cognitive deficits that we all

00:21:55.519 --> 00:22:00.400
intuitively feel when we talk to the models.
So it's not fully there yet.

00:22:00.400 --> 00:22:04.480
You can look at it as not all the 
brain parts are checked off yet.

00:22:04.480 --> 00:22:10.079
This is maybe relevant to the question of thinking 
about how fast these issues will be solved.

00:22:10.799 --> 00:22:13.279
Sometimes people will say 
about continual learning,

00:22:13.279 --> 00:22:19.920
"Look, you could easily replicate this capability.
Just as in-context learning emerged spontaneously

00:22:19.920 --> 00:22:24.960
as a result of pre-training, continual 
learning over longer horizons will emerge

00:22:24.960 --> 00:22:30.720
spontaneously if the model is incentivized to 
recollect information over longer horizons,

00:22:30.720 --> 00:22:39.680
or horizons longer than one session."
So if there's some outer loop RL which has

00:22:39.680 --> 00:22:46.080
many sessions within that outer loop, then this 
continual learning where it fine-tunes itself,

00:22:46.079 --> 00:22:49.519
or it writes to an external memory or 
something, will just emerge spontaneously.

00:22:49.519 --> 00:22:53.759
Do you think things like that are plausible?
I just don't have a prior over

00:22:53.759 --> 00:22:55.440
how plausible that is.
How likely is that to happen?

00:22:55.440 --> 00:22:59.600
I don't know that I fully resonate with that.
These models, when you boot them up and they have

00:22:59.599 --> 00:23:03.039
zero tokens in the window, they're always 
restarting from scratch where they were.

00:23:03.039 --> 00:23:06.399
So I don't know in that 
worldview what it looks like.

00:23:09.359 --> 00:23:13.599
Maybe making some analogies to humans—just because 
I think it's roughly concrete and interesting to

00:23:13.599 --> 00:23:17.199
think through—I feel like when I'm awake, I'm 
building up a context window of stuff that's

00:23:17.200 --> 00:23:19.279
happening during the day.
But when I go to sleep,

00:23:19.279 --> 00:23:23.119
something magical happens where I don't 
think that context window stays around.

00:23:23.839 --> 00:23:27.119
There's some process of distillation 
into the weights of my brain.

00:23:27.839 --> 00:23:30.720
This happens during sleep and all this stuff.
We don't have an equivalent

00:23:30.720 --> 00:23:35.680
of that in large language models.
That's to me more adjacent to when you talk

00:23:35.680 --> 00:23:40.160
about continual learning and so on as absent.
These models don't really have a distillation

00:23:40.160 --> 00:23:47.920
phase of taking what happened, analyzing it 
obsessively, thinking through it, doing some

00:23:47.920 --> 00:23:51.920
synthetic data generation process and distilling 
it back into the weights, and maybe having

00:23:51.920 --> 00:24:01.200
a specific neural net per person. Maybe it's 
a LoRA. It's not a full-weight neural network.

00:24:01.200 --> 00:24:05.039
It's just some small sparse subset 
of the weights that are changed.

00:24:05.039 --> 00:24:10.559
But we do want to create ways of creating 
these individuals that have very long context.

00:24:10.559 --> 00:24:14.879
It's not only remaining in the context window 
because the context windows grow very, very long.

00:24:14.880 --> 00:24:17.680
Maybe we have some very elaborate, 
sparse attention over it.

00:24:17.680 --> 00:24:22.160
But I still think that humans obviously have some 
process for distilling some of that knowledge

00:24:22.160 --> 00:24:27.440
into the weights. We're missing it. I do also 
think that humans have some very elaborate,

00:24:27.440 --> 00:24:33.039
sparse attention scheme, which I think 
we're starting to see some early hints of.

00:24:33.039 --> 00:24:38.480
DeepSeek v3.2 just came out and I saw that they 
have sparse attention as an example, and this is

00:24:38.480 --> 00:24:44.160
one way to have very, very long context windows.
So I feel like we are redoing a lot of the

00:24:44.160 --> 00:24:47.279
cognitive tricks that evolution came up 
with through a very different process.

00:24:47.279 --> 00:24:50.240
But we're going to converge on a 
similar architecture cognitively.

00:24:51.039 --> 00:24:55.200
In 10 years, do you think it'll still be something 
like a transformer, but with much more modified

00:24:55.200 --> 00:24:59.759
attention and more sparse MLPs and so forth?
The way I like to think about it is

00:25:00.559 --> 00:25:05.200
translation invariance in time.
So 10 years ago, where were we? 2015.

00:25:05.200 --> 00:25:09.360
In 2015, we had convolutional neural networks 
primarily, residual networks just came out.

00:25:10.319 --> 00:25:14.000
So remarkably similar, I guess, but quite a 
bit different still. The transformer was not

00:25:14.000 --> 00:25:21.759
around. All these more modern tweaks 
on the transformer were not around.

00:25:21.759 --> 00:25:27.519
Maybe some of the things that we can bet on, I 
think in 10 years by translational equivariance,

00:25:27.519 --> 00:25:31.200
is that we're still training giant neural 
networks with a forward backward pass and

00:25:31.200 --> 00:25:36.400
update through gradient descent, 
but maybe it looks a bit different,

00:25:36.400 --> 00:25:42.080
and it's just that everything is much bigger.
Recently I went back all the way to 1989 which

00:25:42.079 --> 00:25:48.799
was a fun exercise for me, a few years ago, 
because I was reproducing Yann LeCun's 1989

00:25:48.799 --> 00:25:52.720
convolutional network, which was the first neural 
network I'm aware of trained via gradient descent,

00:25:52.720 --> 00:25:57.279
like modern neural network trained 
gradient descent on digit recognition.

00:25:57.279 --> 00:26:00.000
I was just interested in 
how I could modernize this.

00:26:00.000 --> 00:26:01.839
How much of this is algorithms?
How much of this is data?

00:26:01.839 --> 00:26:06.959
How much of this progress is compute and systems?
I was able to very quickly halve the learning

00:26:06.960 --> 00:26:13.200
just by time traveling by 33 years.
So if I time travel by algorithms 33 years,

00:26:13.200 --> 00:26:18.000
I could adjust what Yann LeCun did 
in 1989, and I could halve the error.

00:26:18.000 --> 00:26:21.359
But to get further gains, I 
had to add a lot more data,

00:26:21.359 --> 00:26:25.919
I had to 10x the training set, and then I 
had to add more computational optimizations.

00:26:25.920 --> 00:26:30.480
I had to train for much longer with dropout 
and other regularization techniques.

00:26:30.480 --> 00:26:33.839
So all these things have 
to improve simultaneously.

00:26:35.119 --> 00:26:37.279
We're probably going to have a lot more 
data, we're probably going to have a lot

00:26:37.279 --> 00:26:40.720
better hardware, probably going to have a lot 
better kernels and software, we're probably

00:26:40.720 --> 00:26:43.360
going to have better algorithms.
All of those, it's almost like

00:26:43.359 --> 00:26:48.319
no one of them is winning too much.
All of them are surprisingly equal.

00:26:48.880 --> 00:26:54.800
This has been the trend for a while.
So to answer your question, I expect differences

00:26:54.799 --> 00:26:58.559
algorithmically to what's happening today.
But I do also expect that some of the

00:26:58.559 --> 00:27:01.599
things that have stuck around for a very 
long time will probably still be there.

00:27:01.599 --> 00:27:05.039
It's probably still a giant neural network trained 
with gradient descent. That would be my guess.

00:27:05.039 --> 00:27:13.839
It's surprising that all of those things 
together only halved the error, 30 years

00:27:13.839 --> 00:27:18.000
of progress…. Maybe half is a lot. Because if 
you halve the error, that actually means that…

00:27:18.000 --> 00:27:24.240
Half is a lot. But I guess what was shocking to me 
is everything needs to improve across the board:

00:27:24.240 --> 00:27:28.480
architecture, optimizer, loss function.
It also has improved across the board forever.

00:27:28.480 --> 00:27:31.440
So I expect all those 
changes to be alive and well.

00:27:31.440 --> 00:27:34.320
Yeah. I was about to ask you a very 
similar question about nanochat.

00:27:35.039 --> 00:27:40.079
Since you just coded it up recently, 
every single step in the process of

00:27:40.079 --> 00:27:45.359
building a chatbot is fresh in your RAM.
I'm curious if you had similar thoughts about,

00:27:46.000 --> 00:27:52.000
"Oh, there was no one thing that was 
relevant to going from GPT-2 to nanochat."

00:27:52.000 --> 00:27:55.920
What are some surprising 
takeaways from the experience?

00:27:55.920 --> 00:27:59.039
Of building nanochat? So nanochat 
is a repository I released.

00:27:59.039 --> 00:28:02.000
Was it yesterday or the day 
before? I can't remember.

00:28:03.119 --> 00:28:06.079
We can see the sleep 
deprivation that went into the…

00:28:09.039 --> 00:28:12.879
It's trying to be the simplest complete 
repository that covers the whole pipeline

00:28:12.880 --> 00:28:18.400
end-to-end of building a ChatGPT clone.
So you have all of the steps, not just

00:28:18.400 --> 00:28:22.160
any individual step, which is a bunch.
I worked on all the individual steps

00:28:22.160 --> 00:28:25.920
in the past and released small pieces 
of code that show you how that's done

00:28:25.920 --> 00:28:31.360
in an algorithmic sense, in simple code.
But this handles the entire pipeline.

00:28:32.400 --> 00:28:36.160
In terms of learning, I don't 
know that I necessarily found

00:28:36.160 --> 00:28:40.720
something that I learned from it.
I already had in my mind how you build it.

00:28:40.720 --> 00:28:48.480
This is just the process of mechanically building 
it and making it clean enough so that people can

00:28:48.480 --> 00:28:53.120
learn from it and that they find it useful.
What is the best way for somebody

00:28:53.119 --> 00:28:54.959
to learn from it?
Is it to just delete

00:28:54.960 --> 00:28:58.319
all the code and try to reimplement from 
scratch, try to add modifications to it?

00:28:59.519 --> 00:29:03.359
That's a great question. Basically 
it's about 8,000 lines of code

00:29:03.359 --> 00:29:07.039
that takes you through the entire pipeline.
I would probably put it on the right monitor.

00:29:07.039 --> 00:29:11.440
If you have two monitors, you put it on the right.
You want to build it from scratch,

00:29:11.440 --> 00:29:14.720
you build it from the start.
You're not allowed to copy-paste, you're allowed

00:29:14.720 --> 00:29:17.680
to reference, you're not allowed to copy-paste.
Maybe that's how I would do it.

00:29:18.240 --> 00:29:21.680
But I also think the repository 
by itself is a pretty large beast.

00:29:23.519 --> 00:29:27.599
When you write this code, you don't go from top 
to bottom, you go from chunks and you grow the

00:29:27.599 --> 00:29:31.439
chunks, and that information is absent.
You wouldn't know where to start.

00:29:31.440 --> 00:29:35.759
So it's not just a final repository that's 
needed, it's the building of the repository,

00:29:35.759 --> 00:29:40.400
which is a complicated chunk-growing process.
So that part is not there yet.

00:29:40.400 --> 00:29:46.880
I would love to add that probably later this week.
It's probably a video or something like that.

00:29:49.200 --> 00:29:53.440
Roughly speaking, that's what I would try to do.
Build the stuff yourself, but don't allow

00:29:53.440 --> 00:29:55.680
yourself copy-paste.
I do think that

00:29:55.680 --> 00:29:59.920
there's two types of knowledge, almost.
There's the high-level surface knowledge, but when

00:29:59.920 --> 00:30:04.160
you build something from scratch, you're forced to 
come to terms with what you don't understand and

00:30:04.160 --> 00:30:08.160
you don't know that you don't understand it.
It always leads to a deeper understanding.

00:30:09.680 --> 00:30:13.759
It's the only way to build.
If I can't build it, I don't understand it.

00:30:13.759 --> 00:30:19.440
That’s a Feynman quote, I believe.
I 100% have always believed this very

00:30:19.440 --> 00:30:23.680
strongly, because there are all these micro 
things that are just not properly arranged

00:30:23.680 --> 00:30:25.680
and you don't really have the knowledge.
You just think you have the knowledge.

00:30:25.680 --> 00:30:28.960
So don't write blog posts, don't 
do slides, don't do any of that.

00:30:28.960 --> 00:30:31.759
Build the code, arrange it, get it to work.
It's the only way to go. Otherwise,

00:30:31.759 --> 00:30:35.440
you're missing knowledge.
You tweeted out that coding models

00:30:35.440 --> 00:30:41.120
were of very little help to you in assembling 
this repository. I'm curious why that was.

00:30:43.440 --> 00:30:46.559
I guess I built the repository over 
a period of a bit more than a month.

00:30:46.559 --> 00:30:50.480
I would say there are three major classes 
of how people interact with code right now.

00:30:50.480 --> 00:30:54.799
Some people completely reject all of LLMs 
and they are just writing by scratch.

00:30:54.799 --> 00:30:56.960
This is probably not the 
right thing to do anymore.

00:30:58.160 --> 00:31:02.400
The intermediate part, which is where I am, is 
you still write a lot of things from scratch,

00:31:02.400 --> 00:31:06.880
but you use the autocomplete that's 
available now from these models.

00:31:06.880 --> 00:31:10.240
So when you start writing out a little 
piece of it, it will autocomplete for

00:31:10.240 --> 00:31:12.559
you and you can just tap through.
Most of the time it's correct,

00:31:12.559 --> 00:31:15.519
sometimes it's not, and you edit it.
But you're still very much the

00:31:16.640 --> 00:31:21.680
architect of what you're writing.
Then there's the vibe coding: "Hi,

00:31:21.680 --> 00:31:28.080
please implement this or that," enter, and then 
let the model do it. That's the agents. I do feel

00:31:28.079 --> 00:31:33.199
like the agents work in very specific settings, 
and I would use them in specific settings.

00:31:33.200 --> 00:31:37.279
But these are all tools available to you 
and you have to learn what they're good at,

00:31:37.279 --> 00:31:40.559
what they're not good at, and when to use them.
So the agents are pretty good, for example,

00:31:40.559 --> 00:31:44.559
if you're doing boilerplate stuff.
Boilerplate code that's just

00:31:44.559 --> 00:31:48.319
copy-paste stuff, they're very good at that.
They're very good at stuff that occurs very often

00:31:48.319 --> 00:31:53.599
on the Internet because there are lots of examples 
of it in the training sets of these models.

00:31:55.200 --> 00:31:58.240
There are features of things where 
the models will do very well.

00:31:58.240 --> 00:32:03.039
I would say nanochat is not an example of 
those because it's a fairly unique repository.

00:32:03.039 --> 00:32:08.879
There's not that much code in the way that 
I've structured it. It's not boilerplate code.

00:32:08.880 --> 00:32:13.200
It's intellectually intense code almost, and 
everything has to be very precisely arranged.

00:32:13.200 --> 00:32:21.200
The models have so many cognitive deficits.
One example, they kept misunderstanding the code

00:32:22.480 --> 00:32:26.319
because they have too much memory from 
all the typical ways of doing things on

00:32:26.319 --> 00:32:31.039
the Internet that I just wasn't adopting.
The models, for example—I don't know if I

00:32:31.039 --> 00:32:36.960
want to get into the full details—but they kept 
thinking I'm writing normal code, and I'm not.

00:32:36.960 --> 00:32:41.759
Maybe one example?
You have eight GPUs

00:32:41.759 --> 00:32:44.559
that are all doing forward, backwards.
The way to synchronize gradients between

00:32:44.559 --> 00:32:48.720
them is to use a Distributed Data Parallel 
container of PyTorch, which automatically

00:32:49.519 --> 00:32:52.400
as you're doing the backward, it will start 
communicating and synchronizing gradients.

00:32:52.400 --> 00:32:56.640
I didn't use DDP because I didn't want 
to use it, because it's not necessary.

00:32:56.640 --> 00:33:02.480
I threw it out and wrote my own synchronization 
routine that's inside the step of the optimizer.

00:33:02.480 --> 00:33:06.720
The models were trying to get me to 
use the DDP container. They were very

00:33:06.720 --> 00:33:11.279
concerned. This gets way too technical, 
but I wasn't using that container because

00:33:11.279 --> 00:33:14.000
I don't need it and I have a custom 
implementation of something like it.

00:33:14.000 --> 00:33:15.759
They just couldn't internalize 
that you had your own.

00:33:16.640 --> 00:33:23.200
They couldn't get past that. They kept trying to 
mess up the style. They're way too over-defensive.

00:33:23.200 --> 00:33:27.120
They make all these try-catch statements.
They keep trying to make a production code base,

00:33:27.119 --> 00:33:29.919
and I have a bunch of assumptions 
in my code, and it's okay.

00:33:31.920 --> 00:33:36.560
I don't need all this extra stuff in there.
So I feel like they're bloating the code base,

00:33:36.559 --> 00:33:38.879
bloating the complexity, they keep 
misunderstanding, they're using

00:33:38.880 --> 00:33:46.800
deprecated APIs a bunch of times. It's a total 
mess. It's just not net useful. I can go in,

00:33:46.799 --> 00:33:50.960
I can clean it up, but it's not net useful.
I also feel like it's annoying to have to

00:33:50.960 --> 00:33:54.160
type out what I want in English 
because it's too much typing.

00:33:54.160 --> 00:33:58.640
If I just navigate to the part of the code that I 
want, and I go where I know the code has to appear

00:33:58.640 --> 00:34:02.320
and I start typing out the first few letters, 
autocomplete gets it and just gives you the code.

00:34:03.839 --> 00:34:07.039
This is a very high information 
bandwidth to specify what you want.

00:34:07.039 --> 00:34:10.079
You point to the code where you want 
it, you type out the first few pieces,

00:34:10.079 --> 00:34:15.440
and the model will complete it.
So what I mean is, these models

00:34:15.440 --> 00:34:22.240
are good in certain parts of the stack.
There are two examples where I use the

00:34:22.239 --> 00:34:26.559
models that I think are illustrative.
One was when I generated the report.

00:34:26.559 --> 00:34:30.239
That's more boilerplate-y, so I 
partially vibe-coded some of that stuff.

00:34:30.239 --> 00:34:34.319
That was fine because it's not 
mission-critical stuff, and it works fine.

00:34:34.320 --> 00:34:37.920
The other part is when I was 
rewriting the tokenizer in Rust.

00:34:37.920 --> 00:34:41.119
I'm not as good at Rust 
because I'm fairly new to Rust.

00:34:41.119 --> 00:34:45.839
So there's a bit of vibe coding going on 
when I was writing some of the Rust code.

00:34:45.840 --> 00:34:49.200
But I had a Python implementation that I 
fully understand, and I'm just making sure

00:34:49.199 --> 00:34:53.119
I'm making a more efficient version of it, and 
I have tests so I feel safer doing that stuff.

00:34:56.239 --> 00:35:02.559
They increase accessibility to languages or 
paradigms that you might not be as familiar with.

00:35:02.559 --> 00:35:06.559
I think they're very helpful there as well.
There's a ton of Rust code out there,

00:35:06.559 --> 00:35:09.279
the models are pretty good at it.
I happen to not know that much about it,

00:35:09.280 --> 00:35:12.960
so the models are very useful there.
The reason this question is so interesting

00:35:12.960 --> 00:35:19.199
is because the main story people 
have about AI exploding and getting

00:35:19.199 --> 00:35:24.799
to superintelligence pretty rapidly is AI 
automating AI engineering and AI research.

00:35:25.519 --> 00:35:27.755
They'll look at the fact that you can have 
Claude Code and make entire applications,

00:35:27.755 --> 00:35:32.800
CRUD applications, from scratch and think, "If 
you had this same capability inside of OpenAI

00:35:32.800 --> 00:35:38.480
and DeepMind and everything, just imagine 
a thousand of you or a million of you in

00:35:38.480 --> 00:35:43.519
parallel, finding little architectural tweaks."
It's quite interesting to hear you say that this

00:35:43.519 --> 00:35:48.480
is the thing they're asymmetrically worse at.
It's quite relevant to forecasting whether

00:35:48.480 --> 00:35:52.960
the AI 2027-type explosion is 
likely to happen anytime soon.

00:35:53.519 --> 00:35:58.400
That's a good way of putting it, and you're 
getting at why my timelines are a bit longer.

00:35:58.400 --> 00:36:04.079
You're right. They're not very good at code 
that has never been written before, maybe it's

00:36:04.079 --> 00:36:07.759
one way to put it, which is what we're trying 
to achieve when we're building these models.

00:36:07.760 --> 00:36:14.800
Very naive question, but the architectural 
tweaks that you're adding to nanochat,

00:36:14.800 --> 00:36:17.360
they're in a paper somewhere, right?
They might even be in a repo somewhere.

00:36:20.320 --> 00:36:25.280
Is it surprising that they aren't able to 
integrate that into whenever you're like,

00:36:25.280 --> 00:36:29.760
"Add RoPE embeddings" or something, 
they do that in the wrong way?

00:36:29.760 --> 00:36:34.320
It's tough. They know, but they don't fully know.
They don't know how to fully integrate it into

00:36:34.320 --> 00:36:37.440
the repo and your style and your code and 
your place, and some of the custom things

00:36:37.440 --> 00:36:41.200
that you're doing and how it fits with 
all the assumptions of the repository.

00:36:42.880 --> 00:36:46.960
They do have some knowledge, but they 
haven't gotten to the place where they

00:36:46.960 --> 00:36:52.960
can integrate it and make sense of it.
A lot of the stuff continues to improve.

00:36:54.079 --> 00:36:57.279
Currently, the state-of-the-art 
model that I go to is the GPT-5 Pro,

00:36:57.920 --> 00:37:01.280
and that's a very powerful model.
If I have 20 minutes,

00:37:01.280 --> 00:37:06.000
I will copy-paste my entire repo and I go to 
GPT-5 Pro, the oracle, for some questions.

00:37:06.000 --> 00:37:09.679
Often it's not too bad and surprisingly 
good compared to what existed a year ago.

00:37:11.760 --> 00:37:20.880
Overall, the models are not there.
I feel like the industry is making too

00:37:20.880 --> 00:37:27.119
big of a jump and is trying to pretend like this 
is amazing, and it's not. It's slop. They're not

00:37:27.119 --> 00:37:29.839
coming to terms with it, and maybe they're 
trying to fundraise or something like that.

00:37:29.840 --> 00:37:33.760
I'm not sure what's going on, but we're 
at this intermediate stage. The models are

00:37:33.760 --> 00:37:38.320
amazing. They still need a lot of work.
For now, autocomplete is my sweet spot.

00:37:38.320 --> 00:37:41.200
But sometimes, for some types of 
code, I will go to an LLM agent.

00:37:43.039 --> 00:37:48.320
Here's another reason this is really interesting.
Through the history of programming, there have

00:37:48.320 --> 00:37:55.280
been many productivity improvements—compilers, 
linting, better programming languages—which

00:37:55.920 --> 00:37:59.519
have increased programmer productivity 
but have not led to an explosion.

00:38:00.880 --> 00:38:04.400
That sounds very much like the 
autocomplete tab, and this other

00:38:04.400 --> 00:38:09.039
category is just automation of the programmer.
It's interesting you're seeing more in the

00:38:09.039 --> 00:38:13.679
category of the historical analogies 
of better compilers or something.

00:38:13.679 --> 00:38:19.119
Maybe this gets to one other thought.
I have a hard time differentiating where

00:38:19.119 --> 00:38:23.039
AI begins and stops because I see 
AI as fundamentally an extension of

00:38:23.039 --> 00:38:28.639
computing in a pretty fundamental way.
I see a continuum of this recursive

00:38:28.639 --> 00:38:35.199
self-improvement or speeding up programmers 
all the way from the beginning: code editors,

00:38:37.440 --> 00:38:43.760
syntax highlighting, or checking even of 
the types, like data type checking—all

00:38:44.880 --> 00:38:48.160
these tools that we've built for 
each other. Even search engines.

00:38:48.159 --> 00:38:55.039
Why aren't search engines part of AI? Ranking 
is AI. At some point, Google, even early on,

00:38:55.039 --> 00:38:59.119
was thinking of themselves as an AI company doing 
Google Search engine, which is totally fair.

00:38:59.119 --> 00:39:04.079
I see it as a lot more of a continuum than other 
people do, and it's hard for me to draw the line.

00:39:04.079 --> 00:39:07.360
I feel like we're now getting a much 
better autocomplete, and now we're also

00:39:07.360 --> 00:39:11.760
getting some agents which are these loopy 
things, but they go off-rails sometimes.

00:39:13.519 --> 00:39:18.079
What's going on is that the human is progressively 
doing a bit less and less of the low-level stuff.

00:39:18.079 --> 00:39:20.880
We're not writing the assembly 
code because we have compilers.

00:39:20.880 --> 00:39:23.840
Compilers will take my high-level 
language in C and write the assembly code.

00:39:23.840 --> 00:39:28.800
We're abstracting ourselves very, very slowly.
There's this what I call "autonomy slider," where

00:39:28.800 --> 00:39:32.640
more and more stuff is automated—of the stuff that 
can be automated at any point in time—and we're

00:39:32.639 --> 00:39:37.759
doing a bit less and less and raising ourselves 
in the layer of abstraction over the automation.

00:40:53.920 --> 00:40:56.880
Let's talk about RL a bit.
You tweeted some very

00:40:56.880 --> 00:41:00.880
interesting things about this.
Conceptually, how should we think about

00:41:00.880 --> 00:41:07.360
the way that humans are able to build a rich world 
model just from interacting with our environment,

00:41:07.360 --> 00:41:13.360
and in ways that seem almost irrespective of 
the final reward at the end of the episode?

00:41:13.360 --> 00:41:17.840
If somebody is starting a business, and at 
the end of 10 years, she finds out whether

00:41:17.840 --> 00:41:22.480
the business succeeded or failed, we say that 
she's earned a bunch of wisdom and experience.

00:41:22.480 --> 00:41:25.920
But it's not because the log probs of every 
single thing that happened over the last 10

00:41:25.920 --> 00:41:29.840
years are up-weighted or down-weighted.
Something much more deliberate and

00:41:29.840 --> 00:41:33.200
rich is happening.
What is the ML analogy, and how does that

00:41:33.199 --> 00:41:36.639
compare to what we're doing with LLMs right now?
Maybe the way I would put it is that humans don't

00:41:36.639 --> 00:41:40.639
use reinforcement learning, as I said.
I think they do something different.

00:41:42.639 --> 00:41:48.159
Reinforcement learning is a lot worse than I 
think the average person thinks. Reinforcement

00:41:48.159 --> 00:41:53.119
learning is terrible. It just so happens 
that everything that we had before it is

00:41:53.119 --> 00:41:58.559
much worse because previously we were just 
imitating people, so it has all these issues.

00:41:59.599 --> 00:42:04.000
In reinforcement learning, say you're solving 
a math problem, because it's very simple.

00:42:04.000 --> 00:42:06.880
You're given a math problem and 
you're trying to find the solution.

00:42:08.159 --> 00:42:13.279
In reinforcement learning, you will 
try lots of things in parallel first.

00:42:13.920 --> 00:42:18.639
You're given a problem, you try hundreds of 
different attempts. These attempts can be complex.

00:42:18.639 --> 00:42:22.400
They can be like, "Oh, let me try this, let me try 
that, this didn't work, that didn't work," etc.

00:42:22.400 --> 00:42:25.840
Then maybe you get an answer.
Now you check the back of the book and you see,

00:42:25.840 --> 00:42:30.079
"Okay, the correct answer is this."
You can see that this one, this one,

00:42:30.079 --> 00:42:33.840
and that one got the correct answer, 
but these other 97 of them didn't.

00:42:33.840 --> 00:42:37.840
Literally what reinforcement learning does is it 
goes to the ones that worked really well and every

00:42:37.840 --> 00:42:42.640
single thing you did along the way, every single 
token gets upweighted like, "Do more of this."

00:42:42.639 --> 00:42:46.559
The problem with that is people will say 
that your estimator has high variance,

00:42:46.559 --> 00:42:53.440
but it's just noisy. It's noisy. It almost assumes 
that every single little piece of the solution

00:42:53.440 --> 00:42:56.960
that you made that arrived at the right answer 
was the correct thing to do, which is not true.

00:42:56.960 --> 00:43:00.639
You may have gone down the wrong alleys 
until you arrived at the right solution.

00:43:00.639 --> 00:43:04.000
Every single one of those incorrect things you 
did, as long as you got to the correct solution,

00:43:04.000 --> 00:43:08.400
will be upweighted as, "Do more of this." 
It's terrible. It's noise. You've done all

00:43:08.400 --> 00:43:13.519
this work only to find, at the end, you get a 
single number of like, "Oh, you did correct."

00:43:14.159 --> 00:43:18.159
Based on that, you weigh that entire 
trajectory as like, upweight or downweight.

00:43:19.039 --> 00:43:22.400
The way I like to put it is you're 
sucking supervision through a straw.

00:43:22.400 --> 00:43:24.880
You've done all this work that 
could be a minute of rollout,

00:43:24.880 --> 00:43:29.200
and you're sucking the bits of supervision of the 
final reward signal through a straw and you're

00:43:33.920 --> 00:43:37.119
broadcasting that across the entire trajectory 
and using that to upweight or downweight that

00:43:37.119 --> 00:43:39.920
trajectory. It's just stupid and 
crazy. A human would never do this.

00:43:39.920 --> 00:43:43.119
Number one, a human would 
never do hundreds of rollouts.

00:43:43.119 --> 00:43:47.599
Number two, when a person finds a solution, 
they will have a pretty complicated process

00:43:47.599 --> 00:43:51.839
of review of, "Okay, I think these parts I 
did well, these parts I did not do that well.

00:43:51.840 --> 00:43:55.440
I should probably do this or that." 
They think through things. There's

00:43:55.440 --> 00:44:00.240
nothing in current LLMs that does this. 
There's no equivalent of it. But I do see

00:44:00.239 --> 00:44:03.919
papers popping out that are trying to do this 
because it's obvious to everyone in the field.

00:44:05.280 --> 00:44:09.120
The first imitation learning, by the way, was 
extremely surprising and miraculous and amazing,

00:44:09.119 --> 00:44:14.079
that we can fine-tune by imitation on humans. 
That was incredible. Because in the beginning,

00:44:14.079 --> 00:44:18.239
all we had was base models. Base models 
are autocomplete. It wasn't obvious to

00:44:18.239 --> 00:44:23.359
me at the time, and I had to learn this.
The paper that blew my mind was InstructGPT,

00:44:24.159 --> 00:44:28.000
because it pointed out that you can take 
the pretrained model, which is autocomplete,

00:44:28.000 --> 00:44:32.559
and if you just fine-tune it on text that looks 
like conversations, the model will very rapidly

00:44:32.559 --> 00:44:36.799
adapt to become very conversational, and it 
keeps all the knowledge from pre-training.

00:44:36.800 --> 00:44:41.120
This blew my mind because I didn't understand 
that stylistically, it can adjust so quickly

00:44:41.119 --> 00:44:46.480
and become an assistant to a user through just 
a few loops of fine-tuning on that kind of data.

00:44:46.480 --> 00:44:51.840
It was very miraculous to me that that worked. 
So incredible. That was two to three years of

00:44:51.840 --> 00:44:58.240
work. Now came RL. And RL allows you to do a bit 
better than just imitation learning because you

00:44:58.239 --> 00:45:02.159
can have these reward functions and you 
can hill-climb on the reward functions.

00:45:02.719 --> 00:45:06.159
Some problems have just correct answers, you 
can hill-climb on that without getting expert

00:45:06.159 --> 00:45:10.960
trajectories to imitate. So that's amazing. The 
model can also discover solutions that a human

00:45:10.960 --> 00:45:19.199
might never come up with. This is incredible. 
Yet, it's still stupid. We need more. I saw a

00:45:19.199 --> 00:45:25.199
paper from Google yesterday that tried to 
have this reflect & review idea in mind.

00:45:25.199 --> 00:45:30.319
Was it the memory bank paper or something? I don't 
know. I've seen a few papers along these lines.

00:45:30.320 --> 00:45:37.360
So I expect there to be some major update to how 
we do algorithms for LLMs coming in that realm.

00:45:37.360 --> 00:45:42.480
I think we need three or four or 
five more, something like that.

00:45:42.480 --> 00:45:47.920
You're so good at coming up with evocative 
phrases. "Sucking supervision through a

00:45:47.920 --> 00:45:56.320
straw." It's so good. You're saying the problem 
with outcome-based reward is that you have this

00:45:56.320 --> 00:46:01.519
huge trajectory, and then at the end, you're 
trying to learn every single possible thing

00:46:01.519 --> 00:46:05.440
about what you should do and what you should 
learn about the world from that one final bit.

00:46:07.280 --> 00:46:11.519
Given the fact that this is obvious, why hasn't 
process-based supervision as an alternative been

00:46:11.519 --> 00:46:15.759
a successful way to make models more capable?
What has been preventing us from using

00:46:15.760 --> 00:46:18.000
this alternative paradigm?
Process-based supervision just

00:46:18.000 --> 00:46:21.199
refers to the fact that we're not going to 
have a reward function only at the very end.

00:46:21.199 --> 00:46:24.319
After you've done 10 minutes of work, I'm not 
going to tell you you did well or not well.

00:46:24.320 --> 00:46:26.960
I'm going to tell you at every single 
step of the way how well you're doing.

00:46:28.000 --> 00:46:32.079
The reason we don't have that is 
it's tricky how you do that properly.

00:46:32.079 --> 00:46:34.960
You have partial solutions and you 
don't know how to assign credit.

00:46:34.960 --> 00:46:39.280
So when you get the right answer, it's just 
an equality match to the answer. It’s very

00:46:39.280 --> 00:46:44.640
simple to implement. If you're doing process 
supervision, how do you assign in an automatable

00:46:44.639 --> 00:46:47.839
way, a partial credit assignment?
It's not obvious how you do it.

00:46:47.840 --> 00:46:50.960
Lots of labs are trying to 
do it with these LLM judges.

00:46:50.960 --> 00:46:53.920
You get LLMs to try to do it.
You prompt an LLM, "Hey,

00:46:53.920 --> 00:46:56.720
look at a partial solution of a student.
How well do you think they're doing if the

00:46:56.719 --> 00:47:02.559
answer is this?" and they try to tune the prompt.
The reason that this is tricky is quite subtle.

00:47:02.559 --> 00:47:07.679
It's the fact that anytime you use an LLM to 
assign a reward, those LLMs are giant things

00:47:07.679 --> 00:47:11.359
with billions of parameters, and they're gameable.
If you're reinforcement learning with respect to

00:47:11.360 --> 00:47:15.320
them, you will find adversarial examples 
for your LLM judges, almost guaranteed.

00:47:15.320 --> 00:47:18.559
So you can't do this for too long.
You do maybe 10 steps or 20 steps, and maybe

00:47:18.559 --> 00:47:25.039
it will work, but you can't do 100 or 1,000.
I understand it's not obvious, but basically

00:47:25.039 --> 00:47:30.639
the model will find little cracks.
It will find all these spurious

00:47:30.639 --> 00:47:34.960
things in the nooks and crannies of the 
giant model and find a way to cheat it.

00:47:34.960 --> 00:47:42.639
One example that's prominently in my mind, this 
was probably public, if you're using an LLM judge

00:47:42.639 --> 00:47:47.279
for a reward, you just give it a solution from a 
student and ask it if the student did well or not.

00:47:47.280 --> 00:47:49.280
We were training with 
reinforcement learning against

00:47:49.280 --> 00:47:55.519
that reward function, and it worked really well.
Then, suddenly, the reward became extremely large.

00:47:55.519 --> 00:47:59.840
It was a massive jump, and it did perfect.
You're looking at it like, "Wow, this means

00:47:59.840 --> 00:48:05.280
the student is perfect in all these problems. 
It's fully solved math." But when you look at

00:48:05.280 --> 00:48:08.480
the completions that you're getting from 
the model, they are complete nonsense.

00:48:08.480 --> 00:48:10.400
They start out okay, and then 
they change to "dhdhdhdh."

00:48:11.360 --> 00:48:14.720
It's just like, "Oh, okay, let's take two plus 
three and we do this and this, and then dhdhdhdh."

00:48:15.840 --> 00:48:17.200
You're looking at it, and 
it's like, this is crazy.

00:48:17.199 --> 00:48:21.759
How is it getting a reward of one or 100%?
You look at the LLM judge, and it turns out

00:48:21.760 --> 00:48:27.200
that "dhdhdhdh" is an adversarial example for 
the model, and it assigns 100% probability to it.

00:48:27.199 --> 00:48:30.239
It's just because this is an 
out-of-sample example to the LLM.

00:48:30.239 --> 00:48:34.000
It's never seen it during training, 
and you're in pure generalization land.

00:48:34.000 --> 00:48:37.440
It's never seen it during training, and in 
the pure generalization land, you can find

00:48:37.440 --> 00:48:42.240
these examples that break it.
You're basically training

00:48:42.239 --> 00:48:46.799
the LLM to be a prompt injection model.
Not even that. Prompt injection is way too fancy.

00:48:46.800 --> 00:48:48.960
You're finding adversarial 
examples, as they're called.

00:48:48.960 --> 00:48:55.440
These are nonsensical solutions that are obviously 
wrong, but the model thinks they are amazing.

00:48:55.440 --> 00:48:59.519
To the extent you think this is the 
bottleneck to making RL more functional,

00:48:59.519 --> 00:49:04.400
then that will require making LLMs better judges, 
if you want to do this in an automated way.

00:49:05.199 --> 00:49:07.359
Is it just going to be some sort 
of GAN-like approach where you

00:49:07.360 --> 00:49:11.360
have to train models to be more robust?
The labs are probably doing all that.

00:49:11.920 --> 00:49:14.720
The obvious thing is, "dhdhdhdh" 
should not get 100% reward.

00:49:14.719 --> 00:49:17.759
Okay, well, take "dhdhdhdh," put it 
in the training set of the LLM judge,

00:49:17.760 --> 00:49:21.680
and say this is not 100%, this is 0%.
You can do this, but every time you do

00:49:21.679 --> 00:49:24.799
this, you get a new LLM, and it 
still has adversarial examples.

00:49:24.800 --> 00:49:29.360
There's an infinity of adversarial examples.
Probably if you iterate this a few times, it'll

00:49:29.360 --> 00:49:32.800
probably be harder and harder to find adversarial 
examples, but I'm not 100% sure because this thing

00:49:32.800 --> 00:49:38.560
has a trillion parameters or whatnot.
I bet you the labs are trying.

00:49:41.920 --> 00:49:46.159
I still think we need other ideas.
Interesting. Do you have some shape

00:49:46.159 --> 00:49:54.239
of what the other idea could be?
This idea of a review solution

00:49:54.239 --> 00:49:58.319
encompassing synthetic examples such that 
when you train on them, you get better,

00:49:58.320 --> 00:50:00.800
and meta-learn it in some way.
I think there are some papers

00:50:00.800 --> 00:50:04.320
that I'm starting to see pop out.
I am only at a stage of reading abstracts

00:50:04.320 --> 00:50:08.880
because a lot of these papers are just ideas.
Someone has to make it work on a frontier

00:50:08.880 --> 00:50:13.280
LLM lab scale in full generality 
because when you see these papers,

00:50:13.280 --> 00:50:17.840
they pop up, and it's just a bit noisy.
They're cool ideas, but I haven't seen

00:50:17.840 --> 00:50:23.680
anyone convincingly show that this is possible.
That said, the LLM labs are fairly closed,

00:50:23.679 --> 00:50:33.199
so who knows what they're doing now.
I can conceptualize how you would be able

00:50:33.199 --> 00:50:36.799
to train on synthetic examples or synthetic 
problems that you have made for yourself.

00:50:36.800 --> 00:50:40.720
But there seems to be another thing humans 
do—maybe sleep is this, maybe daydreaming is

00:50:40.719 --> 00:50:46.159
this—which is not necessarily to come up 
with fake problems, but just to reflect.

00:50:47.280 --> 00:50:51.519
I'm not sure what the ML analogy is for 
daydreaming or sleeping, or just reflecting.

00:50:51.519 --> 00:50:54.639
I haven't come up with a new problem.
Obviously, the very basic analogy would just

00:50:54.639 --> 00:50:59.599
be fine-tuning on reflection bits, but I feel like 
in practice that probably wouldn't work that well.

00:51:00.239 --> 00:51:05.119
Do you have some take on what 
the analogy of this thing is?

00:51:05.119 --> 00:51:09.599
I do think that we're missing some aspects there.
As an example, let’s take reading a book.

00:51:11.360 --> 00:51:15.680
Currently when LLMs are reading a book, what that 
means is we stretch out the sequence of text,

00:51:15.679 --> 00:51:19.119
and the model is predicting the next token, 
and it's getting some knowledge from that.

00:51:19.119 --> 00:51:21.519
That's not really what humans do.
When you're reading a book,

00:51:21.519 --> 00:51:25.840
I don't even feel like the book is exposition 
I'm supposed to be attending to and training on.

00:51:25.840 --> 00:51:30.079
The book is a set of prompts for 
me to do synthetic data generation,

00:51:30.079 --> 00:51:33.199
or for you to get to a book club 
and talk about it with your friends.

00:51:33.199 --> 00:51:36.960
It's by manipulating that information 
that you actually gain that knowledge.

00:51:37.679 --> 00:51:42.000
We have no equivalent of that with LLMs. They 
don't really do that. I'd love to see during

00:51:42.000 --> 00:51:46.320
pre-training some stage that thinks through 
the material and tries to reconcile it with

00:51:46.320 --> 00:51:52.160
what it already knows, and thinks through it 
for some amount of time and gets that to work.

00:51:52.159 --> 00:51:54.719
There's no equivalence of any of this. 
This is all research. There are some

00:51:54.719 --> 00:51:59.519
subtle—very subtle that I think are very hard 
to understand—reasons why it's not trivial.

00:51:59.519 --> 00:52:04.159
If I can just describe one: why can't we 
just synthetically generate and train on it?

00:52:04.159 --> 00:52:07.359
Because every synthetic example, if 
I just give synthetic generation of

00:52:07.360 --> 00:52:10.800
the model thinking about a book, you look 
at it and you're like, "This looks great.

00:52:10.800 --> 00:52:12.960
Why can't I train on it?"
You could try, but the model

00:52:12.960 --> 00:52:17.039
will get much worse if you continue trying.
That's because all of the samples you get

00:52:17.039 --> 00:52:21.519
from models are silently collapsed.
Silently—it is not obvious if you look

00:52:21.519 --> 00:52:26.480
at any individual example of it—they occupy 
a very tiny manifold of the possible space of

00:52:27.199 --> 00:52:30.480
thoughts about content.
The LLMs, when they come off,

00:52:30.480 --> 00:52:33.280
they're what we call "collapsed."
They have a collapsed data distribution.

00:52:34.960 --> 00:52:38.400
One easy way to see it is to go to 
ChatGPT and ask it, "Tell me a joke."

00:52:38.400 --> 00:52:41.440
It only has like three jokes.
It's not giving you the whole breadth

00:52:41.440 --> 00:52:47.360
of possible jokes. It knows like three jokes. 
They're silently collapsed. You're not getting

00:52:47.360 --> 00:52:52.559
the richness and the diversity and the entropy 
from these models as you would get from humans.

00:52:52.559 --> 00:52:55.599
Humans are a lot noisier, but 
at least they're not biased,

00:52:56.559 --> 00:53:00.719
in a statistical sense. They're not silently 
collapsed. They maintain a huge amount of entropy.

00:53:00.719 --> 00:53:05.519
So how do you get synthetic data generation to 
work despite the collapse and while maintaining

00:53:05.519 --> 00:53:09.840
the entropy? That’s a research problem.
Just to make sure I understood, the reason

00:53:09.840 --> 00:53:13.200
that the collapse is relevant to synthetic data 
generation is because you want to be able to

00:53:13.199 --> 00:53:20.079
come up with synthetic problems or reflections 
which are not already in your data distribution?

00:53:20.079 --> 00:53:25.679
I guess what I'm saying is, say we have a chapter 
of a book and I ask an LLM to think about it,

00:53:26.559 --> 00:53:28.400
it will give you something 
that looks very reasonable.

00:53:28.400 --> 00:53:31.920
But if I ask it 10 times, you'll 
notice that all of them are the same.

00:53:31.920 --> 00:53:39.840
You can't just keep scaling "reflection" 
on the same amount of prompt information

00:53:39.840 --> 00:53:43.840
and then get returns from that.
Any individual sample will look okay,

00:53:43.840 --> 00:53:47.840
but the distribution of it is quite terrible.
It's quite terrible in such a way that if

00:53:47.840 --> 00:53:50.880
you continue training on too much of 
your own stuff, you actually collapse.

00:53:50.880 --> 00:53:53.680
I think that there's possibly 
no fundamental solution to this.

00:53:54.239 --> 00:54:00.000
I also think humans collapse over time. 
These analogies are surprisingly good.

00:54:00.000 --> 00:54:06.000
Humans collapse during the course of their lives.
This is why children, they haven't overfit yet.

00:54:06.000 --> 00:54:09.840
They will say stuff that will shock you 
because you can see where they're coming from,

00:54:09.840 --> 00:54:14.480
but it's just not the thing people say, 
because they're not yet collapsed. But we're

00:54:14.480 --> 00:54:20.159
collapsed. We end up revisiting the same thoughts.
We end up saying more and more of the same stuff,

00:54:20.159 --> 00:54:24.000
and the learning rates go down, and 
the collapse continues to get worse,

00:54:24.000 --> 00:54:28.480
and then everything deteriorates.
Have you seen this super interesting

00:54:28.480 --> 00:54:34.159
paper that dreaming is a way of preventing 
this kind of overfitting and collapse?

00:54:34.159 --> 00:54:41.279
The reason dreaming is evolutionary adaptive 
is to put you in weird situations that are

00:54:41.840 --> 00:54:44.559
very unlike your day-to-day reality, so 
as to prevent this kind of overfitting.

00:54:44.559 --> 00:54:48.079
It's an interesting idea. I do think 
that when you're generating things

00:54:48.079 --> 00:54:51.360
in your head and then you're attending to 
it, you're training on your own samples,

00:54:51.360 --> 00:54:53.680
you're training on your synthetic data.
If you do it for too long,

00:54:53.679 --> 00:54:59.919
you go off-rails and you collapse way too much.
You always have to seek entropy in your life.

00:55:01.360 --> 00:55:05.120
Talking to other people is a great 
source of entropy, and things like that.

00:55:05.119 --> 00:55:09.839
So maybe the brain has also built some internal 
mechanisms for increasing the amount of entropy

00:55:11.360 --> 00:55:16.720
in that process. That's an interesting idea.
This is a very ill-formed thought so I’ll

00:55:16.719 --> 00:55:20.639
just put it out and let you react to it.
The best learners that we are aware of,

00:55:20.639 --> 00:55:25.679
which are children, are extremely 
bad at recollecting information.

00:55:25.679 --> 00:55:29.199
In fact, at the very earliest stages of 
childhood, you will forget everything.

00:55:29.199 --> 00:55:32.639
You're just an amnesiac about everything 
that happens before a certain year date.

00:55:32.639 --> 00:55:36.079
But you're extremely good at picking up 
new languages and learning from the world.

00:55:36.079 --> 00:55:38.960
Maybe there's some element of being 
able to see the forest for the trees.

00:55:38.960 --> 00:55:44.079
Whereas if you compare it to the opposite end 
of the spectrum, you have LLM pre-training,

00:55:44.079 --> 00:55:47.920
where these models will literally be 
able to regurgitate word-for-word what

00:55:47.920 --> 00:55:52.400
is the next thing in a Wikipedia page.
But their ability to learn abstract

00:55:52.400 --> 00:55:55.920
concepts really quickly, the way 
a child can, is much more limited.

00:55:55.920 --> 00:55:59.840
Then adults are somewhere in between, where 
they don't have the flexibility of childhood

00:55:59.840 --> 00:56:05.920
learning, but they can memorize facts and 
information in a way that is harder for kids.

00:56:05.920 --> 00:56:08.159
I don't know if there's something 
interesting about that spectrum.

00:56:08.159 --> 00:56:10.639
I think there's something very 
interesting about that, 100%.

00:56:10.639 --> 00:56:16.480
I do think that humans have a lot more of 
an element, compared to LLMs, of seeing

00:56:16.480 --> 00:56:19.199
the forest for the trees.
We're not actually that good

00:56:19.199 --> 00:56:25.279
at memorization, which is actually a feature.
Because we're not that good at memorization, we're

00:56:25.920 --> 00:56:33.840
forced to find patterns in a more general sense.
LLMs in comparison are extremely good

00:56:33.840 --> 00:56:35.440
at memorization.
They will recite

00:56:35.440 --> 00:56:40.240
passages from all these training sources.
You can give them completely nonsensical data.

00:56:41.280 --> 00:56:44.640
You can hash some amount of text or something 
like that, you get a completely random sequence.

00:56:44.639 --> 00:56:48.639
If you train on it, even just for a single 
iteration or two, it can suddenly regurgitate

00:56:48.639 --> 00:56:51.679
the entire thing. It will memorize it. 
There's no way a person can read a single

00:56:51.679 --> 00:56:58.399
sequence of random numbers and recite it to you.
That's a feature, not a bug, because it forces

00:56:58.400 --> 00:57:03.360
you to only learn the generalizable components.
Whereas LLMs are distracted by all the memory

00:57:03.360 --> 00:57:06.559
that they have of the pre-training 
documents, and it's probably very

00:57:06.559 --> 00:57:10.559
distracting to them in a certain sense.
So that's why when I talk about the

00:57:10.559 --> 00:57:13.679
cognitive core, I want to remove the 
memory, which is what we talked about.

00:57:13.679 --> 00:57:17.599
I'd love to have them have less memory 
so that they have to look things up,

00:57:17.599 --> 00:57:22.799
and they only maintain the algorithms for 
thought, and the idea of an experiment,

00:57:22.800 --> 00:57:28.320
and all this cognitive glue of acting.
And this is also relevant to preventing

00:57:28.320 --> 00:57:35.039
model collapse?
Let me think. I'm

00:57:35.039 --> 00:57:40.320
not sure. It's almost like a separate axis.
The models are way too good at memorization,

00:57:40.320 --> 00:57:46.480
and somehow we should remove that.
People are much worse, but it's a good thing.

00:57:46.480 --> 00:57:50.719
What is a solution to model collapse?
There are very naive things you could attempt.

00:57:52.400 --> 00:57:55.760
The distribution over logits 
should be wider or something.

00:57:55.760 --> 00:57:58.480
There are many naive things you could try.
What ends up being the problem

00:57:58.480 --> 00:58:02.880
with the naive approaches?
That's a great question. You can imagine having

00:58:02.880 --> 00:58:06.800
a regularization for entropy and things like that.
I guess they just don't work as well empirically

00:58:06.800 --> 00:58:13.039
because right now the models are collapsed.
But I will say most of the tasks that we

00:58:13.039 --> 00:58:18.800
want from them don't actually demand diversity.
That’s probably the answer to what's going on.

00:58:20.000 --> 00:58:22.719
The frontier labs are trying 
to make the models useful.

00:58:22.719 --> 00:58:26.319
I feel like the diversity of 
the outputs is not so much...

00:58:26.320 --> 00:58:29.519
Number one, it's much harder to work with and 
evaluate and all this stuff, but maybe it's not

00:58:29.519 --> 00:58:33.519
what's capturing most of the value.
In fact, it's actively penalized.

00:58:34.159 --> 00:58:39.359
If you're super creative in RL, it's not good.
Yeah. Or maybe if you're doing a lot of writing,

00:58:39.360 --> 00:58:42.480
help from LLMs and stuff like that, it's probably 
bad because the models will silently give

00:58:42.480 --> 00:58:48.480
you all the same stuff.
They won't explore lots

00:58:48.480 --> 00:58:56.240
of different ways of answering a question.
Maybe this diversity, not as many applications

00:58:56.239 --> 00:58:58.399
need it so the models don't have it.
But then it's a problem at

00:58:58.400 --> 00:59:01.680
synthetic data generation time, et cetera.
So we're shooting ourselves in the foot by not

00:59:01.679 --> 00:59:06.879
allowing this entropy to maintain in the model.
Possibly the labs should try harder.

00:59:06.880 --> 00:59:11.360
I think you hinted that it's a very 
fundamental problem, it won't be easy

00:59:11.360 --> 00:59:17.039
to solve. What's your intuition for that?
I don't know if it's super fundamental.

00:59:17.039 --> 00:59:23.440
I don't know if I intended to say that.
I do think that I haven't done these experiments,

00:59:23.440 --> 00:59:26.880
but I do think that you could probably 
regularize the entropy to be higher.

00:59:26.880 --> 00:59:31.360
So you're encouraging the model to give you more 
and more solutions, but you don't want it to

00:59:31.360 --> 00:59:34.480
start deviating too much from the training data.
It's going to start making up its own language.

00:59:34.480 --> 00:59:38.480
It's going to start using words that are 
extremely rare, so it's going to drift too

00:59:38.480 --> 00:59:41.039
much from the distribution.
So I think controlling

00:59:41.039 --> 00:59:47.599
the distribution is just tricky.
It's probably not trivial in that sense.

00:59:47.599 --> 00:59:54.079
How many bits should the optimal core 
of intelligence end up being if you

00:59:54.079 --> 00:59:56.559
just had to make a guess?
The thing we put on the

00:59:56.559 --> 01:00:01.759
von Neumann probes, how big does it have to be?
It's really interesting in the history of the

01:00:01.760 --> 01:00:05.760
field because at one point everything was 
very scaling-pilled in terms of like, "Oh,

01:00:05.760 --> 01:00:08.880
we're gonna make much bigger models, 
trillions of parameter models."

01:00:08.880 --> 01:00:12.000
What the models have done in size 
is they've gone up and now they've

01:00:14.639 --> 01:00:19.359
come down. State-of-the-art models are smaller. 
Even then, I think they memorized way too much.

01:00:20.159 --> 01:00:24.639
So I had a prediction a while back that I almost 
feel like we can get cognitive cores that are

01:00:24.639 --> 01:00:30.960
very good at even a billion parameters.
If you talk to a billion parameter model,

01:00:30.960 --> 01:00:34.240
I think in 20 years, you can have 
a very productive conversation.

01:00:34.239 --> 01:00:39.279
It thinks and it's a lot more like a human.
But if you ask it some factual question, it might

01:00:39.280 --> 01:00:42.240
have to look it up, but it knows that it doesn't 
know and it might have to look it up and it will

01:00:42.239 --> 01:00:44.799
just do all the reasonable things.
That's surprising that you think

01:00:44.800 --> 01:00:47.519
it'll take a billion parameters.
Because already we have billion

01:00:47.519 --> 01:00:51.440
parameter models or a couple billion 
parameter models that are very intelligent.

01:00:51.440 --> 01:00:53.679
Well, state-of-the-art models 
are like a trillion parameters.

01:00:53.679 --> 01:00:58.719
But they remember so much stuff.
Yeah, but I'm surprised that in 10 years,

01:00:58.719 --> 01:01:07.039
given the pace… We have gpt-oss-20b.
That's way better than GPT-4 original,

01:01:07.039 --> 01:01:11.599
which was a trillion plus parameters.
Given that trend, I'm surprised you

01:01:11.599 --> 01:01:15.119
think in 10 years the cognitive 
core is still a billion parameters.

01:01:15.119 --> 01:01:20.000
I'm surprised you're not like, "Oh it's 
gonna be like tens of millions or millions."

01:01:22.159 --> 01:01:26.000
Here's the issue, the training data is 
the internet, which is really terrible.

01:01:26.000 --> 01:01:28.480
There's a huge amount of gains to be 
made because the internet is terrible.

01:01:29.519 --> 01:01:32.079
Even the internet, when you and I think of 
the internet, you're thinking of like The

01:01:32.079 --> 01:01:36.239
Wall Street Journal. That's not what this 
is. When you're looking at a pre-training

01:01:36.239 --> 01:01:40.719
dataset in the frontier lab and you look at a 
random internet document, it's total garbage.

01:01:40.719 --> 01:01:45.599
I don't even know how this works at all.
It's some like stock tickers, symbols,

01:01:46.559 --> 01:01:50.159
it's a huge amount of slop and garbage 
from like all the corners of the internet.

01:01:50.159 --> 01:01:53.199
It's not like your Wall Street Journal 
article, that's extremely rare.

01:01:53.920 --> 01:02:00.400
So because the internet is so terrible, we have 
to build really big models to compress all that.

01:02:00.400 --> 01:02:04.079
Most of that compression is memory 
work instead of cognitive work.

01:02:04.079 --> 01:02:06.960
But what we really want is the 
cognitive part, delete the memory.

01:02:08.239 --> 01:02:12.479
I guess what I'm saying is that we need 
intelligent models to help us refine even

01:02:12.480 --> 01:02:15.920
the pre-training set to just narrow 
it down to the cognitive components.

01:02:15.920 --> 01:02:18.400
Then I think you get away with a 
much smaller model because it's a

01:02:18.400 --> 01:02:22.240
much better dataset and you could train it on it.
But probably it's not trained directly on it, it's

01:02:22.239 --> 01:02:27.039
probably distilled from a much better model still.
But why is the distilled version still a billion?

01:02:28.559 --> 01:02:30.480
I just feel like distillation 
works extremely well.

01:02:30.480 --> 01:02:34.480
So almost every small model, if you have a 
small model, it's almost certainly distilled.

01:02:35.519 --> 01:02:39.679
Right, but why is the distillation in 
10 years not getting below 1 billion?

01:02:39.679 --> 01:02:43.519
Oh, you think it should be smaller than a 
billion? I mean, come on, right? I don't

01:02:43.519 --> 01:02:49.519
know. At some point it should take at least 
a billion knobs to do something interesting.

01:02:49.519 --> 01:02:53.199
You're thinking it should be even smaller?
Yeah. If you look at the trend over the last

01:02:53.199 --> 01:02:57.839
few years of just finding low-hanging fruit and 
going from trillion plus models to models that

01:02:57.840 --> 01:03:03.519
are literally two orders of magnitude smaller in a 
matter of two years and having better performance,

01:03:03.519 --> 01:03:09.360
it makes me think the sort of core of 
intelligence might be even way, way smaller.

01:03:09.360 --> 01:03:11.680
Plenty of room at the bottom, 
to paraphrase Feynman.

01:03:11.679 --> 01:03:14.639
I feel like I'm already contrarian 
by talking about a billion parameter

01:03:14.639 --> 01:03:23.359
cognitive core and you're outdoing me.
Maybe we could get a little bit smaller.

01:03:23.360 --> 01:03:26.400
I do think that practically speaking, you 
want the model to have some knowledge.

01:03:26.400 --> 01:03:30.079
You don't want it to be looking up everything 
because then you can't think in your head.

01:03:30.079 --> 01:03:31.519
You're looking up way too much stuff all the time.

01:03:32.719 --> 01:03:37.759
Some basic curriculum needs to be there for 
knowledge, but it doesn't have esoteric knowledge.

01:03:38.639 --> 01:03:41.359
We're discussing what plausibly 
could be the cognitive core.

01:03:41.360 --> 01:03:46.960
There's a separate question which is what 
will be the size of frontier models over time?

01:03:46.960 --> 01:03:51.840
I'm curious if you have predictions.
We had increasing scale up to maybe GPT 4.5 and

01:03:51.840 --> 01:03:56.720
now we're seeing decreasing or plateauing scale.
There are many reasons this could be going on.

01:03:56.719 --> 01:04:00.879
Do you have a prediction going forward?
Will the biggest models be bigger,

01:04:00.880 --> 01:04:06.079
will they be smaller, will they be the same?
I don't have a super strong prediction.

01:04:07.199 --> 01:04:10.960
The labs are just being practical.
They have a flops budget and a cost budget.

01:04:10.960 --> 01:04:14.240
It just turns out that pre-training is not where 
you want to put most of your flops or your cost.

01:04:14.239 --> 01:04:17.839
That's why the models have gotten smaller.
They are a bit smaller, the pre-training

01:04:17.840 --> 01:04:20.480
stage is smaller, but they make 
it up in reinforcement learning,

01:04:21.199 --> 01:04:25.279
mid-training, and all this stuff that follows.
They're just being practical in terms of all the

01:04:25.280 --> 01:04:30.640
stages and how you get the most bang for the buck.
Forecasting that trend is quite hard.

01:04:30.639 --> 01:04:35.679
I do still expect that there's so much 
low-hanging fruit. That's my basic expectation.

01:04:38.239 --> 01:04:42.479
I have a very wide distribution here.
Do you expect the low-hanging fruit to be

01:04:42.480 --> 01:04:48.400
similar in kind to the kinds of things that have 
been happening over the last two to five years?

01:04:49.519 --> 01:04:54.079
If I look at nanochat versus nanoGPT 
and the architectural tweaks you made,

01:04:54.079 --> 01:04:56.880
is that the flavor of things you 
expect to continue to keep happening?

01:04:58.000 --> 01:05:01.199
You're not expecting any giant paradigm shifts.
For the most part, yeah. I expect the

01:05:01.199 --> 01:05:03.759
datasets to get much, much better.
When you look at the average datasets,

01:05:03.760 --> 01:05:05.520
they're extremely terrible.
They’re so bad that I

01:05:05.519 --> 01:05:10.719
don't even know how anything works.
Look at the average example in the training set:

01:05:10.719 --> 01:05:15.839
factual mistakes, errors, nonsensical things.
Somehow when you do it at scale,

01:05:16.400 --> 01:05:21.119
the noise washes away and you're left with 
some of the signal. Datasets will improve

01:05:21.119 --> 01:05:29.279
a ton. Everything gets better. Our hardware, 
all the kernels for running the hardware and

01:05:29.280 --> 01:05:33.600
maximizing what you get with the hardware.
Nvidia is slowly tuning the hardware itself,

01:05:33.599 --> 01:05:36.960
Tensor Cores, all that needs to 
happen and will continue to happen.

01:05:36.960 --> 01:05:39.840
All the kernels will get better and 
utilize the chip to the max extent.

01:05:39.840 --> 01:05:45.120
All the algorithms will probably improve over 
optimization, architecture, and all the modeling

01:05:45.119 --> 01:05:48.880
components of how everything is done and what 
the algorithms are that we're even training with.

01:05:48.880 --> 01:05:59.280
I do expect that nothing dominates. Everything 
plus 20%. This is roughly what I've seen.

01:07:13.920 --> 01:07:21.440
People have proposed different ways of charting 
how much progress we've made towards full AGI.

01:07:21.440 --> 01:07:25.280
If you can come up with some line, then you 
can see where that line intersects with AGI

01:07:25.280 --> 01:07:29.920
and where that would happen on the x-axis.
People have proposed it's the education level.

01:07:29.920 --> 01:07:34.240
We had a high schooler, and then they went to 
college with RL, and they're going to get a Ph.D.

01:07:34.239 --> 01:07:36.799
I don't like that one.
Or they'll propose horizon

01:07:36.800 --> 01:07:41.760
length. Maybe they can do tasks that take 
a minute, they can do those autonomously.

01:07:41.760 --> 01:07:45.360
Then they can autonomously do tasks that take 
an hour, a human an hour, a human a week.

01:07:46.320 --> 01:07:53.120
How do you think about the relevant y-axis here?
How should we think about how

01:07:53.119 --> 01:07:55.920
AI is making progress?
I have two answers to that.

01:07:55.920 --> 01:07:59.360
Number one, I'm almost tempted to 
reject the question entirely because

01:07:59.360 --> 01:08:02.880
I see this as an extension of computing.
Have we talked about how to chart progress

01:08:02.880 --> 01:08:06.400
in computing, or how do you chart progress 
in computing since the 1970s or whatever?

01:08:06.400 --> 01:08:11.200
What is the y-axis? The whole question is 
funny from that perspective a little bit.

01:08:13.440 --> 01:08:18.960
When people talk about AI and the original AGI 
and how we spoke about it when OpenAI started,

01:08:18.960 --> 01:08:27.920
AGI was a system you could go to that can do any 
economically valuable task at human performance

01:08:27.920 --> 01:08:31.920
or better. That was the definition. I 
was pretty happy with that at the time.

01:08:32.880 --> 01:08:36.480
I've stuck to that definition forever, and 
then people have made up all kinds of other

01:08:36.479 --> 01:08:43.359
definitions. But I like that definition. The first 
concession that people make all the time is they

01:08:43.359 --> 01:08:48.159
just take out all the physical stuff because 
we're just talking about digital knowledge work.

01:08:48.159 --> 01:08:52.079
That's a pretty major concession compared to 
the original definition, which was any task

01:08:52.079 --> 01:08:57.680
a human can do. I can lift things, etc. AI 
can't do that, obviously, but we'll take it.

01:08:57.680 --> 01:09:02.720
What fraction of the economy are we taking away 
by saying, "Oh, only knowledge work?" I don't know

01:09:02.720 --> 01:09:09.920
the numbers. I feel about 10% to 20%, if I had to 
guess, is only knowledge work, someone could work

01:09:09.920 --> 01:09:14.880
from home and perform tasks, something like that.
It's still a really large market.

01:09:16.640 --> 01:09:19.360
What is the size of the 
economy, and what is 10% or 20%?

01:09:19.359 --> 01:09:26.079
We're still talking about a few trillion 
dollars, even in the US, of market share or work.

01:09:26.079 --> 01:09:30.159
So it's still a very massive bucket.
Going back to the definition,

01:09:30.159 --> 01:09:33.840
what I would be looking for is to 
what extent is that definition true?

01:09:35.199 --> 01:09:40.399
Are there jobs or lots of tasks?
If we think of tasks as not jobs but tasks.

01:09:40.399 --> 01:09:46.399
It's difficult because the problem is society will 
refactor based on the tasks that make up jobs,

01:09:47.439 --> 01:09:52.159
based on what's automatable or not.
Today, what jobs are replaceable by AI?

01:09:52.159 --> 01:09:57.119
A good example recently was Geoff Hinton's 
prediction that radiologists would not be

01:09:57.119 --> 01:10:00.000
a job anymore, and this turned out 
to be very wrong in a bunch of ways.

01:10:00.720 --> 01:10:04.240
Radiologists are alive and well and growing, 
even though computer vision is really,

01:10:04.239 --> 01:10:07.519
really good at recognizing all the different 
things that they have to recognize in images.

01:10:07.520 --> 01:10:11.600
It's just a messy, complicated job with a 
lot of surfaces and dealing with patients

01:10:11.600 --> 01:10:17.680
and all this stuff in the context of it.
I don't know that by that definition

01:10:17.680 --> 01:10:22.800
AI has made a huge dent yet.
Some of the jobs that I would

01:10:22.800 --> 01:10:27.119
be looking for have some features that make it 
very amenable to automation earlier than later.

01:10:27.119 --> 01:10:30.960
As an example, call center employees 
often come up, and I think rightly so.

01:10:30.960 --> 01:10:35.760
Call center employees have a number of simplifying 
properties with respect to what's automatable

01:10:35.760 --> 01:10:42.320
today. Their jobs are pretty simple. It's a 
sequence of tasks, and every task looks similar.

01:10:42.319 --> 01:10:45.759
You take a phone call with a person, it's 
10 minutes of interaction or whatever it is,

01:10:45.760 --> 01:10:48.960
probably a bit longer.
In my experience, a lot longer.

01:10:49.600 --> 01:10:53.280
You complete some task in some scheme, 
and you change some database entries

01:10:53.279 --> 01:10:55.599
around or something like that.
So you keep repeating something

01:10:55.600 --> 01:11:01.200
over and over again, and that's your job.
You do want to bring in the task horizon—how

01:11:01.199 --> 01:11:05.039
long it takes to perform a task—and 
then you want to also remove context.

01:11:05.039 --> 01:11:08.880
You're not dealing with different parts of 
services of companies or other customers.

01:11:08.880 --> 01:11:11.840
It's just the database, you, 
and a person you're serving.

01:11:11.840 --> 01:11:15.520
It's more closed, it's more 
understandable, it's purely digital.

01:11:15.520 --> 01:11:18.800
So I would be looking for those things.
But even there, I'm not looking

01:11:18.800 --> 01:11:21.760
at full automation yet.
I'm looking for an autonomy slider.

01:11:21.760 --> 01:11:25.600
I expect that we are not going 
to instantly replace people.

01:11:25.600 --> 01:11:29.039
We're going to be swapping in 
AIs that do 80% of the volume.

01:11:29.039 --> 01:11:33.359
They delegate 20% of the volume to humans, 
and humans are supervising teams of five AIs

01:11:33.359 --> 01:11:39.599
doing the call center work that's more rote.
I would be looking for new interfaces or new

01:11:39.600 --> 01:11:44.400
companies that provide some 
layer that allows you to manage

01:11:44.399 --> 01:11:48.879
some of these AIs that are not yet perfect.
Then I would expect that across the economy.

01:11:48.880 --> 01:11:51.520
A lot of jobs are a lot harder 
than a call center employee.

01:11:52.079 --> 01:11:56.319
With radiologists, I'm totally 
speculating and I have no idea what

01:11:56.319 --> 01:12:03.920
the actual workflow of a radiologist involves.
But one analogy that might be applicable is when

01:12:03.920 --> 01:12:09.279
Waymos were first being rolled out, there'd be a 
person sitting in the front seat, and you just had

01:12:09.279 --> 01:12:12.639
to have them there to make sure that if something 
went really wrong, they're there to monitor.

01:12:12.640 --> 01:12:15.440
Even today, people are still watching 
to make sure things are going well.

01:12:15.439 --> 01:12:19.119
Robotaxi, which was just deployed, 
still has a person inside it.

01:12:19.119 --> 01:12:25.359
Now we could be in a similar situation where 
if you automate 99% of a job, that last 1%

01:12:25.359 --> 01:12:29.759
the human has to do is incredibly valuable 
because it's bottlenecking everything else.

01:12:29.760 --> 01:12:35.039
If it were the case with radiologists, where 
the person sitting in the front of Waymo has

01:12:35.039 --> 01:12:39.199
to be specially trained for years in order 
to provide the last 1%, their wages should

01:12:39.199 --> 01:12:43.679
go up tremendously because they're the 
one thing bottlenecking wide deployment.

01:12:43.680 --> 01:12:46.640
Radiologists, I think their wages have 
gone up for similar reasons, if you're

01:12:46.640 --> 01:12:52.000
the last bottleneck and you're not fungible.
A Waymo driver might be fungible with others.

01:12:53.520 --> 01:12:57.840
So you might see this thing where your wages 
go up until you get to 99% and then fall just

01:12:57.840 --> 01:13:02.960
like that when the last 1% is gone.
And I wonder if we're seeing similar

01:13:02.960 --> 01:13:06.560
things with radiology or salaries of call 
center workers or anything like that.

01:13:07.439 --> 01:13:12.000
That's an interesting question. I don't think 
we're currently seeing that with radiology.

01:13:15.600 --> 01:13:19.120
I think radiology is not a good example.
I don't know why Geoff Hinton picked

01:13:19.119 --> 01:13:24.399
on radiology because I think it's an 
extremely messy, complicated profession.

01:13:25.039 --> 01:13:28.000
I would be a lot more interested in what's 
happening with call center employees today,

01:13:28.000 --> 01:13:32.640
for example, because I would expect a lot 
of the rote stuff to be automatable today.

01:13:32.640 --> 01:13:36.000
I don't have first-level access to it but 
I would be looking for trends of what's

01:13:36.000 --> 01:13:40.079
happening with the call center employees.
Some of the things I would also expect

01:13:40.079 --> 01:13:45.039
is that maybe they are swapping in AI, but 
then I would still wait for a year or two

01:13:45.039 --> 01:13:49.519
because I would potentially expect them to 
pull back and rehire some of the people.

01:13:49.520 --> 01:13:53.120
There's been evidence that that's already been 
happening generally in companies that have been

01:13:53.119 --> 01:13:59.840
adopting AI, which I think is quite surprising.
I also found what was really surprising. AGI,

01:13:59.840 --> 01:14:04.319
right? A thing which would do everything.
We'll take out physical work,

01:14:04.319 --> 01:14:09.039
but it should be able to do all knowledge work.
What you would have naively anticipated is that

01:14:09.039 --> 01:14:14.239
the way this progression would happen is 
that you take a little task that a consultant

01:14:14.239 --> 01:14:19.279
is doing, you take that out of the bucket.
You take a little task that an accountant is

01:14:19.279 --> 01:14:22.319
doing, you take that out of the bucket.
Then you're just doing this

01:14:22.319 --> 01:14:25.279
across all knowledge work.
But instead, if we do believe we're

01:14:25.279 --> 01:14:28.880
on the path of AGI with the current paradigm, 
the progression is very much not like that.

01:14:30.479 --> 01:14:34.000
It does not seem like consultants and accountants 
are getting huge productivity improvements.

01:14:34.000 --> 01:14:39.920
It's very much like programmers are getting 
more and more chiseled away at their work.

01:14:39.920 --> 01:14:46.800
If you look at the revenues of these companies, 
discounting normal chat revenue—which is similar

01:14:46.800 --> 01:14:51.680
to Google or something—just looking at 
API revenues, it's dominated by coding.

01:14:51.680 --> 01:14:56.159
So this thing which is "general", which 
should be able to do any knowledge work,

01:14:56.159 --> 01:15:00.239
is just overwhelmingly doing only coding.
It's a surprising way that you would

01:15:00.239 --> 01:15:04.719
expect the AGI to be deployed.
There's an interesting point

01:15:04.720 --> 01:15:12.400
here. I do believe coding is the perfect 
first thing for these LLMs and agents.

01:15:12.399 --> 01:15:17.039
That’s because coding has always 
fundamentally worked around text.

01:15:17.039 --> 01:15:20.560
It's computer terminals and text, 
and everything is based around text.

01:15:20.560 --> 01:15:24.480
LLMs, the way they're trained 
on the Internet, love text.

01:15:24.479 --> 01:15:28.719
They're perfect text processors, and there's 
all this data out there. It's a perfect fit.

01:15:29.520 --> 01:15:33.520
We also have a lot of infrastructure 
pre-built for handling code and text.

01:15:33.520 --> 01:15:41.040
For example, we have Visual Studio Code 
or your favorite IDE showing you code,

01:15:41.039 --> 01:15:45.279
and an agent can plug into that.
If an agent has a diff where it made some change,

01:15:45.279 --> 01:15:51.039
we suddenly have all this code already that shows 
all the differences to a code base using a diff.

01:15:51.039 --> 01:15:55.920
It's almost like we've pre-built a 
lot of the infrastructure for code.

01:15:55.920 --> 01:15:58.880
Contrast that with some of the 
things that don't enjoy that at all.

01:15:58.880 --> 01:16:03.840
As an example, there are people trying to build 
automation not for coding, but for slides.

01:16:03.840 --> 01:16:07.360
I saw a company doing slides. That's 
much, much harder. The reason it's

01:16:07.359 --> 01:16:11.839
much harder is because slides are not text.
Slides are little graphics, they're arranged

01:16:11.840 --> 01:16:18.880
spatially, and there's a visual component to it.
Slides don't have this pre-built infrastructure.

01:16:18.880 --> 01:16:24.319
For example, if an agent is to make a change to 
your slides, how does a thing show you the diff?

01:16:24.319 --> 01:16:26.960
How do you see the diff?
There's nothing that shows diffs

01:16:26.960 --> 01:16:34.399
for slides. Someone has to build it. Some of these 
things are not amenable to AIs as they are, which

01:16:34.399 --> 01:16:40.239
are text processors, and code surprisingly is.
I’m not sure that alone explains it.

01:16:42.159 --> 01:16:49.279
I personally have tried to get LLMs to be useful 
in domains which are just pure language-in,

01:16:49.279 --> 01:16:55.359
language-out, like rewriting transcripts, 
coming up with clips based on transcripts.

01:16:57.600 --> 01:17:00.720
It's very plausible that I didn't do 
every single possible thing I could do.

01:17:00.720 --> 01:17:05.920
I put a bunch of good examples in context, but 
maybe I should have done some kind of fine-tuning.

01:17:06.560 --> 01:17:13.680
Our mutual friend, Andy Matuschak, told me that 
he tried 50 billion things to try to get models

01:17:13.680 --> 01:17:19.440
to be good at writing spaced repetition prompts.
Again, very much language-in, language-out tasks,

01:17:19.439 --> 01:17:22.559
the kind of thing that should be dead 
center in the repertoire of these LLMs.

01:17:22.560 --> 01:17:25.920
He tried in-context learning 
with a few-shot examples.

01:17:25.920 --> 01:17:35.520
He tried supervised fine-tuning and retrieval.
He could not get them to make

01:17:35.520 --> 01:17:39.760
cards to his satisfaction.
So I find it striking that even in language-out

01:17:39.760 --> 01:17:45.600
domains, it's very hard to get a lot of economic 
value out of these models separate from coding.

01:17:45.600 --> 01:17:52.000
I don't know what explains it.
That makes sense. I'm not

01:17:52.000 --> 01:17:57.279
saying that anything text is trivial.
I do think that code is pretty structured.

01:17:58.720 --> 01:18:04.880
Text is maybe a lot more flowery, and there's 
a lot more entropy in text, I would say.

01:18:04.880 --> 01:18:10.400
I don't know how else to put it.
Also code is hard, and so people feel quite

01:18:10.399 --> 01:18:19.279
empowered by LLMs, even from simple knowledge.
I don't know that I have a very good answer.

01:18:19.279 --> 01:18:24.319
Obviously, text makes it much, much easier, 
but it doesn't mean that all text is trivial.

01:18:25.199 --> 01:18:29.760
How do you think about superintelligence?
Do you expect it to feel qualitatively different

01:18:29.760 --> 01:18:37.039
from normal humans or human companies?
I see it as a progression

01:18:37.039 --> 01:18:42.319
of automation in society.
Extrapolating the trend of computing, there will

01:18:42.319 --> 01:18:46.639
be a gradual automation of a lot of things, and 
superintelligence will an extrapolation of that.

01:18:47.600 --> 01:18:50.560
We expect more and more autonomous 
entities over time that are doing a lot

01:18:50.560 --> 01:18:56.080
of the digital work and then eventually even 
the physical work some amount of time later.

01:18:56.079 --> 01:19:00.559
Basically I see it as just 
automation, roughly speaking.

01:19:00.560 --> 01:19:03.120
But automation includes the things humans 
can already do, and superintelligence

01:19:03.119 --> 01:19:05.920
implies things humans can’t do.
But one of the things that people

01:19:05.920 --> 01:19:09.920
do is invent new things, which I would just 
put into the automation if that makes sense.

01:19:10.960 --> 01:19:18.399
But I guess, less abstractly and more 
qualitatively, do you expect something

01:19:18.399 --> 01:19:26.799
to feel like… Because this thing can either think 
so fast, or has so many copies, or the copies can

01:19:26.800 --> 01:19:35.199
merge back into themselves, or is much smarter, 
any number of advantages an AI might have, will

01:19:36.000 --> 01:19:39.920
the civilization in which these AIs exist will 
just feel qualitatively different from humans?

01:19:39.920 --> 01:19:44.960
I think it will. It is fundamentally automation, 
but it will be extremely foreign. It will look

01:19:44.960 --> 01:19:51.680
really strange. Like you mentioned, we can run 
all of this on a computer cluster and much faster.

01:19:53.039 --> 01:19:58.399
Some of the scenarios that I start to get 
nervous about when the world looks like

01:19:58.399 --> 01:20:01.759
that is this gradual loss of control 
and understanding of what's happening.

01:20:01.760 --> 01:20:06.079
I think that's the most likely outcome, that 
there will be a gradual loss of understanding.

01:20:07.520 --> 01:20:10.400
We'll gradually layer all this stuff 
everywhere, and there will be fewer

01:20:10.399 --> 01:20:14.639
and fewer people who understand it.
Then there will be a gradual loss of

01:20:14.640 --> 01:20:19.360
control and understanding of what's happening.
That to me seems the most likely outcome of how

01:20:19.359 --> 01:20:22.000
all this stuff will go down.
Let me probe on that a bit.

01:20:22.000 --> 01:20:27.520
It's not clear to me that loss of control and 
loss of understanding are the same things.

01:20:27.520 --> 01:20:37.040
A board of directors at TSMC, Intel—name a random 
company—they're just prestigious 80-year-olds.

01:20:37.039 --> 01:20:40.800
They have very little understanding, and maybe 
they don't practically actually have control.

01:20:43.199 --> 01:20:43.760
A better example

01:20:43.760 --> 01:20:47.600
is the President of the United States.
The President has a lot of fucking power.

01:20:48.800 --> 01:20:53.039
I'm not trying to make a good statement 
about the current operant, or maybe I am,

01:20:53.039 --> 01:20:56.479
but the actual level of understanding is 
very different from the level of control.

01:20:56.479 --> 01:21:05.199
I think that's fair. That's a good 
pushback. I think I expect loss of both.

01:21:05.199 --> 01:21:08.880
How come? Loss of understanding is 
obvious, but why loss of control?

01:21:10.159 --> 01:21:14.399
We're really far into a territory where I 
don't know what this looks like, but if I

01:21:14.399 --> 01:21:22.000
were to write sci-fi novels, they would look along 
the lines of not even a single entity that takes

01:21:22.000 --> 01:21:26.479
over everything, but multiple competing entities 
that gradually become more and more autonomous.

01:21:27.359 --> 01:21:30.000
Some of them go rogue and 
the others fight them off.

01:21:31.039 --> 01:21:37.600
It's this hot pot of completely autonomous 
activity that we've delegated to.

01:21:37.600 --> 01:21:43.600
I feel it would have that flavor.
It is not the fact that they are smarter

01:21:43.600 --> 01:21:47.840
than us that is resulting in the loss of control.
It's the fact that they are competing with each

01:21:47.840 --> 01:21:54.400
other, and whatever arises out of that 
competition leads to the loss of control.

01:21:58.960 --> 01:22:04.960
A lot of these things, they will be 
tools to people, they're acting on

01:22:04.960 --> 01:22:07.439
behalf of people or something like that.
So maybe those people are in control,

01:22:07.439 --> 01:22:12.239
but maybe it's a loss of control overall for 
society in the sense of outcomes we want.

01:22:13.840 --> 01:22:19.360
You have entities acting on behalf of individuals 
that are still roughly seen as out of control.

01:22:20.079 --> 01:22:24.000
This is a question I should have asked earlier.
We were talking about how currently it feels like

01:22:24.000 --> 01:22:29.199
when you're doing AI engineering or AI research, 
these models are more in the category of compiler

01:22:29.199 --> 01:22:34.639
rather than in the category of a replacement.
At some point, if you have AGI,

01:22:34.640 --> 01:22:38.720
it should be able to do what you do.
Do you feel like having a million

01:22:38.720 --> 01:22:43.039
copies of you in parallel results in 
some huge speed-up of AI progress?

01:22:43.760 --> 01:22:49.355
If that does happen, do you expect to see an 
intelligence explosion once we have a true AGI?

01:22:49.355 --> 01:22:56.079
I'm not talking about LLMs today.
I do, but it's business as usual because

01:22:56.079 --> 01:22:59.039
we're in an intelligence explosion 
already and have been for decades.

01:23:00.000 --> 01:23:03.600
It's basically the GDP curve that is 
an exponential weighted sum over so

01:23:03.600 --> 01:23:06.160
many aspects of the industry.
Everything is gradually being

01:23:06.159 --> 01:23:10.399
automated and has been for hundreds of years.
The Industrial Revolution is automation and

01:23:10.399 --> 01:23:13.039
some of the physical components and 
tool building and all this stuff.

01:23:13.039 --> 01:23:16.720
Compilers are early software 
automation, et cetera.

01:23:16.720 --> 01:23:21.760
We've been recursively self-improving 
and exploding for a long time.

01:23:21.760 --> 01:23:27.440
Another way to see it is that Earth was a pretty 
boring place if you don't look at the biomechanics

01:23:27.439 --> 01:23:33.439
and so on, and looked very similar.
If you look from space, we're in the

01:23:33.439 --> 01:23:37.519
middle of this firecracker event, 
but we're seeing it in slow motion.

01:23:38.319 --> 01:23:42.799
I definitely feel like this has 
already happened for a very long time.

01:23:42.800 --> 01:23:47.199
Again, I don't see AI as a distinct 
technology with respect to what has

01:23:47.199 --> 01:23:50.800
already been happening for a long time.
You think it's continuous with this

01:23:50.800 --> 01:23:53.840
hyper-exponential trend?
Yes. That's why this was

01:23:53.840 --> 01:23:57.520
very interesting to me, because I was 
trying to find AI in the GDP for a while.

01:23:57.520 --> 01:24:01.120
I thought that GDP should go up.
But then I looked at some of the

01:24:01.119 --> 01:24:04.079
other technologies that I thought 
were very transformative, like

01:24:04.720 --> 01:24:08.480
computers or mobile phones or et cetera.
You can't find them in GDP. GDP is the same

01:24:08.479 --> 01:24:13.839
exponential. Even the early iPhone didn't have the 
App Store, and it didn't have a lot of the bells

01:24:13.840 --> 01:24:18.000
and whistles that the modern iPhone has.
So even though we think of 2008,

01:24:18.000 --> 01:24:21.520
when the iPhone came out, as this major 
seismic change, it's actually not.

01:24:21.520 --> 01:24:25.360
Everything is so spread out and it so 
slowly diffuses that everything ends up

01:24:25.359 --> 01:24:28.719
being averaged up into the same exponential.
It's the exact same thing with computers.

01:24:28.720 --> 01:24:31.440
You can't find them in the GDP 
like, "Oh, we have computers now."

01:24:31.439 --> 01:24:33.759
That's not what happened, because 
it's such slow progression.

01:24:33.760 --> 01:24:37.600
With AI we're going to see the exact same thing. 
It's just more automation. It allows us to write

01:24:37.600 --> 01:24:41.840
different kinds of programs that we couldn't write 
before, but AI is still fundamentally a program.

01:24:42.560 --> 01:24:46.880
It's a new kind of computer and 
a new kind of computing system.

01:24:46.880 --> 01:24:49.840
But it has all these problems, 
it's going to diffuse over time,

01:24:49.840 --> 01:24:52.079
and it's still going to add 
up to the same exponential.

01:24:52.079 --> 01:24:56.399
We're still going to have an exponential 
that's going to get extremely vertical.

01:24:56.399 --> 01:24:59.519
It's going to be very foreign to 
live in that kind of an environment.

01:24:59.520 --> 01:25:05.600
Are you saying that, if you look at the trend 
before the Industrial Revolution to now,

01:25:05.600 --> 01:25:11.039
you have a hyper-exponential where you go 
from 0% growth to then 10,000 years ago,

01:25:11.039 --> 01:25:15.279
0.02% growth, and to now when we're at 2% 
growth. That's a hyper-exponential. Are you

01:25:15.279 --> 01:25:20.159
saying if you're charting AI on there, then 
AI takes you to 20% growth or 200% growth?

01:25:20.159 --> 01:25:23.599
Or are you saying that if you look at 
the last 300 years, what you've been

01:25:23.600 --> 01:25:27.200
seeing is that you have technology after 
technology—computers, electrification,

01:25:27.199 --> 01:25:33.519
steam engines, railways, et cetera—but the 
rate of growth is the exact same, it's 2%.

01:25:33.520 --> 01:25:38.880
Are you saying the rate of growth will go up?
The rate of growth has also stayed

01:25:38.880 --> 01:25:41.520
roughly constant, right?
Only over the last 200, 300 years.

01:25:41.520 --> 01:25:44.320
But over the course of 
human history it's exploded.

01:25:44.319 --> 01:25:49.840
It's gone from 0% to faster, faster, 
faster. Industrial explosion, 2%.

01:25:51.119 --> 01:25:54.479
For a while I tried to find AI 
or look for AI in the GDP curve,

01:25:54.479 --> 01:25:58.239
and I've convinced myself that this is false.
Even when people talk about recursive

01:25:58.239 --> 01:26:01.679
self-improvement and labs and stuff 
like that, this is business as usual.

01:26:01.680 --> 01:26:05.680
Of course it's going to recursively self-improve, 
and it's been recursively self-improving.

01:26:05.680 --> 01:26:11.119
LLMs allow the engineers to work much more 
efficiently to build the next round of LLM,

01:26:11.119 --> 01:26:14.880
and a lot more of the components are 
being automated and tuned and et cetera.

01:26:14.880 --> 01:26:19.119
All the engineers having access 
to Google Search is part of it.

01:26:19.119 --> 01:26:23.039
All the engineers having an IDE, all of them 
having autocomplete or having Claude code,

01:26:23.039 --> 01:26:30.880
et cetera, it's all just part of the same 
speed-up of the whole thing. It's just so smooth.

01:26:30.880 --> 01:26:33.840
Just to clarify, you're saying that 
the rate of growth will not change.

01:26:35.439 --> 01:26:39.439
The intelligence explosion will show up as 
it just enabled us to continue staying on the

01:26:39.439 --> 01:26:42.559
2% growth trajectory, just as the Internet 
helped us stay on the 2% growth trajectory.

01:26:42.560 --> 01:26:45.520
Yes, my expectation is that 
it stays in the same pattern.

01:26:47.920 --> 01:26:55.760
Just to throw the opposite argument against you, 
my expectation is that it blows up because I think

01:26:55.760 --> 01:27:00.800
true AGI—and I'm not talking about LLM coding 
bots, I'm talking about actual replacement of a

01:27:00.800 --> 01:27:07.279
human in a server—is qualitatively different 
from these other productivity-improving

01:27:07.279 --> 01:27:13.119
technologies because it's labor itself.
I think we live in a very labor-constrained world.

01:27:13.119 --> 01:27:17.199
If you talk to any startup founder or any person, 
you can be like, what do you need more of? You

01:27:17.199 --> 01:27:22.800
need really talented people. And if you have 
billions of extra people who are inventing stuff,

01:27:22.800 --> 01:27:28.400
integrating themselves, making companies bottom 
start to finish, that feels qualitatively

01:27:28.399 --> 01:27:32.399
different from a single technology.
It's as if you get 10 billion

01:27:32.399 --> 01:27:37.759
extra people on the planet.
Maybe a counterpoint. I'm pretty willing

01:27:37.760 --> 01:27:42.000
to be convinced one way or another on this point.
But I will say, for example, computing is labor.

01:27:42.000 --> 01:27:45.520
Computing was labor. Computers, a lot 
of jobs disappeared because computers

01:27:45.520 --> 01:27:50.240
are automating a bunch of digital information 
processing that you now don't need a human for.

01:27:50.239 --> 01:27:57.840
So computers are labor, and that has played out.
Self-driving as an example is also computers doing

01:27:57.840 --> 01:28:02.079
labor. That's already been playing 
out. It's still business as usual.

01:28:02.800 --> 01:28:08.000
You have a machine which is spitting out more 
things like that at potentially faster pace.

01:28:08.000 --> 01:28:11.119
Historically, we have examples 
of the growth regime changing

01:28:11.119 --> 01:28:18.399
where you went from 0.2% growth to 2% growth.
It seems very plausible to me that a machine which

01:28:18.399 --> 01:28:23.199
is then spitting out the next self-driving 
car and the next Internet and whatever…

01:28:23.199 --> 01:28:27.519
I see where it's coming from.
At the same time, I do feel like

01:28:27.520 --> 01:28:31.840
people make this assumption of, "We 
have God in a box, and now it can do

01:28:31.840 --> 01:28:36.239
everything," and it just won't look like that.
It's going to be able to do some of the things.

01:28:36.239 --> 01:28:39.359
It's going to fail at some other things.
It's going to be gradually put into society,

01:28:39.359 --> 01:28:43.920
and we'll end up with the same pattern. That 
is my prediction. This assumption of suddenly

01:28:43.920 --> 01:28:49.199
having a completely intelligent, fully flexible, 
fully general human in a box, and we can dispense

01:28:49.199 --> 01:28:55.840
it at arbitrary problems in society, I don't 
think that we will have this discrete change.

01:28:57.600 --> 01:29:02.880
I think we'll arrive at the same kind of 
gradual diffusion of this across the industry.

01:29:03.439 --> 01:29:08.079
It often ends up being misleading 
in these conversations.

01:29:09.279 --> 01:29:12.960
I don't like to use the word intelligence in 
this context because intelligence implies you

01:29:12.960 --> 01:29:18.880
think there'll be a single superintelligence 
sitting in a server and it'll divine how

01:29:18.880 --> 01:29:22.800
to come up with new technologies and 
inventions that cause this explosion.

01:29:22.800 --> 01:29:25.840
That's not what I'm imagining 
when I'm imagining 20% growth.

01:29:25.840 --> 01:29:33.279
I'm imagining that there are billions of 
very smart human-like minds, potentially,

01:29:33.279 --> 01:29:36.479
or that's all that's required.
But the fact that there's hundreds

01:29:36.479 --> 01:29:41.759
of millions of them, billions of them, each 
individually making new products, figuring

01:29:41.760 --> 01:29:46.640
out how to integrate themselves into the economy.
If a highly experienced smart immigrant came to

01:29:46.640 --> 01:29:49.360
the country, you wouldn't need to figure out how 
we integrate them in the economy. They figure it

01:29:49.359 --> 01:29:55.439
out. They could start a company, they could make 
inventions, or increase productivity in the world.

01:29:55.439 --> 01:30:01.439
We have examples, even in the current regime, 
of places that have had 10-20% economic growth.

01:30:01.439 --> 01:30:05.839
If you just have a lot of people and 
less capital in comparison to the people,

01:30:05.840 --> 01:30:12.079
you can have Hong Kong or Shenzhen or 
whatever with decades of 10% plus growth.

01:30:13.119 --> 01:30:17.680
There's a lot of really smart people who are 
ready to make use of the resources and do

01:30:17.680 --> 01:30:22.480
this period of catch-up because we've had this 
discontinuity, and I think AI might be similar.

01:30:24.479 --> 01:30:28.079
I understand, but I still think that 
you're presupposing some discrete jump.

01:30:28.079 --> 01:30:31.439
There's some unlock that we're waiting to claim.
And suddenly we're going to have

01:30:31.439 --> 01:30:34.960
geniuses in data centers.
I still think you're presupposing

01:30:34.960 --> 01:30:39.600
some discrete jump that has no historical 
precedent that I can't find in any of the

01:30:39.600 --> 01:30:43.680
statistics and that I think probably won't happen.
I mean, the Industrial Revolution is such a jump.

01:30:43.680 --> 01:30:49.440
You went from 0.2% growth to 2% growth.
I'm just saying you'll see another jump like that.

01:30:49.439 --> 01:30:53.759
I'm a little bit suspicious, 
I would have to take a look.

01:30:53.760 --> 01:30:58.079
For example, some of the logs are not very 
good from before the Industrial Revolution.

01:30:59.920 --> 01:31:04.640
I'm a bit suspicious of it but 
I don't have strong opinions.

01:31:04.640 --> 01:31:07.440
You're saying that this was a singular 
event that was extremely magical.

01:31:07.439 --> 01:31:09.279
You're saying that maybe there's going 
to be another event that's going to

01:31:09.279 --> 01:31:12.960
be just like that, extremely magical.
It will break the paradigm, and so on.

01:31:12.960 --> 01:31:17.119
I actually don't think… The crucial thing with the 
Industrial Revolution was that it was not magical.

01:31:18.479 --> 01:31:27.679
If you just zoomed in, what you would see in 1770 
or 1870 is not that there was some key invention.

01:31:28.399 --> 01:31:32.960
But at the same time, you did move the 
economy to a regime where the progress

01:31:32.960 --> 01:31:37.840
was much faster and the exponential 10x'd.
I expect a similar thing from AI where it's

01:31:37.840 --> 01:31:42.470
not like there's going to be a single moment 
where we've made the crucial invention.

01:31:42.470 --> 01:31:45.760
It’s an overhang that's being unlocked.
Like maybe there's a new energy source.

01:31:45.760 --> 01:31:49.760
There's some unlock—in this case, some kind of 
a cognitive capacity—and there's an overhang of

01:31:49.760 --> 01:31:52.159
cognitive work to do.
That's right.

01:31:52.159 --> 01:31:55.760
You're expecting that overhang to be filled by 
this new technology when it crosses the threshold.

01:31:56.640 --> 01:32:01.119
Maybe one way to think about it is 
throughout history, a lot of growth

01:32:01.119 --> 01:32:06.399
comes because people come up with ideas, 
and then people are out there doing stuff to

01:32:06.399 --> 01:32:11.039
execute those ideas and make valuable output.
Through most of this time, the population has

01:32:11.039 --> 01:32:14.159
been exploding. That has been driving 
growth. For the last 50 years, people

01:32:14.159 --> 01:32:17.519
have argued that growth has stagnated.
The population in frontier countries

01:32:17.520 --> 01:32:19.840
has also stagnated.
I think we go back to

01:32:19.840 --> 01:32:27.039
the exponential growth in population that 
causes hyper-exponential growth in output.

01:32:28.960 --> 01:32:32.159
It's really hard to tell. I 
understand that viewpoint. I

01:32:32.159 --> 01:33:40.559
don't intuitively feel that viewpoint.
You recommended Nick Lane's book to me.

01:33:40.560 --> 01:33:44.400
On that basis, I also found it super 
interesting and I interviewed him.

01:33:45.199 --> 01:33:49.119
I have some questions about thinking about 
intelligence and evolutionary history.

01:33:49.119 --> 01:33:54.399
Now that you, over the last 20 years of doing AI 
research, you maybe have a more tangible sense of

01:33:54.399 --> 01:34:01.679
what intelligence is, what it takes to develop it.
Are you more or less surprised as a result that

01:34:01.680 --> 01:34:12.159
evolution just spontaneously stumbled upon it?
I love Nick Lane's books. I was just listening

01:34:12.159 --> 01:34:15.279
to his podcast on the way up here.
With respect to intelligence and its

01:34:15.279 --> 01:34:22.239
evolution, it's very, very recent.
I am surprised that it evolved.

01:34:23.039 --> 01:34:25.119
I find it fascinating to think 
about all the worlds out there.

01:34:25.119 --> 01:34:27.439
Say there's a thousand planets 
like Earth and what they look like.

01:34:27.439 --> 01:34:29.599
I think Nick Lane was here talking 
about some of the earliest parts.

01:34:30.880 --> 01:34:34.079
He expects very similar life 
forms, roughly speaking,

01:34:34.079 --> 01:34:38.479
and bacteria-like things in most of them.
There are a few breaks in there.

01:34:39.920 --> 01:34:43.920
The evolution of intelligence intuitively feels 
to me like it should be a fairly rare event.

01:34:45.920 --> 01:34:49.279
Maybe you should base it on 
how long something has existed.

01:34:49.279 --> 01:34:52.880
If bacteria were around for 2 billion years 
and nothing happened, then going to eukaryote

01:34:52.880 --> 01:35:00.000
is probably pretty hard because bacteria came 
up quite early in Earth's evolution or history.

01:35:02.239 --> 01:35:04.719
How long have we had animals?
Maybe a couple hundred million years,

01:35:04.720 --> 01:35:08.320
multicellular animals that 
run around, crawl, et cetera.

01:35:08.319 --> 01:35:15.840
That’s maybe 10% of Earth's lifespan.
Maybe on that timescale it's not too tricky.

01:35:18.000 --> 01:35:20.479
It's still surprising to me, 
intuitively, that it developed.

01:35:20.479 --> 01:35:24.639
I would maybe expect just a lot of animal-like 
life forms doing animal-like things.

01:35:24.640 --> 01:35:28.079
The fact that you can get something 
that creates culture and knowledge

01:35:28.079 --> 01:35:35.439
and accumulates it is surprising to me.
There's a couple of interesting follow-ups.

01:35:35.439 --> 01:35:41.919
If you buy the Sutton perspective that the 
crux of intelligence is animal intelligence…

01:35:41.920 --> 01:35:45.119
The quote he said is "If you got to the 
squirrel, you'd be most of the way to AGI."

01:35:46.560 --> 01:35:51.200
We got to squirrel intelligence right after 
the Cambrian explosion 600 million years ago.

01:35:51.199 --> 01:35:56.000
It seems like what instigated that was the 
oxygenation event 600 million years ago.

01:35:56.000 --> 01:36:01.600
But immediately the intelligence algorithm 
was there to make the squirrel intelligence.

01:36:02.479 --> 01:36:06.239
It's suggestive that animal 
intelligence was like that.

01:36:06.239 --> 01:36:09.119
As soon as you had the oxygen in the 
environment, you had the eukaryote,

01:36:09.119 --> 01:36:14.399
you could just get the algorithm.
Maybe it was an accident that

01:36:14.399 --> 01:36:17.439
evolution stumbled upon it so fast, 
but I don't know if that suggests that

01:36:18.479 --> 01:36:23.199
at the end it's going to be quite simple.
It's so hard to tell with any of this stuff.

01:36:23.199 --> 01:36:26.159
You can base it a bit on how long 
something has existed or how long

01:36:26.159 --> 01:36:30.239
it feels like something has been bottlenecked.
Nick Lane is very good about describing this very

01:36:30.239 --> 01:36:34.239
apparent bottleneck in bacteria and archaea.
For two billion years, nothing happened.

01:36:34.239 --> 01:36:41.199
There’s extreme diversity of biochemistry, 
and yet nothing grows to become animals.

01:36:41.199 --> 01:36:46.399
Two billion years. I don't know that we've 
seen exactly that kind of an equivalent with

01:36:46.399 --> 01:36:51.199
animals and intelligence, to your point.
We could also look at it with respect

01:36:51.199 --> 01:36:55.599
to how many times we think certain 
intelligence has individually sprung up.

01:36:55.600 --> 01:37:03.120
That's a really good thing to investigate.
One thought on that. There's hominid intelligence,

01:37:03.119 --> 01:37:07.840
and then there's bird intelligence.
Ravens, etc., are extremely clever,

01:37:07.840 --> 01:37:13.760
but their brain parts are quite distinct, 
and we don't have that much in common.

01:37:13.760 --> 01:37:18.880
That's a slight indication of maybe 
intelligence springing up a few times.

01:37:18.880 --> 01:37:26.560
In that case, you'd expect it more frequently.
A former guest, Gwern, and Carl Shulman, they’ve

01:37:26.560 --> 01:37:32.720
made a really interesting point about that.
Their perspective is that the scalable algorithm

01:37:32.720 --> 01:37:39.600
which humans have and primates have, arose in 
birds as well, and maybe other times as well.

01:37:39.600 --> 01:37:46.079
But humans found an evolutionary niche which 
rewarded marginal increases in intelligence

01:37:47.279 --> 01:37:52.319
and also had a scalable brain algorithm that 
could achieve those increases in intelligence.

01:37:53.439 --> 01:37:57.199
For example, if a bird had a bigger brain, 
it would just collapse out of the air.

01:37:57.199 --> 01:38:00.559
It's very smart for the size of 
its brain, but it's not in a niche

01:38:00.560 --> 01:38:06.080
which rewards the brain getting bigger.
It’s maybe similar to some really smart…

01:38:08.880 --> 01:38:10.159
Like dolphins?
Exaclty, humans, we have hands that

01:38:10.159 --> 01:38:14.319
reward being able to learn how to do tool use.
We can externalize digestion, more energy to

01:38:14.319 --> 01:38:19.439
the brain, and that kicks off the flywheel.
Also stuff to work with. I'm guessing it would

01:38:19.439 --> 01:38:28.079
be harder if I were a dolphin. How do you have 
fire? The universe of things you can do in water,

01:38:28.880 --> 01:38:32.960
inside water, is probably lower than 
what you can do on land, just chemically.

01:38:33.840 --> 01:38:38.640
I do agree with this viewpoint of these niches 
and what's being incentivized. I still find it

01:38:38.640 --> 01:38:45.920
miraculous. I would have expected things to 
get stuck on animals with bigger muscles.

01:38:47.039 --> 01:38:51.519
Going through intelligence is a 
really fascinating breaking point.

01:38:51.520 --> 01:38:56.160
The way Gwern put it is the reason it was so hard 
is that it's a very tight line between being in

01:38:56.159 --> 01:39:03.199
a situation where something is so important 
to learn that it's not worth distilling the

01:39:03.199 --> 01:39:10.000
exact right circuits directly back into your DNA, 
versus it's not important enough to learn at all.

01:39:10.000 --> 01:39:17.199
It has to be something that incentivizes 
building the algorithm to learn in a lifetime.

01:39:17.199 --> 01:39:21.840
You have to incentivize some kind of adaptability.
You want environments that are unpredictable

01:39:21.840 --> 01:39:24.640
so evolution can't bake your 
algorithms into your weights.

01:39:24.640 --> 01:39:30.000
A lot of animals are pre-baked in this sense.
Humans have to figure it out at test

01:39:30.000 --> 01:39:35.359
time when they get born.
You want these environments

01:39:35.359 --> 01:39:40.079
that change really rapidly, where you 
can't foresee what will work well.

01:39:42.000 --> 01:39:44.079
You create intelligence to 
figure it out at test time.

01:39:45.199 --> 01:39:48.399
Quintin Pope had this interesting blog post 
where he's saying the reason he doesn't

01:39:48.399 --> 01:39:55.279
expect a sharp takeoff is that humans had the 
sharp takeoff where 60,000 years ago we seem

01:39:55.279 --> 01:39:59.759
to have had the cognitive architectures 
that we have today. 10,000 years ago,

01:39:59.760 --> 01:40:04.560
agricultural revolution, modernity.
What was happening in that 50,000 years?

01:40:04.560 --> 01:40:10.960
You had to build this cultural scaffold where 
you can accumulate knowledge over generations.

01:40:11.520 --> 01:40:16.386
This is an ability that exists for 
free in the way we do AI training.

01:40:16.386 --> 01:40:21.520
In many cases they are literally distilled.
If you retrain a model, they can be trained

01:40:21.520 --> 01:40:24.800
on each other, they can be trained 
on the same pre-training corpus,

01:40:25.359 --> 01:40:31.279
they don't literally have to start from scratch.
There's a sense in which it took humans a long

01:40:31.279 --> 01:40:35.519
time to get this cultural loop going, but it just 
comes for free with the way we do LLM training.

01:40:36.239 --> 01:40:39.439
Yes and no. Because LLMs don't really 
have the equivalent of culture.

01:40:39.439 --> 01:40:42.000
Maybe we're giving them way too 
much and incentivizing not to

01:40:42.000 --> 01:40:45.119
create it or something like that.
But the invention of culture and of

01:40:45.119 --> 01:40:48.800
written record and of passing down notes 
between each other, I don't think there's

01:40:48.800 --> 01:40:53.199
an equivalent of that with LLMs right now.
LLMs don't really have culture right now and

01:40:53.199 --> 01:40:58.399
it's one of the impediments I would say.
Can you give me some sense of what

01:40:58.399 --> 01:41:01.839
LLM culture might look like?
In the simplest case it would be a

01:41:01.840 --> 01:41:06.880
giant scratchpad that the LLM can edit and as it's 
reading stuff or as it's helping out with work,

01:41:06.880 --> 01:41:10.560
it's editing the scratchpad for itself.
Why can't an LLM write a book for the other

01:41:10.560 --> 01:41:16.240
LLMs? That would be cool. Why can't other 
LLMs read this LLM's book and be inspired

01:41:16.239 --> 01:41:20.399
by it or shocked by it or something like that?
There's no equivalence for any of this stuff.

01:41:20.399 --> 01:41:23.279
Interesting. When would you expect 
that kind of thing to start happening?

01:41:24.640 --> 01:41:31.119
Also, multi-agent systems and a sort of 
independent AI civilization and culture?

01:41:31.119 --> 01:41:34.399
There are two powerful ideas in the 
realm of multi-agent that have both

01:41:34.399 --> 01:41:38.639
not been really claimed or so on.
The first one I would say is culture

01:41:38.640 --> 01:41:44.320
and LLMs having a growing repertoire 
of knowledge for their own purposes.

01:41:44.319 --> 01:41:47.519
The second one looks a lot more 
like the powerful idea of self-play.

01:41:47.520 --> 01:41:53.040
In my mind it’s extremely powerful.
Evolution has a lot of competition

01:41:53.039 --> 01:41:59.600
driving intelligence and evolution.
In AlphaGo more algorithmically,

01:41:59.600 --> 01:42:03.760
AlphaGo is playing against itself and that's 
how it learns to get really good at Go.

01:42:03.760 --> 01:42:07.520
There's no equivalent of self-playing LLMs, 
but I would expect that to also exist.

01:42:07.520 --> 01:42:10.640
No one has done it yet.
Why can't an LLM for example, create a bunch

01:42:10.640 --> 01:42:16.320
of problems that another LLM is learning to solve?
Then the LLM is always trying to serve more and

01:42:16.319 --> 01:42:21.519
more difficult problems, stuff like that.
There's a bunch of ways to organize it.

01:42:22.159 --> 01:42:26.559
It's a realm of research, but I haven't 
seen anything that convincingly claims

01:42:26.560 --> 01:42:31.600
both of those multi-agent improvements.
We're mostly in the realm of a single

01:42:31.600 --> 01:42:37.280
individual agent, but that will change.
In the realm of culture also,

01:42:37.279 --> 01:42:41.199
I would also bucket organizations.
We haven't seen anything like that convincingly

01:42:41.199 --> 01:42:45.840
either. That's why we're still early.
Can you identify the key bottleneck

01:42:45.840 --> 01:42:50.159
that's preventing this kind 
of collaboration between LLMs?

01:42:50.159 --> 01:42:56.079
Maybe the way I would put it is, 
some of these analogies work and

01:42:56.079 --> 01:43:01.119
they shouldn't, but somehow, remarkably, they do.
A lot of the smaller models, or the dumber models,

01:43:01.119 --> 01:43:06.720
remarkably resemble a kindergarten student, or an 
elementary school student or high school student.

01:43:07.520 --> 01:43:10.720
Somehow, we still haven't graduated 
enough where this stuff can take over.

01:43:12.000 --> 01:43:17.039
My Claude Code or Codex, they still 
feel like this elementary-grade student.

01:43:17.039 --> 01:43:21.519
I know that they can take PhD quizzes, 
but they still cognitively feel like a

01:43:21.520 --> 01:43:24.960
kindergarten or an elementary school student.
I don't think they can create culture because

01:43:24.960 --> 01:43:32.319
they're still kids. They're savant kids. 
They have perfect memory of all this stuff.

01:43:33.199 --> 01:43:36.720
They can convincingly create all 
kinds of slop that looks really good.

01:43:36.720 --> 01:43:38.800
But I still think they don't really know 
what they're doing and they don't really

01:43:38.800 --> 01:43:43.039
have the cognition across all these little 
checkboxes that we still have to collect.

01:43:43.760 --> 01:43:50.159
You've talked about how you were at Tesla 
leading self-driving from 2017 to 2022.

01:43:50.159 --> 01:43:58.399
And you firsthand saw this progress from cool 
demos to now thousands of cars out there actually

01:43:58.399 --> 01:44:01.279
autonomously doing drives.
Why did that take a decade?

01:44:01.279 --> 01:44:06.559
What was happening through that time?
One thing I will almost instantly push

01:44:06.560 --> 01:44:12.160
back on is that this is not even near done, 
in a bunch of ways that I'm going to get to.

01:44:13.279 --> 01:44:16.800
Self-driving is very interesting because 
it's definitely where I get a lot of my

01:44:16.800 --> 01:44:22.480
intuitions because I spent five years on it.
It has this entire history where the first demos

01:44:22.479 --> 01:44:28.159
of self-driving go all the way to the 1980s.
You can see a demo from CMU in 1986.

01:44:28.159 --> 01:44:34.159
There's a truck that's driving itself on 
roads. Fast forward. When I was joining Tesla,

01:44:34.159 --> 01:44:40.159
I had a very early demo of Waymo.
It basically gave me a perfect drive

01:44:40.159 --> 01:44:46.239
in 2014 or something like that, so 
a perfect Waymo drive a decade ago.

01:44:46.239 --> 01:44:49.599
It took us around Palo Alto and so on 
because I had a friend who worked there.

01:44:50.640 --> 01:44:53.440
I thought it was very close and 
then it still took a long time.

01:44:54.800 --> 01:45:02.079
For some kinds of tasks and jobs and so on, 
there's a very large demo-to-product gap where the

01:45:02.079 --> 01:45:07.119
demo is very easy, but the product is very hard.
It's especially the case in cases like

01:45:07.119 --> 01:45:10.800
self-driving where the cost 
of failure is too high.

01:45:11.760 --> 01:45:15.840
Many industries, tasks, and jobs maybe don't have 
that property, but when you do have that property,

01:45:15.840 --> 01:45:19.840
that definitely increases the timelines.
For example, in software engineering,

01:45:19.840 --> 01:45:24.560
I do think that property does exist.
For a lot of vibe coding, it doesn't.

01:45:24.560 --> 01:45:28.640
But if you're writing actual production-grade 
code, that property should exist, because any

01:45:28.640 --> 01:45:32.079
kind of mistake leads to a security 
vulnerability or something like that.

01:45:32.079 --> 01:45:36.159
Millions and hundreds of millions of 
people's personal Social Security numbers

01:45:36.159 --> 01:45:41.599
get leaked or something like that.
So in software, people should be careful,

01:45:42.159 --> 01:45:46.000
kind of like in self-driving.
In self-driving, if things go wrong,

01:45:46.000 --> 01:45:52.960
you might get injured. There are worse 
outcomes. But in software, it's almost

01:45:52.960 --> 01:45:58.640
unbounded how terrible something could be.
I do think that they share that property.

01:45:59.920 --> 01:46:04.880
What takes the long amount of time and the way 
to think about it is that it's a march of nines.

01:46:04.880 --> 01:46:10.319
Every single nine is a constant amount of work.
Every single nine is the same amount of work.

01:46:10.319 --> 01:46:16.719
When you get a demo and something works 90% 
of the time, that's just the first nine.

01:46:16.720 --> 01:46:18.880
Then you need the second nine, a third 
nine, a fourth nine, a fifth nine.

01:46:18.880 --> 01:46:23.039
While I was at Tesla for five years or so, we 
went through maybe three nines or two nines.

01:46:23.039 --> 01:46:25.600
I don't know what it is, but 
multiple nines of iteration.

01:46:25.600 --> 01:46:29.520
There are still more nines to go.
That's why these things take so long.

01:46:31.520 --> 01:46:35.360
It's definitely formative for me, seeing 
something that was a demo. I'm very

01:46:35.359 --> 01:46:40.880
unimpressed by demos. Whenever I see demos of 
anything, I'm extremely unimpressed by that.

01:46:43.439 --> 01:46:45.919
If it's a demo that someone cooked 
up as a showing, it's worse.

01:46:45.920 --> 01:46:47.600
If you can interact with it, it's a bit better.

01:46:47.600 --> 01:46:50.960
But even then, you're not done. You need the 
actual product. It's going to face all these

01:46:50.960 --> 01:46:53.760
challenges when it comes in contact 
with reality and all these different

01:46:53.760 --> 01:46:57.440
pockets of behavior that need patching.
We're going to see all this stuff play

01:46:57.439 --> 01:47:01.839
out. It's a march of nines. Each nine is 
constant. Demos are encouraging. It’s still

01:47:01.840 --> 01:47:07.199
a huge amount of work to do.
It is a critical safety domain,

01:47:07.199 --> 01:47:10.880
unless you're doing vibe coding, 
which is all nice and fun and so on.

01:47:11.840 --> 01:47:14.560
That's why this also enforced my 
timelines from that perspective.

01:47:16.000 --> 01:47:20.560
It's very interesting to hear you say that, that 
the safety guarantees you need from software

01:47:20.560 --> 01:47:24.560
are not dissimilar to self-driving.
What people will often say is that

01:47:24.560 --> 01:47:30.160
self-driving took so long because 
the cost of failure is so high.

01:47:30.159 --> 01:47:34.319
A human makes a mistake on average every 
400,000 miles or every seven years.

01:47:34.319 --> 01:47:39.359
If you had to release a coding agent that 
couldn't make a mistake for at least seven years,

01:47:39.359 --> 01:47:43.439
it would be much harder to deploy.
But your point is that if you made a

01:47:43.439 --> 01:47:47.839
catastrophic coding mistake, like breaking 
some important system every seven years...

01:47:47.840 --> 01:47:51.199
Very easy to do.
In fact, in terms of wall clock time,

01:47:51.199 --> 01:47:54.559
it would be much less than seven years because 
you're constantly outputting code like that.

01:47:57.279 --> 01:48:00.079
In terms of tokens, it would be seven years.
But in terms of wall clock time...

01:48:00.079 --> 01:48:02.880
In some ways, it's a much harder problem.
Self-driving is just one of

01:48:02.880 --> 01:48:07.119
thousands of things that people do.
It's almost like a single vertical, I suppose.

01:48:07.119 --> 01:48:08.960
Whereas when we're talking about 
general software engineering,

01:48:08.960 --> 01:48:14.720
it's even more... There's more surface area.
There's another objection people make to that

01:48:14.720 --> 01:48:21.119
analogy, which is that with self-driving, what 
took a big fraction of that time was solving

01:48:21.119 --> 01:48:27.039
the problem of having basic perception 
that's robust, building representations,

01:48:27.039 --> 01:48:32.079
and having a model that has some common 
sense so it can generalize to when it sees

01:48:32.079 --> 01:48:37.039
something that's slightly out of distribution.
If somebody's waving down the road this way,

01:48:37.039 --> 01:48:40.640
you don't need to train for it.
The thing will have some understanding

01:48:40.640 --> 01:48:44.400
of how to respond to something like that.
These are things we're getting for free

01:48:44.399 --> 01:48:49.679
with LLMs or VLMs today, so we don't have to 
solve these very basic representation problems.

01:48:49.680 --> 01:48:54.320
So now deploying AIs across different domains 
will sort of be like deploying a self-driving

01:48:54.319 --> 01:48:58.559
car with current models to a different city, 
which is hard but not like a 10-year-long task.

01:48:59.199 --> 01:49:03.119
I'm not 100% sure if I fully agree with that.
I don't know how much we're getting for free.

01:49:03.840 --> 01:49:06.560
There's still a lot of gaps in 
understanding what we are getting.

01:49:07.680 --> 01:49:10.240
We're definitely getting more 
generalizable intelligence in a

01:49:10.239 --> 01:49:14.479
single entity, whereas self-driving is a 
very special-purpose task that requires.

01:49:14.479 --> 01:49:18.559
In some sense building a special-purpose task 
is maybe even harder in a certain sense because

01:49:18.560 --> 01:49:22.240
it doesn't fall out from a more general thing 
that you're doing at scale, if that makes sense.

01:49:24.239 --> 01:49:30.399
But the analogy still doesn't fully 
resonate because the LLMs are still

01:49:30.399 --> 01:49:33.519
pretty fallible and they have a lot of 
gaps that still need to be filled in.

01:49:33.520 --> 01:49:35.680
I don't think that we're 
getting magical generalization

01:49:35.680 --> 01:49:42.320
completely out of the box, in a certain sense.
The other aspect that I wanted to return to is

01:49:42.960 --> 01:49:50.480
that self-driving cars are nowhere near done 
still. The deployments are pretty minimal.

01:49:51.359 --> 01:49:54.479
Even Waymo and so on has very few cars.
They're doing that roughly speaking

01:49:54.479 --> 01:49:59.439
because they're not economical.
They've built something that lives in the future.

01:50:00.319 --> 01:50:03.759
They've had to pull back the future, 
but they had to make it uneconomical.

01:50:05.840 --> 01:50:09.279
There are all these costs, not just 
marginal costs for those cars and

01:50:09.279 --> 01:50:12.399
their operation and maintenance, but 
also the capex of the entire thing.

01:50:13.119 --> 01:50:16.159
Making it economical is still 
going to be a slog for them.

01:50:17.920 --> 01:50:21.520
Also, when you look at these cars and 
there's no one driving, I actually think

01:50:21.520 --> 01:50:26.480
it's a little bit deceiving because there 
are very elaborate teleoperation centers

01:50:26.479 --> 01:50:32.639
of people kind of in a loop with these cars.
I don't have the full extent of it, but there's

01:50:32.640 --> 01:50:36.160
more human-in-the-loop than you might expect.
There are people somewhere out there

01:50:36.800 --> 01:50:39.920
beaming in from the sky.
I don't know if they're

01:50:39.920 --> 01:50:42.800
fully in the loop with the driving.
Some of the time they are, but they're

01:50:42.800 --> 01:50:45.600
certainly involved and there are people.
In some sense, we haven't actually removed

01:50:45.600 --> 01:50:48.560
the person, we've moved them to 
somewhere where you can't see them.

01:50:48.560 --> 01:50:51.680
I still think there will be some work, as you 
mentioned, going from environment to environment.

01:50:52.800 --> 01:50:55.680
There are still challenges 
to make self-driving real.

01:50:55.680 --> 01:50:59.600
But I do agree that it's definitely crossed 
a threshold where it kind of feels real,

01:50:59.600 --> 01:51:03.280
unless it's really teleoperated.
For example, Waymo can't go to

01:51:03.279 --> 01:51:07.119
all the different parts of the city.
My suspicion is that it's parts of the city

01:51:07.119 --> 01:51:11.840
where you don't get good signal.
Anyway, I don't know anything

01:51:11.840 --> 01:51:17.920
about the stack. I'm just making stuff up.
You led self-driving for five years at Tesla.

01:51:17.920 --> 01:51:19.600
Sorry, I don't know anything 
about the specifics of Waymo.

01:51:21.119 --> 01:51:22.880
By the way, I love Waymo 
and I take it all the time.

01:51:24.880 --> 01:51:29.119
I just think that people are sometimes a 
little bit too naive about some of the progress

01:51:29.119 --> 01:51:33.439
and there's still a huge amount of work.
Tesla took in my mind a much more scalable

01:51:33.439 --> 01:51:39.439
approach and the team is doing extremely well.
I'm kind of on the record for predicting

01:51:39.439 --> 01:51:42.399
how this thing will go.
Waymo had an early start

01:51:42.399 --> 01:51:45.920
because you can package up so many sensors.
But I do think Tesla is taking the more

01:51:45.920 --> 01:51:48.720
scalable strategy and it's going 
to look a lot more like that.

01:51:48.720 --> 01:51:54.240
So this will still have to play out and hasn't.
But I don't want to talk about self-driving as

01:51:54.239 --> 01:51:59.840
something that took a decade because it 
didn't take it yet, if that makes sense.

01:51:59.840 --> 01:52:05.119
Because one, the start is at 1980 and not 10 
years ago, and then two, the end is not here yet.

01:52:05.119 --> 01:52:08.399
The end is not near yet because when 
we're talking about self-driving,

01:52:08.399 --> 01:52:13.759
usually in my mind it's self-driving at scale.
People don't have to get a driver's license, etc.

01:52:13.760 --> 01:52:19.119
I'm curious to bounce two other ways in 
which the analogy might be different.

01:52:19.119 --> 01:52:24.479
The reason I'm especially curious about this is 
because the question of how fast AI is deployed,

01:52:24.479 --> 01:52:28.719
how valuable it is when it's early 
on is potentially the most important

01:52:28.720 --> 01:52:31.119
question in the world right now.
If you're trying to model what the

01:52:31.119 --> 01:52:35.199
year 2030 looks like, this is the question 
you ought to have some understanding of.

01:52:36.159 --> 01:52:41.519
Another thing you might think is one, you have 
this latency requirement with self-driving.

01:52:42.239 --> 01:52:45.439
I have no idea what the actual models are, but I 
assume it’s like tens of millions of parameters

01:52:45.439 --> 01:52:51.199
or something, which is not the necessary 
constraint for knowledge work with LLMs.

01:52:51.199 --> 01:52:56.880
Maybe it might be with computer use and stuff.
But the other big one is, maybe more

01:52:56.880 --> 01:53:02.800
importantly, on this capex question.
Yes, there is additional cost to serving

01:53:02.800 --> 01:53:12.000
up an additional copy of a model, but the opex 
of a session is quite low and you can amortize

01:53:12.000 --> 01:53:17.279
the cost of AI into the training run itself, 
depending on how inference scaling goes and stuff.

01:53:17.279 --> 01:53:23.759
But it's certainly not as much as building a whole 
new car to serve another instance of a model.

01:53:23.760 --> 01:53:28.560
So the economics of deploying more 
widely are much more favorable.

01:53:28.560 --> 01:53:31.120
I think that's right. If you're 
sticking to the realm of bits,

01:53:31.119 --> 01:53:36.640
bits are a million times easier than anything 
that touches the physical world. I definitely

01:53:36.640 --> 01:53:42.880
grant that. Bits are completely changeable, 
arbitrarily reshuffleable at a very rapid speed.

01:53:42.880 --> 01:53:50.319
You would expect a much faster adaptation also in 
the industry and so on. What was the first one?

01:53:50.319 --> 01:53:53.519
The latency requirements and 
its implications for model size?

01:53:53.520 --> 01:53:56.080
I think that's roughly right. I also 
think that if we are talking about

01:53:56.079 --> 01:54:00.079
knowledge work at scale, there will be some 
latency requirements, practically speaking,

01:54:00.079 --> 01:54:06.159
because we're going to have to create a 
huge amount of compute and serve that.

01:54:06.159 --> 01:54:10.079
The last aspect that I very briefly want 
to also talk about is all the rest of it.

01:54:13.279 --> 01:54:17.519
What does society think about it? What are 
the legal ramifications? How is it working

01:54:17.520 --> 01:54:24.080
legally? How is it working insurance-wise? 
What are those layers of it and aspects of it?

01:54:25.039 --> 01:54:27.600
What is the equivalent of people 
putting a cone on a Waymo?

01:54:28.800 --> 01:54:34.480
There are going to be equivalents of all that.
So I feel like self-driving is a very nice

01:54:34.479 --> 01:54:38.000
analogy that you can borrow things from.
What is the equivalent of a cone in the car?

01:54:38.000 --> 01:54:45.279
What is the equivalent of a teleoperating worker 
who's hidden away and all the aspects of it.

01:54:45.279 --> 01:54:48.479
Do you have any opinions on what this 
implies about the current AI buildout,

01:54:49.199 --> 01:54:56.159
which would 10x the amount of available compute 
in the world in a year or two and maybe more

01:54:56.159 --> 01:55:00.800
than 100x it by the end of the decade.
If the use of AI will be lower than

01:55:00.800 --> 01:55:04.560
some people naively predict, does 
that mean that we're overbuilding

01:55:04.560 --> 01:55:08.560
compute or is that a separate question?
Kind of like what happened with railroads.

01:55:08.560 --> 01:55:10.880
With what, sorry?
Was it railroads or?

01:55:10.880 --> 01:55:14.079
Yeah, it was.
Yeah. There's historical precedent.

01:55:14.079 --> 01:55:18.479
Or was it with the telecommunication industry?
Pre-paving the internet that only came a decade

01:55:18.479 --> 01:55:24.000
later and creating a whole bubble in the 
telecommunications industry in the late '90s.

01:55:28.000 --> 01:55:33.199
I understand I'm sounding very pessimistic 
here. I'm actually optimistic. I think this

01:55:33.199 --> 01:55:36.479
will work. I think it's tractable. I'm 
only sounding pessimistic because when

01:55:36.479 --> 01:55:40.399
I go on my Twitter timeline, I see all 
this stuff that makes no sense to me.

01:55:42.560 --> 01:55:47.360
There's a lot of reasons for why that exists.
A lot of it is honestly just fundraising.

01:55:47.359 --> 01:55:50.479
It's just incentive structures. 
A lot of it may be fundraising.

01:55:50.479 --> 01:55:55.439
A lot of it is just attention, converting 
attention to money on the internet,

01:55:55.439 --> 01:56:00.239
stuff like that.
There's a lot of

01:56:00.239 --> 01:56:05.920
that going on, and I'm only reacting to that.
But I'm still overall very bullish on technology.

01:56:05.920 --> 01:56:09.920
We're going to work through all this stuff.
There's been a rapid amount of progress.

01:56:09.920 --> 01:56:15.199
I don't know that there's overbuilding.
I think we're going to be able to gobble up what,

01:56:15.199 --> 01:56:20.239
in my understanding, is being built.
For example, Claude Code or OpenAI Codex

01:56:20.239 --> 01:56:22.880
and stuff like that didn't even 
exist a year ago. Is that right?

01:56:24.960 --> 01:56:29.920
This is a miraculous technology that didn't exist.
There's going to be a huge amount of demand,

01:56:29.920 --> 01:56:36.000
as we see the demand in ChatGPT already and so on.
So I don't know that there's overbuilding.

01:56:37.920 --> 01:56:42.319
I'm just reacting to some of the very fast 
timelines that people continue to say incorrectly.

01:56:42.319 --> 01:56:46.559
I've heard many, many times over the course 
of my 15 years in AI where very reputable

01:56:46.560 --> 01:56:53.440
people keep getting this wrong all the time.
I want this to be properly calibrated, and some

01:56:53.439 --> 01:56:59.119
of this also has geopolitical ramifications and 
things like that with some of these questions.

01:56:59.680 --> 01:57:03.200
I don't want people to make 
mistakes in that sphere of things.

01:57:04.079 --> 01:57:07.760
I do want us to be grounded in the 
reality of what technology is and isn't.

01:57:08.880 --> 01:57:15.840
Let's talk about education and Eureka.
One thing you could do is start another AI

01:57:15.840 --> 01:57:21.119
lab and then try to solve those problems.
I’m curious what you're up to now,

01:57:21.680 --> 01:57:26.320
and why not AI research itself?
I guess the way I would put it

01:57:26.319 --> 01:57:32.319
is I feel some amount of determinism 
around the things that AI labs are doing.

01:57:33.359 --> 01:57:41.199
I feel like I could help out there, but I 
don't know that I would uniquely improve it.

01:57:42.239 --> 01:57:46.479
My personal big fear is that a lot of this 
stuff happens on the side of humanity,

01:57:46.479 --> 01:57:52.799
and that humanity gets disempowered by it.
I care not just about all the Dyson spheres

01:57:52.800 --> 01:57:55.440
that we're going to build and that AI is 
going to build in a fully autonomous way,

01:57:55.439 --> 01:58:00.479
I care about what happens to humans.
I want humans to be well off in the future.

01:58:00.479 --> 01:58:04.079
I feel like that's where I can a 
lot more uniquely add value than

01:58:04.079 --> 01:58:11.199
an incremental improvement in the frontier lab.
I'm most afraid of something depicted in movies

01:58:11.199 --> 01:58:15.920
like WALL-E or Idiocracy or something like that, 
where humanity is on the side of this stuff.

01:58:16.800 --> 01:58:19.920
I want humans to be much, 
much better in this future.

01:58:21.119 --> 01:58:24.640
To me, this is through education 
that you can achieve this.

01:58:26.000 --> 01:58:30.560
So what are you working on there?
The easiest way I can describe it is

01:58:30.560 --> 01:58:34.800
we're trying to build the Starfleet Academy.
I don’t know if you’ve watched Star Trek.

01:58:34.800 --> 01:58:37.600
I haven’t.
Starfleet Academy is

01:58:37.600 --> 01:58:43.840
this elite institution for frontier technology, 
building spaceships, and graduating cadets to be

01:58:43.840 --> 01:58:47.920
the pilots of these spaceships and whatnot.
So I just imagine an elite institution for

01:58:47.920 --> 01:58:56.480
technical knowledge and a kind of school that's 
very up-to-date and a premier institution.

01:58:56.479 --> 01:59:03.519
A category of questions I have for you is 
explaining how one teaches technical or

01:59:03.520 --> 01:59:08.000
scientific content well, because you 
are one of the world masters at it.

01:59:08.640 --> 01:59:11.760
I'm curious both about how you think about 
it for content you've already put out there

01:59:11.760 --> 01:59:16.079
on YouTube, but also, to the extent it's any 
different, how you think about it for Eureka.

01:59:16.880 --> 01:59:20.560
With respect to Eureka, one thing that 
is very fascinating to me about education

01:59:20.560 --> 01:59:24.960
is that I do think education will pretty 
fundamentally change with AIs on the side.

01:59:24.960 --> 01:59:30.399
It has to be rewired and changed to some extent.
I still think that we're pretty early.

01:59:30.399 --> 01:59:32.639
There's going to be a lot of people who 
are going to try to do the obvious things.

01:59:33.439 --> 01:59:38.079
Have an LLM and ask it questions.
Do all the basic things that you would

01:59:38.079 --> 01:59:40.159
do via prompting right now.
It's helpful,

01:59:40.159 --> 01:59:44.399
but it still feels to me a bit like slop.
I'd like to do it properly, and I think the

01:59:44.399 --> 01:59:50.559
capability is not there for what I would want.
What I'd want is an actual tutor experience.

01:59:51.359 --> 01:59:57.119
A prominent example in my mind is I was 
recently learning Korean, so language learning.

01:59:57.119 --> 02:00:00.319
I went through a phase where I was 
learning Korean by myself on the internet.

02:00:00.319 --> 02:00:06.079
I went through a phase where I was part of a small 
class in Korea taking Korean with a bunch of other

02:00:06.079 --> 02:00:08.079
people, which was really funny.
We had a teacher and 10

02:00:08.079 --> 02:00:12.479
people or so taking Korean.
Then I switched to a one-on-one tutor.

02:00:13.600 --> 02:00:19.600
I guess what was fascinating to me was, I think 
I had a really good tutor, but just thinking

02:00:19.600 --> 02:00:25.200
through what this tutor was doing for me and how 
incredible that experience was and how high the

02:00:25.199 --> 02:00:31.679
bar is for what I want to build eventually.
Instantly from a very short

02:00:31.680 --> 02:00:35.760
conversation, she understood where I am 
as a student, what I know and don't know.

02:00:35.760 --> 02:00:41.039
She was able to probe exactly the kinds of 
questions or things to understand my world model.

02:00:41.039 --> 02:00:44.319
No LLM will do that for you 
100% right now, not even close.

02:00:44.319 --> 02:00:49.359
But a tutor will do that if they're good.
Once she understands, she really served

02:00:49.359 --> 02:00:52.799
me all the things that I needed at 
my current sliver of capability.

02:00:52.800 --> 02:00:56.880
I need to be always appropriately challenged.
I can't be faced with something too hard or

02:00:56.880 --> 02:01:00.800
too trivial, and a tutor is really good 
at serving you just the right stuff.

02:01:01.680 --> 02:01:07.600
I felt like I was the only constraint to learning.
I was always given the perfect information. I'm

02:01:07.600 --> 02:01:11.120
the only constraint. I felt good because 
I'm the only impediment that exists.

02:01:11.119 --> 02:01:14.239
It's not that I can't find knowledge or 
that it's not properly explained or etc.

02:01:14.239 --> 02:01:18.639
It's just my ability to memorize and so on.
This is what I want for people.

02:01:18.640 --> 02:01:21.200
How do you automate that?
Very good question. At

02:01:21.199 --> 02:01:26.960
the current capability, you don't.
That's why I think it's not actually the

02:01:26.960 --> 02:01:31.359
right time to build this kind of an AI tutor.
I still think it's a useful product,

02:01:31.359 --> 02:01:38.479
and lots of people will build it, but the bar 
is so high and the capability is not there.

02:01:40.399 --> 02:01:45.439
Even today, I would say ChatGPT is an 
extremely valuable educational product.

02:01:45.439 --> 02:01:48.639
But for me, it was so fascinating 
to see how high the bar is.

02:01:48.640 --> 02:01:53.039
When I was with her, I almost felt 
like there's no way I can build this.

02:01:53.039 --> 02:01:55.199
But you are building it, right?
Anyone who's had a really good

02:01:55.199 --> 02:02:01.439
tutor is like, "How are you going to build 
this?" I'm waiting for that capability. I

02:02:04.399 --> 02:02:09.039
did some AI consulting for computer vision.
A lot of times, the value that I brought to

02:02:09.039 --> 02:02:13.519
the company was telling them not to use AI.
I was the AI expert, and they described the

02:02:13.520 --> 02:02:17.680
problem, and I said, "Don't use AI." 
This is my value add. I feel like it's

02:02:17.680 --> 02:02:21.600
the same in education right now, where 
I feel like for what I have in mind,

02:02:21.600 --> 02:02:26.160
it's not yet the time, but the time will come.
For now, I'm building something that looks

02:02:26.159 --> 02:02:30.319
maybe a bit more conventional that has a 
physical and digital component and so on.

02:02:30.319 --> 02:02:35.039
But it's obvious how this 
should look in the future.

02:02:35.039 --> 02:02:37.760
To the extent you're willing to 
say, what is the thing you hope

02:02:37.760 --> 02:02:43.280
will be released this year or next year?
I'm building the first course. I want to

02:02:43.279 --> 02:02:48.000
have a really, really good course, the 
obvious state-of-the-art destination

02:02:48.000 --> 02:02:51.439
you go to to learn, AI in this case.
That's just what I'm familiar with, so it's

02:02:51.439 --> 02:02:56.079
a really good first product to get to be really 
good at it. So that's what I'm building. Nanochat,

02:02:56.079 --> 02:03:00.720
which you briefly mentioned, is a capstone project 
of LLM101N, which is a class that I'm building.

02:03:02.479 --> 02:03:04.719
That's a really big piece of it.
But now I have to build out a lot of

02:03:04.720 --> 02:03:10.960
the intermediates, and then I have to hire a small 
team of TAs and so on and build the entire course.

02:03:11.520 --> 02:03:15.440
One more thing that I would say is that many 
times, when people think about education,

02:03:15.439 --> 02:03:20.239
they think more about what I would say is 
a softer component of diffusing knowledge.

02:03:21.760 --> 02:03:26.960
I have something very hard and technical in mind.
In my mind, education is the very difficult

02:03:26.960 --> 02:03:33.439
technical process of building ramps to knowledge.
In my mind, nanochat is a ramp to

02:03:33.439 --> 02:03:38.000
knowledge because it's very simple.
It's the super simplified full-stack thing.

02:03:38.000 --> 02:03:41.920
If you give this artifact to someone and they 
look through it, they're learning a ton of stuff.

02:03:43.760 --> 02:03:48.079
It's giving you a lot of what I call eurekas 
per second, which is understanding per second.

02:03:48.079 --> 02:03:52.720
That's what I want, lots of eurekas per second.
So to me, this is a technical problem of

02:03:52.720 --> 02:03:58.640
how do we build these ramps to knowledge.
So I almost think of Eureka as maybe not that

02:03:58.640 --> 02:04:03.119
different from some of the frontier labs 
or some of the work that's going on there.

02:04:03.119 --> 02:04:07.599
I want to figure out how to build these 
ramps very efficiently so that people are

02:04:07.600 --> 02:04:13.520
never stuck and everything is always 
not too hard or not too trivial, and

02:04:14.239 --> 02:04:18.960
you have just the right material to progress.
You're imagining in the short term that instead

02:04:18.960 --> 02:04:24.640
of a tutor being able to probe your understanding, 
if you have enough self-awareness to be able to

02:04:24.640 --> 02:04:29.680
probe yourself, you're never going to be stuck.
You can find the right answer between talking

02:04:29.680 --> 02:04:33.440
to the TA or talking to an LLM and 
looking at the reference implementation.

02:04:33.439 --> 02:04:38.879
It sounds like automation or 
AI is not a significant part.

02:04:38.880 --> 02:04:46.480
So far, the big alpha here is your 
ability to explain AI codified

02:04:46.479 --> 02:04:51.359
in the source material of the class.
That's fundamentally what the course is.

02:04:51.359 --> 02:04:55.119
You always have to be calibrated to 
what capability exists in the industry.

02:04:55.920 --> 02:04:59.279
A lot of people are going to 
pursue just asking ChatGPT, etc.

02:04:59.279 --> 02:05:03.840
But I think right now, for example, if you go to 
ChatGPT and you say, teach me AI, there's no way.

02:05:03.840 --> 02:05:09.119
It's going to give you some slop.
AI is never going to write nanochat right now.

02:05:09.119 --> 02:05:12.239
But nanochat is a really 
useful intermediate point.

02:05:13.520 --> 02:05:15.760
I'm collaborating with AI 
to create all this material,

02:05:15.760 --> 02:05:21.039
so AI is still fundamentally very helpful.
Earlier on, I built CS231n at Stanford,

02:05:21.039 --> 02:05:26.319
which I think was the first deep learning 
class at Stanford, which became very popular.

02:05:27.680 --> 02:05:31.760
The difference in building out 231n 
then and LLM101N now is quite stark.

02:05:33.119 --> 02:05:37.840
I feel really empowered by the LLMs as they 
exist right now, but I'm very much in the loop.

02:05:37.840 --> 02:05:40.960
They're helping me build the 
materials, I go much faster.

02:05:40.960 --> 02:05:45.119
They're doing a lot of the boring stuff, etc.
I feel like I'm developing the course much faster,

02:05:45.119 --> 02:05:50.159
and it's LLM-infused, but it's not yet at a 
place where it can creatively create the content.

02:05:50.159 --> 02:05:53.439
I'm still there to do that.
The trickiness is always

02:05:53.439 --> 02:05:57.039
calibrating yourself to what exists.
When you imagine what is available

02:05:57.039 --> 02:06:02.399
through Eureka in a couple of years, it 
seems like the big bottleneck is going to be

02:06:02.399 --> 02:06:08.399
finding Karpathys in field after field who can 
convert their understanding into these ramps.

02:06:09.439 --> 02:06:14.799
It would change over time. Right now, 
it would be hiring faculty to help work

02:06:14.800 --> 02:06:20.400
hand-in-hand with AI and a team of people 
probably to build state-of-the-art courses.

02:06:21.439 --> 02:06:28.159
Over time maybe some of the TAs can become AIs.
You just take all the course materials and then

02:06:28.159 --> 02:06:33.119
I think you could serve a very good automated 
TA for the student when they have more basic

02:06:33.119 --> 02:06:36.079
questions or something like that.
But I think you'll need faculty

02:06:36.079 --> 02:06:40.319
for the overall architecture of a 
course and making sure that it fits.

02:06:40.319 --> 02:06:45.039
So I see a progression of how this will evolve.
Maybe at some future point I'm not even that

02:06:45.039 --> 02:06:47.760
useful and AI is doing most of the 
design much better than I could.

02:06:47.760 --> 02:06:50.159
But I still think that's going 
to take some time to play out.

02:06:50.880 --> 02:06:56.720
Are you imagining that people who have expertise 
in other fields are then contributing courses,

02:06:56.720 --> 02:07:01.119
or do you feel like it's quite 
essential to the vision that you,

02:07:01.119 --> 02:07:06.479
given your understanding of how you want to 
teach, are the one designing the content?

02:07:07.119 --> 02:07:09.519
Sal Khan is narrating all 
the videos on Khan Academy.

02:07:09.520 --> 02:07:12.960
Are you imagining something like that?
No, I will hire faculty because there

02:07:12.960 --> 02:07:18.640
are domains in which I'm not an expert.
That's the only way to offer the state-of-the-art

02:07:18.640 --> 02:07:24.160
experience for the student ultimately.
I do expect that I would hire faculty, but

02:07:24.159 --> 02:07:29.920
I will probably stick around in AI for some time.
I do have something more conventional in mind for

02:07:29.920 --> 02:07:32.880
the current capability than what 
people would probably anticipate.

02:07:33.680 --> 02:07:38.640
When I'm building Starfleet Academy, I do probably 
imagine a physical institution, and maybe a tier

02:07:38.640 --> 02:07:44.160
below that a digital offering that is not the 
state-of-the-art experience you would get when

02:07:44.159 --> 02:07:48.479
someone comes in physically full-time and we 
work through material from start to end and

02:07:48.479 --> 02:07:53.679
make sure you understand it. That's the physical 
offering. The digital offering is a bunch of stuff

02:07:53.680 --> 02:07:57.680
on the internet and maybe some LLM assistant.
It's a bit more gimmicky in a tier below, but

02:07:57.680 --> 02:08:04.960
at least it's accessible to 8 billion people.
I think you're basically inventing college

02:08:04.960 --> 02:08:11.680
from first principles for the tools that 
are available today and just selecting

02:08:11.680 --> 02:08:17.440
for people who have the motivation and the 
interest of really engaging with material.

02:08:18.079 --> 02:08:21.359
There's going to have to be a lot of not 
just education but also re-education.

02:08:21.359 --> 02:08:26.000
I would love to help out there because 
the jobs will probably change quite a bit.

02:08:27.359 --> 02:08:29.839
For example, today a lot of people are 
trying to upskill in AI specifically.

02:08:29.840 --> 02:08:32.400
I think it's a really good 
course to teach in this respect.

02:08:34.960 --> 02:08:41.119
Motivation-wise, before AGI motivation is very 
simple to solve because people want to make money.

02:08:41.119 --> 02:08:46.960
This is how you make money in the industry today.
Post-AGI is a lot more interesting possibly

02:08:46.960 --> 02:08:49.760
because if everything is automated 
and there's nothing to do for anyone,

02:08:49.760 --> 02:08:57.680
why would anyone go to a school?
I often say that pre-AGI education

02:08:57.680 --> 02:09:05.520
is useful. Post-AGI education is fun. In 
a similar way, people go to the gym today.

02:09:06.560 --> 02:09:10.400
We don't need their physical strength 
to manipulate heavy objects because we

02:09:10.399 --> 02:09:12.399
have machines that do that.
They still go to the gym.

02:09:12.399 --> 02:09:15.119
Why do they go to the gym?
Because it's fun, it's healthy,

02:09:16.239 --> 02:09:24.000
and you look hot when you have a six-pack.
It's attractive for people to do that

02:09:24.000 --> 02:09:28.880
in a very deep, psychological, 
evolutionary sense for humanity.

02:09:30.319 --> 02:09:33.759
Education will play out in the same way.
You'll go to school like you go to the gym.

02:09:36.000 --> 02:09:40.720
Right now, not that many people learn 
because learning is hard. You bounce

02:09:40.720 --> 02:09:44.720
from material. Some people overcome that 
barrier, but for most people, it's hard.

02:09:46.000 --> 02:09:50.319
It's a technical problem to solve.
It's a technical problem to do what my tutor

02:09:50.319 --> 02:09:53.439
did for me when I was learning Korean.
It's tractable and buildable,

02:09:53.439 --> 02:09:55.759
and someone should build it.
It's going to make learning

02:09:55.760 --> 02:10:00.480
anything trivial and desirable, and people 
will do it for fun because it's trivial.

02:10:00.479 --> 02:10:05.439
If I had a tutor like that for any arbitrary piece 
of knowledge, it's going to be so much easier to

02:10:05.439 --> 02:10:07.599
learn anything, and people will do it.
They'll do it for the same

02:10:07.600 --> 02:10:13.600
reasons they go to the gym.
That sounds different from using…

02:10:14.159 --> 02:10:21.279
So post-AGI, you're using this as 
entertainment or as self-betterment.

02:10:21.279 --> 02:10:25.199
But it sounded like you had a vision 
also that this education is relevant to

02:10:25.199 --> 02:10:30.800
keeping humanity in control of AI. That sounds 
different. Is it entertaining for some people,

02:10:30.800 --> 02:10:32.800
but then empowerment for some others?
How do you think about that?

02:10:32.800 --> 02:10:40.079
I do think eventually it's a bit of 
a losing game, if that makes sense.

02:10:41.439 --> 02:10:44.159
It is in the long term.
In the long term, which

02:10:44.159 --> 02:10:47.519
is longer than maybe most people in the 
industry think about, it's a losing game.

02:10:47.520 --> 02:10:53.440
I do think people can go so far and we've barely 
scratched the surface of how much a person can go.

02:10:53.439 --> 02:10:56.639
That's just because people are bouncing off 
of material that's too easy or too hard.

02:10:59.279 --> 02:11:03.039
People will be able to go much further.
Anyone will speak five languages because

02:11:03.039 --> 02:11:09.680
why not? Because it's so trivial. Anyone will know 
all the basic curriculum of undergrad, et cetera.

02:11:09.680 --> 02:11:13.200
Now that I'm understanding the 
vision, that's very interesting.

02:11:14.079 --> 02:11:18.000
It has a perfect analog in gym culture.
I don't think 100 years

02:11:18.000 --> 02:11:22.720
ago anybody would be ripped.
Nobody would have been able to just spontaneously

02:11:22.720 --> 02:11:29.199
bench two plates or three plates or something.
It's very common now because of this idea of

02:11:29.199 --> 02:11:33.519
systematically training and lifting weights in 
the gym, or systematically training to be able

02:11:33.520 --> 02:11:38.480
to run a marathon, which is a capability 
most humans would not spontaneously have.

02:11:38.479 --> 02:11:43.519
You're imagining similar things for 
learning across many different domains,

02:11:43.520 --> 02:11:48.080
much more intensely, deeply, faster.
Exactly. I am betting a bit implicitly

02:11:48.079 --> 02:11:56.559
on some of the timelessness of human nature.
It will be desirable to do all these things,

02:11:58.239 --> 02:12:02.079
and I think people will look up 
to it as they have for millennia.

02:12:03.600 --> 02:12:07.360
This will continue to be true.
There's some evidence of that historically.

02:12:07.359 --> 02:12:11.279
If you look at, for example, aristocrats, or you 
look at ancient Greece or something like that,

02:12:11.279 --> 02:12:15.359
whenever you had little pocket environments 
that were post-AGI in a certain sense, people

02:12:15.359 --> 02:12:20.719
have spent a lot of their time flourishing in a 
certain way, either physically or cognitively.

02:12:22.159 --> 02:12:28.159
I feel okay about the prospects of that.
If this is false and I'm wrong and we end up in a

02:12:29.119 --> 02:12:35.039
WALL-E or Idiocracy future, then I don't even care 
if there are Dyson spheres. This is a terrible

02:12:35.039 --> 02:12:41.359
outcome. I really do care about humanity.
Everyone has to just be

02:12:41.359 --> 02:12:46.559
superhuman in a certain sense.
It's still a world in which that is not enabling

02:12:46.560 --> 02:12:51.600
us to… It's like the culture world, right?
You're not fundamentally going to be able

02:12:51.600 --> 02:12:57.680
to transform the trajectory 
of technology or influence

02:12:57.680 --> 02:13:03.119
decisions by your own labor or cognition alone.
Maybe you can influence decisions because the AI

02:13:03.119 --> 02:13:10.079
is asking for your approval, but it's not because 
I've invented something or I've come up with a new

02:13:10.079 --> 02:13:15.920
design that I'm really influencing the future.
Maybe. I think there will be a transitional

02:13:15.920 --> 02:13:19.119
period where we are going to be 
able to be in the loop and advance

02:13:19.119 --> 02:13:23.439
things if we understand a lot of stuff.
In the long-term, that probably goes away.

02:13:25.039 --> 02:13:28.319
It might even become a sport.
Right now you have powerlifters

02:13:28.319 --> 02:13:33.519
who go extreme in this direction.
What is powerlifting in a cognitive era?

02:13:33.520 --> 02:13:36.800
Maybe it's people who are really trying 
to make Olympics out of knowing stuff.

02:13:39.600 --> 02:13:43.600
If you have a perfect AI tutor, 
maybe you can get extremely far.

02:13:43.600 --> 02:13:48.480
I feel that the geniuses of 
today are barely scratching the

02:13:48.479 --> 02:13:55.839
surface of what a human mind can do, I think.
I love this vision. I also feel like the person

02:13:55.840 --> 02:14:02.239
you have the most product-market fit with is me 
because my job involves having to learn different

02:14:02.239 --> 02:14:10.800
subjects every week, and I am very excited.
I'm similar, for that matter. A lot of people,

02:14:10.800 --> 02:14:15.039
for example, hate school and want to get 
out of it. I really liked school. I loved

02:14:15.039 --> 02:14:16.800
learning things, et cetera.
I wanted to stay in school.

02:14:16.800 --> 02:14:19.440
I stayed all the way until Ph.D. and 
then they wouldn't let me stay longer,

02:14:19.439 --> 02:14:25.039
so I went to the industry.
Roughly speaking, I love learning,

02:14:25.039 --> 02:14:29.199
even for the sake of learning, but I also love 
learning because it's a form of empowerment and

02:14:29.199 --> 02:14:32.000
being useful and productive.
You also made a point that

02:14:32.800 --> 02:14:36.640
was subtle and I want to spell it out.
With what’s happened so far with online

02:14:36.640 --> 02:14:44.160
courses, why haven't they already enabled us to 
enable every single human to know everything?

02:14:44.720 --> 02:14:51.360
They're just so motivation-laden because there are 
no obvious on-ramps and it's so easy to get stuck.

02:14:52.640 --> 02:15:00.000
If you had this thing instead—like a really 
good human tutor—it would just be such an

02:15:00.000 --> 02:15:04.239
unlock from a motivation perspective.
I think so. It feels bad to bounce from

02:15:04.239 --> 02:15:09.519
material. It feels bad. You get negative reward 
from sinking an amount of time in something and it

02:15:09.520 --> 02:15:14.080
doesn't pan out, or being completely bored because 
what you're getting is too easy or too hard.

02:15:16.159 --> 02:15:20.639
When you do it properly, learning feels good.
It's a technical problem to get there.

02:15:21.920 --> 02:15:25.920
For a while, it's going to be AI plus human 
collab, and at some point, maybe it's just AI.

02:15:27.119 --> 02:15:32.880
Can I ask some questions about teaching well?
If you had to give advice to another educator

02:15:32.880 --> 02:15:39.680
in another field that you're curious about to 
make the kinds of YouTube tutorials you've made.

02:15:40.399 --> 02:15:43.119
Maybe it might be especially interesting 
to talk about domains where you can't

02:15:43.920 --> 02:15:47.600
test someone's technical understanding by 
having them code something up or something.

02:15:47.600 --> 02:15:54.480
What advice would you give them?
That's a pretty broad topic. There are 10–20 tips

02:15:54.479 --> 02:16:03.359
and tricks that I semi-consciously do probably.
But a lot of this comes

02:16:03.359 --> 02:16:06.079
from my physics background.
I really, really did enjoy my physics background.

02:16:06.079 --> 02:16:10.960
I have a whole rant on how everyone 
should learn physics in early school

02:16:10.960 --> 02:16:15.840
education because early school education is 
not about accumulating knowledge or memory

02:16:15.840 --> 02:16:18.960
for tasks later in the industry.
It's about booting up a brain.

02:16:18.960 --> 02:16:22.960
Physics uniquely boots up the brain the 
best because some of the things that they

02:16:22.960 --> 02:16:26.480
get you to do in your brain during 
physics is extremely valuable later.

02:16:26.479 --> 02:16:31.359
The idea of building models and abstractions 
and understanding that there's a first-order

02:16:31.359 --> 02:16:34.799
approximation that describes most of the system, 
but then there're second-order, third-order,

02:16:34.799 --> 02:16:39.279
fourth-order terms that may or may not be present.
The idea that you're observing a very noisy

02:16:39.280 --> 02:16:43.280
system, but there are these fundamental 
frequencies that you can abstract away.

02:16:43.280 --> 02:16:47.759
When a physicist walks into the class and 
they say, "Assume there's a spherical cow,"

02:16:48.719 --> 02:16:53.039
everyone laughs at that, but this is brilliant.
It's brilliant thinking that's very generalizable

02:16:53.040 --> 02:16:58.080
across the industry because a cow can be 
approximated as a sphere in a bunch of ways.

02:16:58.799 --> 02:17:04.239
There's a really good book, for example, Scale.
It's from a physicist talking about biology.

02:17:04.239 --> 02:17:06.000
Maybe this is also a book 
I would recommend reading.

02:17:06.559 --> 02:17:11.359
You can get a lot of really interesting 
approximations and chart scaling laws of animals.

02:17:11.359 --> 02:17:15.439
You can look at their heartbeats and 
things like that, and they line up with

02:17:15.439 --> 02:17:18.960
the size of the animal and things like that.
You can talk about an animal as a volume.

02:17:20.879 --> 02:17:25.759
You can talk about the heat dissipation of that, 
because your heat dissipation grows as the surface

02:17:25.760 --> 02:17:29.840
area, which is growing as a square.
But your heat creation or generation

02:17:29.840 --> 02:17:33.120
is growing as a cube.
So I just feel like physicists

02:17:33.120 --> 02:17:36.079
have all the right cognitive tools to 
approach problem solving in the world.

02:17:36.079 --> 02:17:39.280
So because of that training, I 
always try to find the first-order

02:17:39.280 --> 02:17:43.439
terms or the second-order terms of everything.
When I'm observing a system or a thing, I have a

02:17:43.439 --> 02:17:48.960
tangle of a web of ideas or knowledge in my mind.
I'm trying to find, what is the thing that

02:17:48.959 --> 02:17:52.559
matters? What is the first-order component? 
How can I simplify it? How can I have a

02:17:52.559 --> 02:17:57.280
simplest thing that shows that thing, shows it in 
action, and then I can tack on the other terms?

02:17:58.318 --> 02:18:03.039
Maybe an example from one of my repos that I 
think illustrates it well is called micrograd.

02:18:03.040 --> 02:18:06.720
I don't know if you're familiar with this.
So micrograd is 100 lines of code

02:18:06.719 --> 02:18:10.319
that shows backpropagation.
You can create neural networks

02:18:10.318 --> 02:18:14.879
out of simple operations like plus and times, et 
cetera. Lego blocks of neural networks. You build

02:18:14.879 --> 02:18:19.119
up a computational graph and you do a forward 
pass and a backward pass to get the gradients.

02:18:19.120 --> 02:18:21.760
Now, this is at the heart of 
all neural network learning.

02:18:21.760 --> 02:18:25.360
So micrograd is a 100 lines of 
pretty interpretable Python code,

02:18:25.359 --> 02:18:29.280
and it can do forward and backward arbitrary 
neural networks, but not efficiently.

02:18:29.280 --> 02:18:32.719
So micrograd, these 100 lines of Python, 
are everything you need to understand how

02:18:32.719 --> 02:18:38.159
neural networks train. Everything else is just 
efficiency. Everything else is efficiency. There's

02:18:38.159 --> 02:18:41.359
a huge amount of work to get efficiency.
You need your tensors, you lay them out,

02:18:41.359 --> 02:18:43.519
you stride them, you make sure 
your kernels, orchestrating

02:18:43.520 --> 02:18:47.121
memory movement correctly, et cetera.
It's all just efficiency, roughly speaking.

02:18:47.120 --> 02:18:50.880
But the core intellectual piece of neural 
network training is micrograd. It's 100 lines.

02:18:50.879 --> 02:18:55.358
You can easily understand it. It's a recursive 
application of chain rule to derive the gradient,

02:18:55.359 --> 02:18:58.079
which allows you to optimize any 
arbitrary differentiable function.

02:18:58.079 --> 02:19:06.959
So I love finding these small-order terms and 
serving them on a platter and discovering them.

02:19:06.959 --> 02:19:11.759
I feel like education is the most intellectually 
interesting thing because you have a tangle

02:19:11.760 --> 02:19:16.880
of understanding and you're trying to lay 
it out in a way that creates a ramp where

02:19:16.879 --> 02:19:21.920
everything only depends on the thing before it.
I find that this untangling of knowledge is just

02:19:21.920 --> 02:19:27.200
so intellectually interesting as a cognitive task.
I love doing it personally, but I just

02:19:27.200 --> 02:19:31.200
have a fascination with trying to lay things 
out in a certain way. Maybe that helps me.

02:19:31.200 --> 02:19:35.439
It also makes the learning 
experience so much more motivated.

02:19:35.439 --> 02:19:42.559
Your tutorial on the transformer begins 
with bigrams, literally a lookup table from,

02:19:42.559 --> 02:19:46.559
"Here's the word right now, or here's 
the previous word, here's the next word."

02:19:46.559 --> 02:19:48.959
It's literally just a lookup table.
That’s the essence of it, yeah.

02:19:48.959 --> 02:19:53.599
It’s such a brilliant way, starting with a 
lookup table and then going to a transformer.

02:19:53.600 --> 02:19:57.040
Each piece is motivated. Why would you add 
that? Why would you add the next thing?

02:19:57.040 --> 02:20:01.439
You could memorize the attention formula, 
but having an understanding of why every

02:20:01.439 --> 02:20:06.000
single piece is relevant, what problem it solves.
You're presenting the pain before you present a

02:20:06.000 --> 02:20:08.478
solution, and how clever is that?
You want to take the student

02:20:08.478 --> 02:20:11.838
through that progression.
There are a lot of other small

02:20:11.840 --> 02:20:17.600
things that make it nice and engaging and 
interesting. Always prompting the student.

02:20:17.600 --> 02:20:22.000
There's a lot of small things like that are 
important and a lot of good educators will do

02:20:22.000 --> 02:20:27.680
this. How would you solve this? I'm not going to 
present the solution before you guess. That would

02:20:27.680 --> 02:20:34.240
be wasteful. That's a little bit of a…I don’t 
want to swear but it’s a dick move towards you

02:20:34.239 --> 02:20:38.148
to present you with the solution before I give 
you a shot to try to come up with it yourself.

02:20:38.148 --> 02:20:46.638
Because if you try to come up with it yourself, 
you get a better understanding of what the action

02:20:46.639 --> 02:20:52.479
space is, what the objective is, and then 
why only this action fulfills that objective.

02:20:53.439 --> 02:20:57.918
You have a chance to try it yourself, and you 
have an appreciation when I give you the solution.

02:20:58.639 --> 02:21:01.359
It maximizes the amount of 
knowledge per new fact added.

02:21:02.879 --> 02:21:10.799
Why do you think, by default, people who are 
genuine experts in their field are often bad

02:21:10.799 --> 02:21:16.399
at explaining it to somebody ramping up?
It's the curse of knowledge and expertise.

02:21:16.398 --> 02:21:20.239
This is a real phenomenon, and I suffered 
from it myself as much as I try not to.

02:21:21.040 --> 02:21:24.000
But you take certain things for granted, 
and you can't put yourself in the shoes

02:21:24.000 --> 02:21:28.079
of new people who are just starting out.
This is pervasive and happens to me as

02:21:28.079 --> 02:21:32.239
well. One thing that's extremely helpful. 
As an example, someone was trying to show

02:21:32.239 --> 02:21:37.119
me a paper in biology recently, and I just 
instantly had so many terrible questions.

02:21:38.079 --> 02:21:42.719
What I did was I used ChatGPT to ask the 
questions with the paper in the context window.

02:21:43.920 --> 02:21:47.760
It worked through some of the simple things.
Then I shared the thread to the person who

02:21:49.120 --> 02:21:54.800
wrote that paper or worked on that work.
I felt like if they could see the dumb

02:21:54.799 --> 02:21:57.679
questions I had, it might help 
them explain better in the future.

02:22:00.159 --> 02:22:04.879
For my material, I would love it if people 
shared their dumb conversations with ChatGPT

02:22:04.879 --> 02:22:07.679
about the stuff that I've created 
because it really helps me put myself

02:22:07.680 --> 02:22:14.880
again in the shoes of someone who's starting out.
Another trick that just works astoundingly well.

02:22:16.239 --> 02:22:25.199
If somebody writes a paper or a blog post or an 
announcement, it is in 100% of cases that just

02:22:25.200 --> 02:22:33.120
the narration or the transcription of how they 
would explain it to you over lunch is way more,

02:22:33.120 --> 02:22:39.280
not only understandable, but actually 
also more accurate and scientific,

02:22:39.280 --> 02:22:44.079
in the sense that people have a bias 
to explain things in the most abstract,

02:22:44.879 --> 02:22:48.479
jargon-filled way possible and to clear 
their throat for four paragraphs before

02:22:48.478 --> 02:22:51.438
they explain the central idea.
But there's something about

02:22:51.439 --> 02:22:57.679
communicating one-on-one with a person 
which compels you to just say the thing.

02:22:57.680 --> 02:23:00.639
Just say the thing. I saw that 
tweet, I thought it was really good.

02:23:00.639 --> 02:23:05.119
I shared it with a bunch of people.
I noticed this many, many times.

02:23:06.159 --> 02:23:10.000
The most prominent example is that I 
remember back in my PhD days doing research.

02:23:11.040 --> 02:23:14.560
You read someone's paper, and you 
work to understand what it's doing.

02:23:15.359 --> 02:23:18.639
Then you catch them, you're having beers 
at the conference later, and you ask them,

02:23:18.639 --> 02:23:23.200
"So this paper, what were you doing? What is the 
paper about?" They will just tell you these three

02:23:23.200 --> 02:23:26.720
sentences that perfectly captured the essence 
of that paper and totally give you the idea.

02:23:26.719 --> 02:23:30.719
And you didn't have to read the paper.
It's only when you're sitting at the table

02:23:30.719 --> 02:23:33.359
with a beer or something, and they're 
like, "Oh yeah, the paper is just,

02:23:33.359 --> 02:23:37.200
you take this idea, you take that idea and try 
this experiment and you try out this thing."

02:23:37.200 --> 02:23:40.800
They have a way of just putting it 
conversationally just perfectly.

02:23:40.799 --> 02:23:47.039
Why isn't that the abstract?
Exactly. This is coming from the

02:23:47.040 --> 02:23:51.200
perspective of how somebody who's trying to 
explain an idea should formulate it better.

02:23:51.200 --> 02:23:57.040
What is your advice as a student to other 
students, if you don't have a Karpathy

02:23:57.040 --> 02:24:00.880
who is doing the exposition of an idea?
If you're reading a paper from somebody

02:24:00.879 --> 02:24:07.119
or reading a book, what strategies do 
you employ to learn material you're

02:24:07.120 --> 02:24:13.920
interested in in fields you're not an expert at?
I don't know that I have unique tips and tricks,

02:24:13.920 --> 02:24:25.040
to be honest. It's a painful process. One thing 
that has always helped me quite a bit is—I

02:24:26.799 --> 02:24:31.679
had a small tweet about this—learning things 
on demand is pretty nice. Learning depth-wise.

02:24:31.680 --> 02:24:35.120
I do feel you need a bit of alternation of 
learning depth-wise, on demand—you're trying

02:24:35.120 --> 02:24:38.479
to achieve a certain project that you're going 
to get a reward from—and learning breadth-wise,

02:24:38.478 --> 02:24:42.318
which is just, "Oh, let's do whatever 101, 
and here's all the things you might need."

02:24:42.318 --> 02:24:45.359
Which is a lot of school—does breadth-wise 
learning, like, "Oh, trust me, you'll need

02:24:45.359 --> 02:24:50.960
this later," that kind of stuff. Okay, I trust 
you. I'll learn it because I guess I need it.

02:24:50.959 --> 02:24:53.839
But I love the kind of learning 
where you'll get a reward out of

02:24:53.840 --> 02:24:58.000
doing something, and you're learning on demand.
The other thing that I've found extremely helpful.

02:24:59.840 --> 02:25:04.880
This is an aspect where education is a bit more 
selfless, but explaining things to people is a

02:25:04.879 --> 02:25:09.039
beautiful way to learn something more deeply.
This happens to me all the time.

02:25:09.040 --> 02:25:13.359
It probably happens to other people too because 
I realize if I don't really understand something,

02:25:13.359 --> 02:25:17.200
I can't explain it.
I'm trying and I'm like,

02:25:17.200 --> 02:25:21.280
"Oh, I don't understand this."
It's so annoying to come to terms with that.

02:25:21.280 --> 02:25:25.359
You can go back and make sure you understood it.
It fills these gaps of your understanding.

02:25:25.359 --> 02:25:28.479
It forces you to come to terms 
with them and to reconcile them.

02:25:28.478 --> 02:25:33.279
I love to re-explain things and people 
should be doing that more as well.

02:25:33.280 --> 02:25:36.239
That forces you to manipulate the knowledge 
and make sure that you know what you're

02:25:36.239 --> 02:25:40.398
talking about when you're explaining it.
That's an excellent note to close on. Andrej,

02:25:40.398 --> 02:25:42.159
that was great.
Thank you.
