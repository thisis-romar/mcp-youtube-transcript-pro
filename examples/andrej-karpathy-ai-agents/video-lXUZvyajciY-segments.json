[
  {
    "start": 48.56,
    "duration": 4.4,
    "text": "Today I'm speaking with Andrej Karpathy.\nAndrej, why do you say that this will be\u00a0\u00a0"
  },
  {
    "start": 52.96,
    "duration": 5.52,
    "text": "the decade of agents and not the year of agents?\nFirst of all, thank you for having me here. I'm\u00a0\u00a0"
  },
  {
    "start": 58.48,
    "duration": 4.528,
    "text": "excited to be here. The quote you've just\u00a0\nmentioned, \"It's the decade of agents,\"\u00a0\u00a0"
  },
  {
    "start": 63.008,
    "duration": 6.512,
    "text": "is actually a reaction to a pre-existing quote.\nI'm not actually sure who said this but they were\u00a0\u00a0"
  },
  {
    "start": 69.52,
    "duration": 5.2,
    "text": "alluding to this being the year of agents with\u00a0\nrespect to LLMs and how they were going to evolve.\u00a0"
  },
  {
    "start": 76.24,
    "duration": 4.24,
    "text": "I was triggered by that because there's some\u00a0\nover-prediction going on in the industry.\u00a0"
  },
  {
    "start": 81.28,
    "duration": 4.24,
    "text": "In my mind, this is more accurately\u00a0\ndescribed as the decade of agents.\u00a0"
  },
  {
    "start": 85.52,
    "duration": 2.96,
    "text": "We have some very early agents that\u00a0\nare extremely impressive and that I\u00a0\u00a0"
  },
  {
    "start": 88.48,
    "duration": 5.76,
    "text": "use daily\u2014Claude and Codex and so on\u2014but I\u00a0\nstill feel there's so much work to be done.\u00a0"
  },
  {
    "start": 95.44,
    "duration": 2.8,
    "text": "My reaction is we'll be working\u00a0\nwith these things for a decade.\u00a0"
  },
  {
    "start": 98.24,
    "duration": 3.2,
    "text": "They're going to get better,\u00a0\nand it's going to be wonderful.\u00a0"
  },
  {
    "start": 102.0,
    "duration": 4.88,
    "text": "I was just reacting to the\u00a0\ntimelines of the implication.\u00a0"
  },
  {
    "start": 106.88,
    "duration": 3.84,
    "text": "What do you think will take a decade to\u00a0\naccomplish? What are the bottlenecks?\u00a0"
  },
  {
    "start": 111.84,
    "duration": 4.4,
    "text": "Actually making it work. When you're talking\u00a0\nabout an agent, or what the labs have in mind\u00a0\u00a0"
  },
  {
    "start": 116.24,
    "duration": 3.68,
    "text": "and maybe what I have in mind as well, you\u00a0\nshould think of it almost like an employee or\u00a0\u00a0"
  },
  {
    "start": 119.92,
    "duration": 4.16,
    "text": "an intern that you would hire to work with you.\nFor example, you work with some employees here.\u00a0"
  },
  {
    "start": 124.72,
    "duration": 4.0,
    "text": "When would you prefer to have an agent like\u00a0\nClaude or Codex do that work? Currently,\u00a0\u00a0"
  },
  {
    "start": 128.72,
    "duration": 3.04,
    "text": "of course they can't. What would it\u00a0\ntake for them to be able to do that?\u00a0"
  },
  {
    "start": 131.76,
    "duration": 2.08,
    "text": "Why don't you do it today?\nThe reason you don't do it\u00a0\u00a0"
  },
  {
    "start": 133.84,
    "duration": 3.52,
    "text": "today is because they just don't work.\nThey don't have enough intelligence,\u00a0\u00a0"
  },
  {
    "start": 137.36,
    "duration": 3.52,
    "text": "they're not multimodal enough, they\u00a0\ncan't do computer use and all this stuff.\u00a0"
  },
  {
    "start": 140.88,
    "duration": 3.28,
    "text": "They don't do a lot of the things you've\u00a0\nalluded to earlier. They don't have\u00a0\u00a0"
  },
  {
    "start": 144.16,
    "duration": 2.96,
    "text": "continual learning. You can't just tell\u00a0\nthem something and they'll remember it.\u00a0"
  },
  {
    "start": 147.12,
    "duration": 2.72,
    "text": "They're cognitively lacking\u00a0\nand it's just not working.\u00a0"
  },
  {
    "start": 150.56,
    "duration": 2.24,
    "text": "It will take about a decade to\u00a0\nwork through all of those issues.\u00a0"
  },
  {
    "start": 152.8,
    "duration": 8.4,
    "text": "Interesting. As a professional podcaster\u00a0\nand a viewer of AI from afar, it's easy\u00a0\u00a0"
  },
  {
    "start": 162.08,
    "duration": 5.04,
    "text": "for me to identify what's lacking: continual\u00a0\nlearning is lacking, or multimodality is lacking.\u00a0"
  },
  {
    "start": 167.12,
    "duration": 5.6,
    "text": "But I don't really have a good way\u00a0\nof trying to put a timeline on it.\u00a0"
  },
  {
    "start": 172.72,
    "duration": 5.2,
    "text": "If somebody asks how long continual learning\u00a0\nwill take, I have no prior about whether\u00a0\u00a0"
  },
  {
    "start": 177.92,
    "duration": 3.52,
    "text": "this is a project that should take 5\u00a0\nyears, 10 years, or 50 years. Why a\u00a0\u00a0"
  },
  {
    "start": 181.44,
    "duration": 6.24,
    "text": "decade? Why not one year? Why not 50 years?\nThis is where you get into a bit of my own\u00a0\u00a0"
  },
  {
    "start": 187.68,
    "duration": 6.32,
    "text": "intuition, and doing a bit of an extrapolation\u00a0\nwith respect to my own experience in the field.\u00a0"
  },
  {
    "start": 194.64,
    "duration": 5.28,
    "text": "I've been in AI for almost two decades.\nIt's going to be 15 years or so, not that long.\u00a0"
  },
  {
    "start": 199.92,
    "duration": 3.12,
    "text": "You had Richard Sutton here,\u00a0\nwho was around for much longer.\u00a0"
  },
  {
    "start": 203.04,
    "duration": 4.96,
    "text": "I do have about 15 years of experience of people\u00a0\nmaking predictions, of seeing how they turned out.\u00a0"
  },
  {
    "start": 208.0,
    "duration": 2.48,
    "text": "Also I was in the industry for\u00a0\na while, I was in research,\u00a0\u00a0"
  },
  {
    "start": 210.48,
    "duration": 1.92,
    "text": "and I've worked in the industry for a while.\u00a0"
  },
  {
    "start": 212.4,
    "duration": 3.92,
    "text": "I have a general intuition\u00a0\nthat I have left from that.\u00a0"
  },
  {
    "start": 217.76,
    "duration": 6.08,
    "text": "I feel like the problems are tractable, they're\u00a0\nsurmountable, but they're still difficult.\u00a0"
  },
  {
    "start": 223.84,
    "duration": 3.2,
    "text": "If I just average it out, it\u00a0\njust feels like a decade to me.\u00a0"
  },
  {
    "start": 227.04,
    "duration": 3.84,
    "text": "This is quite interesting. I want\u00a0\nto hear not only the history,\u00a0\u00a0"
  },
  {
    "start": 230.88,
    "duration": 7.2,
    "text": "but what people in the room felt was about to\u00a0\nhappen at various different breakthrough moments.\u00a0"
  },
  {
    "start": 238.08,
    "duration": 4.96,
    "text": "What were the ways in which their feelings were\u00a0\neither overly pessimistic or overly optimistic?\u00a0"
  },
  {
    "start": 243.6,
    "duration": 3.52,
    "text": "Should we just go through each of them one by one?\nThat's a giant question because you're talking\u00a0\u00a0"
  },
  {
    "start": 247.12,
    "duration": 3.76,
    "text": "about 15 years of stuff that happened.\nAI is so wonderful because there have been\u00a0\u00a0"
  },
  {
    "start": 250.88,
    "duration": 5.36,
    "text": "a number of seismic shifts where the entire\u00a0\nfield has suddenly looked a different way.\u00a0"
  },
  {
    "start": 257.04,
    "duration": 4.0,
    "text": "I've maybe lived through two or three of those.\nI still think there will continue to be\u00a0\u00a0"
  },
  {
    "start": 261.04,
    "duration": 4.16,
    "text": "some because they come with\u00a0\nalmost surprising regularity.\u00a0"
  },
  {
    "start": 265.2,
    "duration": 4.32,
    "text": "When my career began, when I started to work on\u00a0\ndeep learning, when I became interested in deep\u00a0\u00a0"
  },
  {
    "start": 269.52,
    "duration": 4.48,
    "text": "learning, this was by chance of being right next\u00a0\nto Geoff Hinton at the University of Toronto.\u00a0"
  },
  {
    "start": 274.0,
    "duration": 3.04,
    "text": "Geoff Hinton, of course, is\u00a0\nthe godfather figure of AI.\u00a0"
  },
  {
    "start": 277.04,
    "duration": 3.12,
    "text": "He was training all these neural networks.\nI thought it was incredible and interesting.\u00a0"
  },
  {
    "start": 280.16,
    "duration": 3.52,
    "text": "This was not the main thing that\u00a0\neveryone in AI was doing by far.\u00a0"
  },
  {
    "start": 283.68,
    "duration": 5.76,
    "text": "This was a niche little subject on the side.\nThat's maybe the first dramatic seismic shift\u00a0\u00a0"
  },
  {
    "start": 289.44,
    "duration": 4.56,
    "text": "that came with the AlexNet and so on.\nAlexNet reoriented everyone, and everyone\u00a0\u00a0"
  },
  {
    "start": 294.0,
    "duration": 5.6,
    "text": "started to train neural networks, but it\u00a0\nwas still very per-task, per specific task.\u00a0"
  },
  {
    "start": 299.6,
    "duration": 4.8,
    "text": "Maybe I have an image classifier or I have a\u00a0\nneural machine translator or something like that.\u00a0"
  },
  {
    "start": 304.4,
    "duration": 6.56,
    "text": "People became very slowly interested in agents.\nPeople started to think, \"Okay, maybe we have a\u00a0\u00a0"
  },
  {
    "start": 310.96,
    "duration": 3.28,
    "text": "check mark next to the visual cortex or something\u00a0\nlike that, but what about the other parts of the\u00a0\u00a0"
  },
  {
    "start": 314.24,
    "duration": 4.8,
    "text": "brain, and how can we get a full agent or a\u00a0\nfull entity that can interact in the world?\"\u00a0"
  },
  {
    "start": 319.6,
    "duration": 6.4,
    "text": "The Atari deep reinforcement learning shift\u00a0\nin 2013 or so was part of that early effort\u00a0\u00a0"
  },
  {
    "start": 326.0,
    "duration": 4.16,
    "text": "of agents, in my mind, because it was an\u00a0\nattempt to try to get agents that not just\u00a0\u00a0"
  },
  {
    "start": 330.16,
    "duration": 3.92,
    "text": "perceive the world, but also take actions and\u00a0\ninteract and get rewards from environments.\u00a0"
  },
  {
    "start": 334.08,
    "duration": 4.4,
    "text": "At the time, this was Atari games.\nI feel that was a misstep.\u00a0"
  },
  {
    "start": 339.68,
    "duration": 5.92,
    "text": "It was a misstep that even the early OpenAI that\u00a0\nI was a part of adopted because at that time,\u00a0\u00a0"
  },
  {
    "start": 345.6,
    "duration": 4.56,
    "text": "the zeitgeist was reinforcement learning\u00a0\nenvironments, games, game playing,\u00a0\u00a0"
  },
  {
    "start": 350.16,
    "duration": 3.92,
    "text": "beat games, get lots of different types of\u00a0\ngames, and OpenAI was doing a lot of that.\u00a0"
  },
  {
    "start": 354.08,
    "duration": 7.36,
    "text": "That was another prominent part of AI where maybe\u00a0\nfor two or three or four years, everyone was doing\u00a0\u00a0"
  },
  {
    "start": 361.44,
    "duration": 5.36,
    "text": "reinforcement learning on games.\nThat was all a bit of a misstep.\u00a0"
  },
  {
    "start": 366.8,
    "duration": 3.2,
    "text": "What I was trying to do at OpenAI is\u00a0\nI was always a bit suspicious of games\u00a0\u00a0"
  },
  {
    "start": 370.0,
    "duration": 3.92,
    "text": "as being this thing that would lead to AGI.\nBecause in my mind, you want something like\u00a0\u00a0"
  },
  {
    "start": 373.92,
    "duration": 3.68,
    "text": "an accountant or something that's\u00a0\ninteracting with the real world.\u00a0"
  },
  {
    "start": 377.6,
    "duration": 6.24,
    "text": "I just didn't see how games add up to it.\nMy project at OpenAI, for example, was within the\u00a0\u00a0"
  },
  {
    "start": 383.84,
    "duration": 6.24,
    "text": "scope of the Universe project, on an agent that\u00a0\nwas using keyboard and mouse to operate web pages.\u00a0"
  },
  {
    "start": 390.08,
    "duration": 3.76,
    "text": "I really wanted to have something that\u00a0\ninteracts with the actual digital world\u00a0\u00a0"
  },
  {
    "start": 393.84,
    "duration": 3.44,
    "text": "that can do knowledge work.\nIt just so turns out that this\u00a0\u00a0"
  },
  {
    "start": 397.28,
    "duration": 3.92,
    "text": "was extremely early, way too early, so early\u00a0\nthat we shouldn't have been working on that.\u00a0"
  },
  {
    "start": 402.08,
    "duration": 5.12,
    "text": "Because if you're just stumbling your way around\u00a0\nand keyboard mashing and mouse clicking and trying\u00a0\u00a0"
  },
  {
    "start": 407.2,
    "duration": 4.88,
    "text": "to get rewards in these environments, your\u00a0\nreward is too sparse and you just won't learn.\u00a0"
  },
  {
    "start": 412.08,
    "duration": 2.8,
    "text": "You're going to burn a forest\u00a0\ncomputing, and you're never\u00a0\u00a0"
  },
  {
    "start": 414.88,
    "duration": 3.68,
    "text": "going to get something off the ground.\nWhat you're missing is this power of\u00a0\u00a0"
  },
  {
    "start": 418.56,
    "duration": 3.76,
    "text": "representation in the neural network.\nFor example, today people are training\u00a0\u00a0"
  },
  {
    "start": 422.32,
    "duration": 3.44,
    "text": "those computer-using agents, but they're\u00a0\ndoing it on top of a large language model.\u00a0"
  },
  {
    "start": 425.76,
    "duration": 2.96,
    "text": "You have to get the language model first,\u00a0\nyou have to get the representations first,\u00a0\u00a0"
  },
  {
    "start": 428.72,
    "duration": 3.04,
    "text": "and you have to do that by all the\u00a0\npre-training and all the LLM stuff.\u00a0"
  },
  {
    "start": 431.76,
    "duration": 6.16,
    "text": "I feel maybe loosely speaking, people\u00a0\nkept trying to get the full thing too\u00a0\u00a0"
  },
  {
    "start": 437.92,
    "duration": 4.88,
    "text": "early a few times, where people really try\u00a0\nto go after agents too early, I would say.\u00a0"
  },
  {
    "start": 442.8,
    "duration": 3.44,
    "text": "That was Atari and Universe\u00a0\nand even my own experience.\u00a0"
  },
  {
    "start": 446.24,
    "duration": 3.44,
    "text": "You actually have to do some things\u00a0\nfirst before you get to those agents.\u00a0"
  },
  {
    "start": 450.56,
    "duration": 5.52,
    "text": "Now the agents are a lot more competent, but maybe\u00a0\nwe're still missing some parts of that stack.\u00a0"
  },
  {
    "start": 456.08,
    "duration": 4.16,
    "text": "I would say those are the three major\u00a0\nbuckets of what people were doing:\u00a0\u00a0"
  },
  {
    "start": 460.24,
    "duration": 4.32,
    "text": "training neural nets per-tasks,\u00a0\ntrying the first round of agents,\u00a0\u00a0"
  },
  {
    "start": 464.56,
    "duration": 4.08,
    "text": "and then maybe the LLMs and seeking the\u00a0\nrepresentation power of the neural networks\u00a0\u00a0"
  },
  {
    "start": 468.64,
    "duration": 5.04,
    "text": "before you tack on everything else on top.\nInteresting. If I were to steelman the Sutton\u00a0\u00a0"
  },
  {
    "start": 474.32,
    "duration": 3.52,
    "text": "perspective, it would be that humans\u00a0\ncan just take on everything at once,\u00a0\u00a0"
  },
  {
    "start": 477.84,
    "duration": 4.08,
    "text": "or even animals can take on everything at once.\nAnimals are maybe a better example because they\u00a0\u00a0"
  },
  {
    "start": 481.92,
    "duration": 3.44,
    "text": "don't even have the scaffold of language.\nThey just get thrown out into the world,\u00a0\u00a0"
  },
  {
    "start": 485.36,
    "duration": 3.36,
    "text": "and they just have to make sense\u00a0\nof everything without any labels.\u00a0"
  },
  {
    "start": 490.0,
    "duration": 4.32,
    "text": "The vision for AGI then should just be\u00a0\nsomething which looks at sensory data,\u00a0\u00a0"
  },
  {
    "start": 494.32,
    "duration": 4.48,
    "text": "looks at the computer screen, and it just\u00a0\nfigures out what's going on from scratch.\u00a0"
  },
  {
    "start": 498.8,
    "duration": 3.44,
    "text": "If a human were put in a similar situation and\u00a0\nhad to be trained from scratch\u2026 This is like a\u00a0\u00a0"
  },
  {
    "start": 502.24,
    "duration": 3.76,
    "text": "human growing up or an animal growing up.\nWhy shouldn't that be the vision for AI,\u00a0\u00a0"
  },
  {
    "start": 506.0,
    "duration": 3.92,
    "text": "rather than this thing where we're\u00a0\ndoing millions of years of training?\u00a0"
  },
  {
    "start": 509.92,
    "duration": 5.68,
    "text": "That's a really good question. Sutton was\u00a0\non your podcast and I saw the podcast and I\u00a0\u00a0"
  },
  {
    "start": 515.6,
    "duration": 4.4,
    "text": "had a write-up about that podcast that\u00a0\ngets into a bit of how I see things.\u00a0"
  },
  {
    "start": 521.6,
    "duration": 4.64,
    "text": "I'm very careful to make analogies to\u00a0\nanimals because they came about by a\u00a0\u00a0"
  },
  {
    "start": 526.24,
    "duration": 3.6,
    "text": "very different optimization process.\nAnimals are evolved, and they come\u00a0\u00a0"
  },
  {
    "start": 529.84,
    "duration": 6.0,
    "text": "with a huge amount of hardware that's built in.\nFor example, my example in the post was the zebra.\u00a0"
  },
  {
    "start": 535.84,
    "duration": 3.44,
    "text": "A zebra gets born, and a few minutes later\u00a0\nit's running around and following its mother.\u00a0"
  },
  {
    "start": 539.28,
    "duration": 4.64,
    "text": "That's an extremely complicated thing to do.\u00a0\nThat's not reinforcement learning. That's\u00a0\u00a0"
  },
  {
    "start": 543.92,
    "duration": 4.24,
    "text": "something that's baked in. Evolution obviously\u00a0\nhas some way of encoding the weights of our\u00a0\u00a0"
  },
  {
    "start": 548.16,
    "duration": 4.48,
    "text": "neural nets in ATCGs, and I have no idea\u00a0\nhow that works, but it apparently works.\u00a0"
  },
  {
    "start": 554.64,
    "duration": 5.76,
    "text": "Brains just came from a very different process,\u00a0\nand I'm very hesitant to take inspiration from it\u00a0\u00a0"
  },
  {
    "start": 560.4,
    "duration": 5.36,
    "text": "because we're not actually running that process.\nIn my post, I said we're not building animals.\u00a0"
  },
  {
    "start": 565.76,
    "duration": 4.48,
    "text": "We're building ghosts or spirits or\u00a0\nwhatever people want to call it, because\u00a0\u00a0"
  },
  {
    "start": 571.92,
    "duration": 5.6,
    "text": "we're not doing training by evolution.\nWe're doing training by imitation of humans\u00a0\u00a0"
  },
  {
    "start": 577.52,
    "duration": 5.28,
    "text": "and the data that they've put on the Internet.\nYou end up with these ethereal spirit entities\u00a0\u00a0"
  },
  {
    "start": 582.8,
    "duration": 2.56,
    "text": "because they're fully digital\u00a0\nand they're mimicking humans.\u00a0"
  },
  {
    "start": 585.36,
    "duration": 3.36,
    "text": "It's a different kind of intelligence.\nIf you imagine a space of intelligences,\u00a0\u00a0"
  },
  {
    "start": 588.72,
    "duration": 4.56,
    "text": "we're starting off at a different point almost.\u00a0\nWe're not really building animals. But it's also\u00a0\u00a0"
  },
  {
    "start": 593.28,
    "duration": 3.52,
    "text": "possible to make them a bit more animal-like\u00a0\nover time, and I think we should be doing that.\u00a0\u00a0"
  },
  {
    "start": 598.48,
    "duration": 6.4,
    "text": "One more point. I do feel Sutton has a very...\nHis framework is, \"We want to build animals.\"\u00a0"
  },
  {
    "start": 604.88,
    "duration": 2.8,
    "text": "I think that would be wonderful if we can\u00a0\nget that to work. That would be amazing.\u00a0\u00a0"
  },
  {
    "start": 607.68,
    "duration": 5.44,
    "text": "If there were a single algorithm that you\u00a0\ncan just run on the Internet and it learns\u00a0\u00a0"
  },
  {
    "start": 613.12,
    "duration": 5.68,
    "text": "everything, that would be incredible.\nI'm not sure that it exists and that's\u00a0\u00a0"
  },
  {
    "start": 618.8,
    "duration": 4.64,
    "text": "certainly not what animals do, because\u00a0\nanimals have this outer loop of evolution.\u00a0"
  },
  {
    "start": 624.4,
    "duration": 4.0,
    "text": "A lot of what looks like learning is\u00a0\nmore like maturation of the brain.\u00a0"
  },
  {
    "start": 628.4,
    "duration": 4.48,
    "text": "I think there's very little\u00a0\nreinforcement learning for animals.\u00a0"
  },
  {
    "start": 632.88,
    "duration": 4.24,
    "text": "A lot of the reinforcement learning is more\u00a0\nlike motor tasks; it's not intelligence tasks.\u00a0"
  },
  {
    "start": 637.12,
    "duration": 3.36,
    "text": "So I actually kind of think humans\u00a0\ndon\u2019t really use RL, roughly speaking.\u00a0"
  },
  {
    "start": 641.12,
    "duration": 1.84,
    "text": "Can you repeat the last sentence?\nA lot of that intelligence is\u00a0\u00a0"
  },
  {
    "start": 642.96,
    "duration": 2.64,
    "text": "not motor task\u2026it's what, sorry?\nA lot of the reinforcement learning, in my\u00a0\u00a0"
  },
  {
    "start": 645.6,
    "duration": 6.08,
    "text": "perspective, would be things that are a lot more\u00a0\nmotor-like, simple tasks like throwing a hoop.\u00a0"
  },
  {
    "start": 653.44,
    "duration": 4.0,
    "text": "But I don't think that humans use reinforcement\u00a0\nlearning for a lot of intelligence tasks\u00a0\u00a0"
  },
  {
    "start": 657.44,
    "duration": 4.08,
    "text": "like problem-solving and so on.\nThat doesn't mean we shouldn't\u00a0\u00a0"
  },
  {
    "start": 661.52,
    "duration": 4.4,
    "text": "do that for research, but I just feel\u00a0\nlike that's what animals do or don't.\u00a0"
  },
  {
    "start": 665.92,
    "duration": 3.68,
    "text": "I'm going to take a second to digest that\u00a0\nbecause there are a lot of different ideas.\u00a0"
  },
  {
    "start": 669.6,
    "duration": 4.64,
    "text": "Here\u2019s one clarifying question I can\u00a0\nask to understand the perspective.\u00a0"
  },
  {
    "start": 675.28,
    "duration": 3.36,
    "text": "You suggest that evolution is doing\u00a0\nthe kind of thing that pre-training\u00a0\u00a0"
  },
  {
    "start": 678.64,
    "duration": 5.36,
    "text": "does in the sense of building something\u00a0\nwhich can then understand the world.\u00a0"
  },
  {
    "start": 684.0,
    "duration": 4.32,
    "text": "The difference is that evolution\u00a0\nhas to be titrated in the case\u00a0\u00a0"
  },
  {
    "start": 688.32,
    "duration": 9.04,
    "text": "of humans through three gigabytes of DNA.\nThat's very unlike the weights of a model.\u00a0"
  },
  {
    "start": 697.36,
    "duration": 5.44,
    "text": "Literally, the weights of the model are\u00a0\na brain, which obviously does not exist\u00a0\u00a0"
  },
  {
    "start": 702.8,
    "duration": 3.76,
    "text": "in the sperm and the egg.\nSo it has to be grown.\u00a0"
  },
  {
    "start": 706.56,
    "duration": 4.08,
    "text": "Also, the information for every single\u00a0\nsynapse in the brain simply cannot exist\u00a0\u00a0"
  },
  {
    "start": 710.64,
    "duration": 4.96,
    "text": "in the three gigabytes that exist in the DNA.\nEvolution seems closer to finding the algorithm\u00a0\u00a0"
  },
  {
    "start": 715.6,
    "duration": 5.04,
    "text": "which then does the lifetime learning.\nNow, maybe the lifetime learning is\u00a0\u00a0"
  },
  {
    "start": 720.64,
    "duration": 4.0,
    "text": "not analogous to RL, to your point.\nIs that compatible with the thing you\u00a0\u00a0"
  },
  {
    "start": 724.64,
    "duration": 2.32,
    "text": "were saying, or would you disagree with that?\nI think so. I would agree with you that there's\u00a0\u00a0"
  },
  {
    "start": 726.96,
    "duration": 2.08,
    "text": "some miraculous compression\u00a0\ngoing on because obviously,\u00a0\u00a0"
  },
  {
    "start": 729.04,
    "duration": 4.96,
    "text": "the weights of the neural net are not stored\u00a0\nin ATCGs. There's some dramatic compression.\u00a0\u00a0"
  },
  {
    "start": 734.0,
    "duration": 4.88,
    "text": "There are some learning algorithms encoded that\u00a0\ntake over and do some of the learning online.\u00a0"
  },
  {
    "start": 738.88,
    "duration": 4.72,
    "text": "I definitely agree with you on that.\nI would say I'm a lot more practically minded.\u00a0"
  },
  {
    "start": 743.6,
    "duration": 2.48,
    "text": "I don't come at it from the\u00a0\nperspective of, let's build animals.\u00a0"
  },
  {
    "start": 746.08,
    "duration": 2.88,
    "text": "I come from it from the perspective\u00a0\nof, let's build useful things.\u00a0"
  },
  {
    "start": 748.96,
    "duration": 2.96,
    "text": "I have a hard hat on, and I'm just\u00a0\nobserving that we're not going to do\u00a0\u00a0"
  },
  {
    "start": 751.92,
    "duration": 5.04,
    "text": "evolution, because I don't know how to do that.\nBut it does turn out we can build these ghosts,\u00a0\u00a0"
  },
  {
    "start": 756.96,
    "duration": 6.16,
    "text": "spirit-like entities, by imitating internet\u00a0\ndocuments. This works. It's a way to bring you\u00a0\u00a0"
  },
  {
    "start": 763.12,
    "duration": 5.28,
    "text": "up to something that has a lot of built-in\u00a0\nknowledge and intelligence in some way,\u00a0\u00a0"
  },
  {
    "start": 768.4,
    "duration": 3.52,
    "text": "similar to maybe what evolution has done.\nThat's why I call pre-training\u00a0\u00a0"
  },
  {
    "start": 771.92,
    "duration": 4.24,
    "text": "this crappy evolution.\nIt's the practically possible version\u00a0\u00a0"
  },
  {
    "start": 776.16,
    "duration": 5.04,
    "text": "with our technology and what we have available\u00a0\nto us to get to a starting point where we can do\u00a0\u00a0"
  },
  {
    "start": 781.2,
    "duration": 4.16,
    "text": "things like reinforcement learning and so on.\nJust to steelman the other perspective,\u00a0\u00a0"
  },
  {
    "start": 785.36,
    "duration": 3.84,
    "text": "after doing this Sutton interview and thinking\u00a0\nabout it a bit, he has an important point here.\u00a0"
  },
  {
    "start": 789.2,
    "duration": 4.88,
    "text": "Evolution does not give us the knowledge, really.\nIt gives us the algorithm to find the knowledge,\u00a0\u00a0"
  },
  {
    "start": 794.08,
    "duration": 5.36,
    "text": "and that seems different from pre-training.\nPerhaps the perspective is that pre-training helps\u00a0\u00a0"
  },
  {
    "start": 799.44,
    "duration": 3.84,
    "text": "build the kind of entity which can learn better.\nIt teaches meta-learning, and therefore\u00a0\u00a0"
  },
  {
    "start": 803.28,
    "duration": 5.04,
    "text": "it is similar to finding an algorithm.\nBut if it's \"Evolution gives us knowledge,\u00a0\u00a0"
  },
  {
    "start": 808.32,
    "duration": 3.12,
    "text": "pre-training gives us knowledge,\"\u00a0\nthat analogy seems to break down.\u00a0"
  },
  {
    "start": 811.44,
    "duration": 3.6,
    "text": "It's subtle and I think you're right to\u00a0\npush back on it, but basically the thing\u00a0\u00a0"
  },
  {
    "start": 815.04,
    "duration": 3.84,
    "text": "that pre-training is doing, you're getting\u00a0\nthe next-token predictor over the internet,\u00a0\u00a0"
  },
  {
    "start": 818.88,
    "duration": 4.8,
    "text": "and you're training that into a neural net.\nIt's doing two things that are unrelated.\u00a0"
  },
  {
    "start": 823.68,
    "duration": 2.4,
    "text": "Number one, it's picking up all\u00a0\nthis knowledge, as I call it.\u00a0"
  },
  {
    "start": 826.08,
    "duration": 5.04,
    "text": "Number two, it's actually becoming intelligent.\nBy observing the algorithmic patterns in the\u00a0\u00a0"
  },
  {
    "start": 831.12,
    "duration": 4.56,
    "text": "internet, it boots up all these little circuits\u00a0\nand algorithms inside the neural net to do things\u00a0\u00a0"
  },
  {
    "start": 835.68,
    "duration": 5.12,
    "text": "like in-context learning and all this stuff.\nYou don't need or want the knowledge.\u00a0"
  },
  {
    "start": 840.8,
    "duration": 4.48,
    "text": "I think that's probably holding back the neural\u00a0\nnetworks overall because it's getting them to rely\u00a0\u00a0"
  },
  {
    "start": 845.28,
    "duration": 4.48,
    "text": "on the knowledge a little too much sometimes.\nFor example, I feel agents, one thing they're\u00a0\u00a0"
  },
  {
    "start": 849.76,
    "duration": 3.68,
    "text": "not very good at, is going off the data\u00a0\nmanifold of what exists on the internet.\u00a0"
  },
  {
    "start": 853.44,
    "duration": 3.92,
    "text": "If they had less knowledge or less\u00a0\nmemory, maybe they would be better.\u00a0"
  },
  {
    "start": 857.92,
    "duration": 3.76,
    "text": "What I think we have to do going forward\u2014and\u00a0\nthis would be part of the research paradigms\u2014is\u00a0\u00a0"
  },
  {
    "start": 863.68,
    "duration": 5.04,
    "text": "figure out ways to remove some of the knowledge\u00a0\nand to keep what I call this cognitive core.\u00a0"
  },
  {
    "start": 868.72,
    "duration": 5.36,
    "text": "It's this intelligent entity that is stripped from\u00a0\nknowledge but contains the algorithms and contains\u00a0\u00a0"
  },
  {
    "start": 874.08,
    "duration": 5.28,
    "text": "the magic of intelligence and problem-solving\u00a0\nand the strategies of it and all this stuff.\u00a0"
  },
  {
    "start": 879.36,
    "duration": 4.72,
    "text": "There's so much interesting stuff there. Let's\u00a0\nstart with in-context learning. This is an\u00a0\u00a0"
  },
  {
    "start": 884.08,
    "duration": 4.72,
    "text": "obvious point, but I think it's worth just\u00a0\nsaying it explicitly and meditating on it.\u00a0"
  },
  {
    "start": 888.8,
    "duration": 5.04,
    "text": "The situation in which these models seem the most\u00a0\nintelligent\u2014in which I talk to them and I'm like,\u00a0\u00a0"
  },
  {
    "start": 893.84,
    "duration": 4.4,
    "text": "\"Wow, there's really something on the other end\u00a0\nthat's responding to me thinking about things\u2014is\u00a0\u00a0"
  },
  {
    "start": 898.24,
    "duration": 3.6,
    "text": "if it makes a mistake it's like, \"Oh wait, that's\u00a0\nthe wrong way to think about it. I'm backing up.\"\u00a0\u00a0"
  },
  {
    "start": 901.84,
    "duration": 2.64,
    "text": "All that is happening in context.\nThat's where I feel like the real\u00a0\u00a0"
  },
  {
    "start": 904.48,
    "duration": 6.08,
    "text": "intelligence is that you can visibly see.\nThat in-context learning process is\u00a0\u00a0"
  },
  {
    "start": 910.56,
    "duration": 5.76,
    "text": "developed by gradient descent on pre-training.\nIt spontaneously meta-learns in-context learning,\u00a0\u00a0"
  },
  {
    "start": 916.32,
    "duration": 5.28,
    "text": "but the in-context learning itself is not\u00a0\ngradient descent, in the same way that our\u00a0\u00a0"
  },
  {
    "start": 921.6,
    "duration": 4.96,
    "text": "lifetime intelligence as humans to be able\u00a0\nto do things is conditioned by evolution\u00a0\u00a0"
  },
  {
    "start": 926.56,
    "duration": 4.32,
    "text": "but our learning during our lifetime is\u00a0\nhappening through some other process.\u00a0"
  },
  {
    "start": 930.88,
    "duration": 3.6,
    "text": "I don't fully agree with that, but\u00a0\nyou should continue your thought.\u00a0"
  },
  {
    "start": 934.48,
    "duration": 2.4,
    "text": "Well, I'm very curious to understand\u00a0\nhow that analogy breaks down.\u00a0"
  },
  {
    "start": 936.88,
    "duration": 3.76,
    "text": "I'm hesitant to say that in-context\u00a0\nlearning is not doing gradient descent.\u00a0"
  },
  {
    "start": 941.76,
    "duration": 5.52,
    "text": "It's not doing explicit gradient descent.\nIn-context learning is pattern completion\u00a0\u00a0"
  },
  {
    "start": 947.28,
    "duration": 3.04,
    "text": "within a token window.\nIt just turns out that there's\u00a0\u00a0"
  },
  {
    "start": 950.32,
    "duration": 3.44,
    "text": "a huge amount of patterns on the internet.\nYou're right, the model learns to complete\u00a0\u00a0"
  },
  {
    "start": 953.76,
    "duration": 4.24,
    "text": "the pattern, and that's inside the weights.\nThe weights of the neural network are trying\u00a0\u00a0"
  },
  {
    "start": 958.0,
    "duration": 4.08,
    "text": "to discover patterns and complete the pattern.\nThere's some adaptation that happens inside\u00a0\u00a0"
  },
  {
    "start": 962.08,
    "duration": 4.08,
    "text": "the neural network, which is magical\u00a0\nand just falls out from the internet\u00a0\u00a0"
  },
  {
    "start": 966.16,
    "duration": 4.4,
    "text": "just because there's a lot of patterns.\nI will say that there have been some papers\u00a0\u00a0"
  },
  {
    "start": 970.56,
    "duration": 3.44,
    "text": "that I thought were interesting that look at\u00a0\nthe mechanisms behind in-context learning.\u00a0"
  },
  {
    "start": 974.0,
    "duration": 3.68,
    "text": "I do think it's possible that in-context\u00a0\nlearning runs a small gradient descent loop\u00a0\u00a0"
  },
  {
    "start": 977.68,
    "duration": 4.64,
    "text": "internally in the layers of the neural network.\nI recall one paper in particular where they were\u00a0\u00a0"
  },
  {
    "start": 982.32,
    "duration": 8.32,
    "text": "doing linear regression using in-context learning.\nYour inputs into the neural network are XY pairs,\u00a0\u00a0"
  },
  {
    "start": 990.64,
    "duration": 4.96,
    "text": "XY, XY, XY that happen to be on the line.\nThen you do X and you expect Y.\u00a0"
  },
  {
    "start": 995.6,
    "duration": 5.04,
    "text": "The neural network, when you train it\u00a0\nin this way, does linear regression.\u00a0"
  },
  {
    "start": 1001.52,
    "duration": 4.72,
    "text": "Normally when you would run linear regression, you\u00a0\nhave a small gradient descent optimizer that looks\u00a0\u00a0"
  },
  {
    "start": 1006.24,
    "duration": 4.4,
    "text": "at XY, looks at an error, calculates the gradient\u00a0\nof the weights and does the update a few times.\u00a0"
  },
  {
    "start": 1010.64,
    "duration": 4.16,
    "text": "It just turns out that when they looked at the\u00a0\nweights of that in-context learning algorithm,\u00a0\u00a0"
  },
  {
    "start": 1014.8,
    "duration": 4.32,
    "text": "they found some analogies to\u00a0\ngradient descent mechanics.\u00a0"
  },
  {
    "start": 1019.12,
    "duration": 4.8,
    "text": "In fact, I think the paper was even stronger\u00a0\nbecause they hardcoded the weights of a neural\u00a0\u00a0"
  },
  {
    "start": 1023.92,
    "duration": 6.08,
    "text": "network to do gradient descent through\u00a0\nattention and all the internals of the\u00a0\u00a0"
  },
  {
    "start": 1030.0,
    "duration": 4.56,
    "text": "neural network. That's just my only pushback.\u00a0\nWho knows how in-context learning works,\u00a0\u00a0"
  },
  {
    "start": 1034.56,
    "duration": 5.36,
    "text": "but I think that it's probably doing a bit of some\u00a0\nfunky gradient descent internally. I think that\u00a0\u00a0"
  },
  {
    "start": 1039.92,
    "duration": 4.88,
    "text": "that's possible. I was only pushing back on your\u00a0\nsaying that it's not doing in-context learning.\u00a0"
  },
  {
    "start": 1044.8,
    "duration": 3.44,
    "text": "Who knows what it's doing, but it's probably maybe\u00a0\ndoing something similar to it, but we don't know.\u00a0"
  },
  {
    "start": 1048.24,
    "duration": 6.24,
    "text": "So then it's worth thinking okay, if\u00a0\nin-context learning and pre-training\u00a0\u00a0"
  },
  {
    "start": 1054.48,
    "duration": 4.8,
    "text": "are both implementing something like gradient\u00a0\ndescent, why does it feel like with in-context\u00a0\u00a0"
  },
  {
    "start": 1059.28,
    "duration": 5.36,
    "text": "learning we're getting to this continual\u00a0\nlearning, real intelligence-like thing?\u00a0"
  },
  {
    "start": 1064.64,
    "duration": 5.28,
    "text": "Whereas you don't get the analogous feeling just\u00a0\nfrom pre-training. You could argue that. If it's\u00a0\u00a0"
  },
  {
    "start": 1069.92,
    "duration": 3.12,
    "text": "the same algorithm, what could be different?\nOne way you could think about it is,\u00a0\u00a0"
  },
  {
    "start": 1073.04,
    "duration": 7.2,
    "text": "how much information does the model store\u00a0\nper information it receives from training?\u00a0"
  },
  {
    "start": 1080.24,
    "duration": 3.6,
    "text": "If you look at pre-training, if\u00a0\nyou look at Llama 3 for example,\u00a0\u00a0"
  },
  {
    "start": 1083.84,
    "duration": 6.16,
    "text": "I think it's trained on 15 trillion tokens.\nIf you look at the 70B model, that would\u00a0\u00a0"
  },
  {
    "start": 1090.0,
    "duration": 5.28,
    "text": "be the equivalent of 0.07 bits per\u00a0\ntoken that it sees in pre-training,\u00a0\u00a0"
  },
  {
    "start": 1095.28,
    "duration": 3.52,
    "text": "in terms of the information in the weights\u00a0\nof the model compared to the tokens it reads.\u00a0"
  },
  {
    "start": 1098.8,
    "duration": 4.0,
    "text": "Whereas if you look at the KV cache\u00a0\nand how it grows per additional token\u00a0\u00a0"
  },
  {
    "start": 1102.8,
    "duration": 6.48,
    "text": "in in-context learning, it's like 320 kilobytes.\nSo that's a 35 million-fold difference in how much\u00a0\u00a0"
  },
  {
    "start": 1109.28,
    "duration": 5.6,
    "text": "information per token is assimilated by the model.\nI wonder if that's relevant at all.\u00a0"
  },
  {
    "start": 1114.88,
    "duration": 4.88,
    "text": "I kind of agree. The way I usually put this is\u00a0\nthat anything that happens during the training of\u00a0\u00a0"
  },
  {
    "start": 1119.76,
    "duration": 5.84,
    "text": "the neural network, the knowledge is only a hazy\u00a0\nrecollection of what happened in training time.\u00a0"
  },
  {
    "start": 1125.6,
    "duration": 3.28,
    "text": "That's because the compression is dramatic.\nYou're taking 15 trillion tokens and you're\u00a0\u00a0"
  },
  {
    "start": 1128.88,
    "duration": 2.8,
    "text": "compressing it to just your final neural\u00a0\nnetwork of a few billion parameters.\u00a0"
  },
  {
    "start": 1131.68,
    "duration": 2.16,
    "text": "Obviously it's a massive\u00a0\namount of compression going on.\u00a0"
  },
  {
    "start": 1134.56,
    "duration": 3.36,
    "text": "So I refer to it as a hazy\u00a0\nrecollection of the internet documents.\u00a0"
  },
  {
    "start": 1137.92,
    "duration": 2.8,
    "text": "Whereas anything that happens in the\u00a0\ncontext window of the neural network\u2014you're\u00a0\u00a0"
  },
  {
    "start": 1140.72,
    "duration": 3.52,
    "text": "plugging in all the tokens and building up\u00a0\nall those KV cache representations\u2014is very\u00a0\u00a0"
  },
  {
    "start": 1144.24,
    "duration": 3.76,
    "text": "directly accessible to the neural net.\nSo I compare the KV cache and the stuff\u00a0\u00a0"
  },
  {
    "start": 1148.0,
    "duration": 3.6,
    "text": "that happens at test time to\u00a0\nmore like a working memory.\u00a0"
  },
  {
    "start": 1151.6,
    "duration": 4.64,
    "text": "All the stuff that's in the context window is\u00a0\nvery directly accessible to the neural net.\u00a0"
  },
  {
    "start": 1156.24,
    "duration": 4.88,
    "text": "There's always these almost surprising\u00a0\nanalogies between LLMs and humans.\u00a0"
  },
  {
    "start": 1161.12,
    "duration": 4.16,
    "text": "I find them surprising because we're not\u00a0\ntrying to build a human brain directly.\u00a0"
  },
  {
    "start": 1165.28,
    "duration": 2.0,
    "text": "We're just finding that this\u00a0\nworks and we're doing it.\u00a0"
  },
  {
    "start": 1167.28,
    "duration": 3.44,
    "text": "But I do think that anything\u00a0\nthat's in the weights, it's a\u00a0\u00a0"
  },
  {
    "start": 1170.72,
    "duration": 5.36,
    "text": "hazy recollection of what you read a year ago.\nAnything that you give it as a context at test\u00a0\u00a0"
  },
  {
    "start": 1176.08,
    "duration": 4.0,
    "text": "time is directly in the working memory.\nThat's a very powerful analogy to\u00a0\u00a0"
  },
  {
    "start": 1180.08,
    "duration": 2.08,
    "text": "think through things.\nWhen you, for example,\u00a0\u00a0"
  },
  {
    "start": 1182.16,
    "duration": 3.84,
    "text": "go to an LLM and you ask it about some book\u00a0\nand what happened in it, like Nick Lane's\u00a0\u00a0"
  },
  {
    "start": 1186.0,
    "duration": 3.68,
    "text": "book or something like that, the LLM will often\u00a0\ngive you some stuff which is roughly correct.\u00a0"
  },
  {
    "start": 1189.68,
    "duration": 3.92,
    "text": "But if you give it the full chapter and ask it\u00a0\nquestions, you're going to get much better results\u00a0\u00a0"
  },
  {
    "start": 1193.6,
    "duration": 2.64,
    "text": "because it's now loaded in the\u00a0\nworking memory of the model.\u00a0"
  },
  {
    "start": 1196.24,
    "duration": 4.08,
    "text": "So a very long way of saying\u00a0\nI agree and that's why.\u00a0"
  },
  {
    "start": 1200.32,
    "duration": 2.56,
    "text": "Stepping back, what is the part\u00a0\nabout human intelligence that we\u00a0\u00a0"
  },
  {
    "start": 1203.76,
    "duration": 4.64,
    "text": "have most failed to replicate with these models?\u00a0"
  },
  {
    "start": 1212.4,
    "duration": 4.72,
    "text": "Just a lot of it. So maybe one way to think\u00a0\nabout it, I don't know if this is the best way,\u00a0\u00a0"
  },
  {
    "start": 1217.12,
    "duration": 5.52,
    "text": "but I almost feel like \u2014 again, making these\u00a0\nanalogies imperfect as they are \u2014 we've stumbled\u00a0\u00a0"
  },
  {
    "start": 1222.64,
    "duration": 4.72,
    "text": "by with the transformer neural network,\u00a0\nwhich is extremely powerful, very general.\u00a0"
  },
  {
    "start": 1227.36,
    "duration": 4.0,
    "text": "You can train transformers on audio, or\u00a0\nvideo, or text, or whatever you want,\u00a0\u00a0"
  },
  {
    "start": 1231.36,
    "duration": 3.92,
    "text": "and it just learns patterns and they're\u00a0\nvery powerful, and it works really well.\u00a0"
  },
  {
    "start": 1235.28,
    "duration": 3.68,
    "text": "That to me almost indicates that this\u00a0\nis some piece of cortical tissue.\u00a0"
  },
  {
    "start": 1238.96,
    "duration": 3.68,
    "text": "It's something like that, because the\u00a0\ncortex is famously very plastic as well.\u00a0"
  },
  {
    "start": 1242.64,
    "duration": 5.44,
    "text": "You can rewire parts of brains.\nThere were the slightly gruesome experiments\u00a0\u00a0"
  },
  {
    "start": 1248.08,
    "duration": 6.08,
    "text": "with rewiring the visual cortex to the auditory\u00a0\ncortex, and this animal learned fine, et cetera.\u00a0"
  },
  {
    "start": 1254.16,
    "duration": 4.4,
    "text": "So I think that this is cortical tissue.\nI think when we're doing reasoning and\u00a0\u00a0"
  },
  {
    "start": 1258.56,
    "duration": 5.44,
    "text": "planning inside the neural networks, doing\u00a0\nreasoning traces for thinking models,\u00a0\u00a0"
  },
  {
    "start": 1264.0,
    "duration": 7.6,
    "text": "that's kind of like the prefrontal cortex.\nMaybe those are like little checkmarks,\u00a0\u00a0"
  },
  {
    "start": 1271.6,
    "duration": 3.92,
    "text": "but I still think there are many brain\u00a0\nparts and nuclei that are not explored.\u00a0"
  },
  {
    "start": 1275.52,
    "duration": 2.96,
    "text": "For example, there's a basal ganglia doing a\u00a0\nbit of reinforcement learning when we fine-tune\u00a0\u00a0"
  },
  {
    "start": 1278.48,
    "duration": 5.12,
    "text": "the models on reinforcement learning. But where's\u00a0\nthe hippocampus? Not obvious what that would be.\u00a0"
  },
  {
    "start": 1283.6,
    "duration": 2.8,
    "text": "Some parts are probably not important.\nMaybe the cerebellum is not important\u00a0\u00a0"
  },
  {
    "start": 1286.4,
    "duration": 2.72,
    "text": "to cognition, its thoughts, so\u00a0\nmaybe we can skip some of it.\u00a0"
  },
  {
    "start": 1289.12,
    "duration": 3.6,
    "text": "But I still think there's, for example, the\u00a0\namygdala, all the emotions and instincts.\u00a0"
  },
  {
    "start": 1293.44,
    "duration": 3.36,
    "text": "There's probably a bunch of other nuclei\u00a0\nin the brain that are very ancient that\u00a0\u00a0"
  },
  {
    "start": 1296.8,
    "duration": 4.48,
    "text": "I don't think we've really replicated.\nI don't know that we should be pursuing the\u00a0\u00a0"
  },
  {
    "start": 1301.28,
    "duration": 4.24,
    "text": "building of an analog of a human brain.\nI'm an engineer mostly at heart.\u00a0"
  },
  {
    "start": 1308.16,
    "duration": 4.0,
    "text": "Maybe another way to answer the question is that\u00a0\nyou're not going to hire this thing as an intern.\u00a0"
  },
  {
    "start": 1312.16,
    "duration": 3.36,
    "text": "It's missing a lot of it because it comes with\u00a0\na lot of these cognitive deficits that we all\u00a0\u00a0"
  },
  {
    "start": 1315.52,
    "duration": 4.88,
    "text": "intuitively feel when we talk to the models.\nSo it's not fully there yet.\u00a0"
  },
  {
    "start": 1320.4,
    "duration": 4.08,
    "text": "You can look at it as not all the\u00a0\nbrain parts are checked off yet.\u00a0"
  },
  {
    "start": 1324.48,
    "duration": 5.6,
    "text": "This is maybe relevant to the question of thinking\u00a0\nabout how fast these issues will be solved.\u00a0"
  },
  {
    "start": 1330.8,
    "duration": 2.48,
    "text": "Sometimes people will say\u00a0\nabout continual learning,\u00a0\u00a0"
  },
  {
    "start": 1333.28,
    "duration": 6.64,
    "text": "\"Look, you could easily replicate this capability.\nJust as in-context learning emerged spontaneously\u00a0\u00a0"
  },
  {
    "start": 1339.92,
    "duration": 5.04,
    "text": "as a result of pre-training, continual\u00a0\nlearning over longer horizons will emerge\u00a0\u00a0"
  },
  {
    "start": 1344.96,
    "duration": 5.76,
    "text": "spontaneously if the model is incentivized to\u00a0\nrecollect information over longer horizons,\u00a0\u00a0"
  },
  {
    "start": 1350.72,
    "duration": 8.96,
    "text": "or horizons longer than one session.\"\nSo if there's some outer loop RL which has\u00a0\u00a0"
  },
  {
    "start": 1359.68,
    "duration": 6.4,
    "text": "many sessions within that outer loop, then this\u00a0\ncontinual learning where it fine-tunes itself,\u00a0\u00a0"
  },
  {
    "start": 1366.08,
    "duration": 3.44,
    "text": "or it writes to an external memory or\u00a0\nsomething, will just emerge spontaneously.\u00a0"
  },
  {
    "start": 1369.52,
    "duration": 4.24,
    "text": "Do you think things like that are plausible?\nI just don't have a prior over\u00a0\u00a0"
  },
  {
    "start": 1373.76,
    "duration": 1.68,
    "text": "how plausible that is.\nHow likely is that to happen?\u00a0"
  },
  {
    "start": 1375.44,
    "duration": 4.16,
    "text": "I don't know that I fully resonate with that.\nThese models, when you boot them up and they have\u00a0\u00a0"
  },
  {
    "start": 1379.6,
    "duration": 3.44,
    "text": "zero tokens in the window, they're always\u00a0\nrestarting from scratch where they were.\u00a0"
  },
  {
    "start": 1383.04,
    "duration": 3.36,
    "text": "So I don't know in that\u00a0\nworldview what it looks like.\u00a0"
  },
  {
    "start": 1389.36,
    "duration": 4.24,
    "text": "Maybe making some analogies to humans\u2014just because\u00a0\nI think it's roughly concrete and interesting to\u00a0\u00a0"
  },
  {
    "start": 1393.6,
    "duration": 3.6,
    "text": "think through\u2014I feel like when I'm awake, I'm\u00a0\nbuilding up a context window of stuff that's\u00a0\u00a0"
  },
  {
    "start": 1397.2,
    "duration": 2.08,
    "text": "happening during the day.\nBut when I go to sleep,\u00a0\u00a0"
  },
  {
    "start": 1399.28,
    "duration": 3.84,
    "text": "something magical happens where I don't\u00a0\nthink that context window stays around.\u00a0"
  },
  {
    "start": 1403.84,
    "duration": 3.28,
    "text": "There's some process of distillation\u00a0\ninto the weights of my brain.\u00a0"
  },
  {
    "start": 1407.84,
    "duration": 2.88,
    "text": "This happens during sleep and all this stuff.\nWe don't have an equivalent\u00a0\u00a0"
  },
  {
    "start": 1410.72,
    "duration": 4.96,
    "text": "of that in large language models.\nThat's to me more adjacent to when you talk\u00a0\u00a0"
  },
  {
    "start": 1415.68,
    "duration": 4.48,
    "text": "about continual learning and so on as absent.\nThese models don't really have a distillation\u00a0\u00a0"
  },
  {
    "start": 1420.16,
    "duration": 7.76,
    "text": "phase of taking what happened, analyzing it\u00a0\nobsessively, thinking through it, doing some\u00a0\u00a0"
  },
  {
    "start": 1427.92,
    "duration": 4.0,
    "text": "synthetic data generation process and distilling\u00a0\nit back into the weights, and maybe having\u00a0\u00a0"
  },
  {
    "start": 1431.92,
    "duration": 9.28,
    "text": "a specific neural net per person. Maybe it's\u00a0\na LoRA. It's not a full-weight neural network.\u00a0"
  },
  {
    "start": 1441.2,
    "duration": 3.84,
    "text": "It's just some small sparse subset\u00a0\nof the weights that are changed.\u00a0"
  },
  {
    "start": 1445.04,
    "duration": 5.52,
    "text": "But we do want to create ways of creating\u00a0\nthese individuals that have very long context.\u00a0"
  },
  {
    "start": 1450.56,
    "duration": 4.32,
    "text": "It's not only remaining in the context window\u00a0\nbecause the context windows grow very, very long.\u00a0"
  },
  {
    "start": 1454.88,
    "duration": 2.8,
    "text": "Maybe we have some very elaborate,\u00a0\nsparse attention over it.\u00a0"
  },
  {
    "start": 1457.68,
    "duration": 4.48,
    "text": "But I still think that humans obviously have some\u00a0\nprocess for distilling some of that knowledge\u00a0\u00a0"
  },
  {
    "start": 1462.16,
    "duration": 5.28,
    "text": "into the weights. We're missing it. I do also\u00a0\nthink that humans have some very elaborate,\u00a0\u00a0"
  },
  {
    "start": 1467.44,
    "duration": 5.6,
    "text": "sparse attention scheme, which I think\u00a0\nwe're starting to see some early hints of.\u00a0"
  },
  {
    "start": 1473.04,
    "duration": 5.44,
    "text": "DeepSeek v3.2 just came out and I saw that they\u00a0\nhave sparse attention as an example, and this is\u00a0\u00a0"
  },
  {
    "start": 1478.48,
    "duration": 5.68,
    "text": "one way to have very, very long context windows.\nSo I feel like we are redoing a lot of the\u00a0\u00a0"
  },
  {
    "start": 1484.16,
    "duration": 3.12,
    "text": "cognitive tricks that evolution came up\u00a0\nwith through a very different process.\u00a0"
  },
  {
    "start": 1487.28,
    "duration": 2.96,
    "text": "But we're going to converge on a\u00a0\nsimilar architecture cognitively.\u00a0"
  },
  {
    "start": 1491.04,
    "duration": 4.16,
    "text": "In 10 years, do you think it'll still be something\u00a0\nlike a transformer, but with much more modified\u00a0\u00a0"
  },
  {
    "start": 1495.2,
    "duration": 4.56,
    "text": "attention and more sparse MLPs and so forth?\nThe way I like to think about it is\u00a0\u00a0"
  },
  {
    "start": 1500.56,
    "duration": 4.64,
    "text": "translation invariance in time.\nSo 10 years ago, where were we? 2015.\u00a0"
  },
  {
    "start": 1505.2,
    "duration": 4.16,
    "text": "In 2015, we had convolutional neural networks\u00a0\nprimarily, residual networks just came out.\u00a0"
  },
  {
    "start": 1510.32,
    "duration": 3.68,
    "text": "So remarkably similar, I guess, but quite a\u00a0\nbit different still. The transformer was not\u00a0\u00a0"
  },
  {
    "start": 1514.0,
    "duration": 7.76,
    "text": "around. All these more modern tweaks\u00a0\non the transformer were not around.\u00a0"
  },
  {
    "start": 1521.76,
    "duration": 5.76,
    "text": "Maybe some of the things that we can bet on, I\u00a0\nthink in 10 years by translational equivariance,\u00a0\u00a0"
  },
  {
    "start": 1527.52,
    "duration": 3.68,
    "text": "is that we're still training giant neural\u00a0\nnetworks with a forward backward pass and\u00a0\u00a0"
  },
  {
    "start": 1531.2,
    "duration": 5.2,
    "text": "update through gradient descent,\u00a0\nbut maybe it looks a bit different,\u00a0\u00a0"
  },
  {
    "start": 1536.4,
    "duration": 5.68,
    "text": "and it's just that everything is much bigger.\nRecently I went back all the way to 1989 which\u00a0\u00a0"
  },
  {
    "start": 1542.08,
    "duration": 6.72,
    "text": "was a fun exercise for me, a few years ago,\u00a0\nbecause I was reproducing Yann LeCun's 1989\u00a0\u00a0"
  },
  {
    "start": 1548.8,
    "duration": 3.92,
    "text": "convolutional network, which was the first neural\u00a0\nnetwork I'm aware of trained via gradient descent,\u00a0\u00a0"
  },
  {
    "start": 1552.72,
    "duration": 4.56,
    "text": "like modern neural network trained\u00a0\ngradient descent on digit recognition.\u00a0"
  },
  {
    "start": 1557.28,
    "duration": 2.72,
    "text": "I was just interested in\u00a0\nhow I could modernize this.\u00a0"
  },
  {
    "start": 1560.0,
    "duration": 1.84,
    "text": "How much of this is algorithms?\nHow much of this is data?\u00a0"
  },
  {
    "start": 1561.84,
    "duration": 5.12,
    "text": "How much of this progress is compute and systems?\nI was able to very quickly halve the learning\u00a0\u00a0"
  },
  {
    "start": 1566.96,
    "duration": 6.24,
    "text": "just by time traveling by 33 years.\nSo if I time travel by algorithms 33 years,\u00a0\u00a0"
  },
  {
    "start": 1573.2,
    "duration": 4.8,
    "text": "I could adjust what Yann LeCun did\u00a0\nin 1989, and I could halve the error.\u00a0"
  },
  {
    "start": 1578.0,
    "duration": 3.36,
    "text": "But to get further gains, I\u00a0\nhad to add a lot more data,\u00a0\u00a0"
  },
  {
    "start": 1581.36,
    "duration": 4.56,
    "text": "I had to 10x the training set, and then I\u00a0\nhad to add more computational optimizations.\u00a0"
  },
  {
    "start": 1585.92,
    "duration": 4.56,
    "text": "I had to train for much longer with dropout\u00a0\nand other regularization techniques.\u00a0"
  },
  {
    "start": 1590.48,
    "duration": 3.36,
    "text": "So all these things have\u00a0\nto improve simultaneously.\u00a0"
  },
  {
    "start": 1595.12,
    "duration": 2.16,
    "text": "We're probably going to have a lot more\u00a0\ndata, we're probably going to have a lot\u00a0\u00a0"
  },
  {
    "start": 1597.28,
    "duration": 3.44,
    "text": "better hardware, probably going to have a lot\u00a0\nbetter kernels and software, we're probably\u00a0\u00a0"
  },
  {
    "start": 1600.72,
    "duration": 2.64,
    "text": "going to have better algorithms.\nAll of those, it's almost like\u00a0\u00a0"
  },
  {
    "start": 1603.36,
    "duration": 4.96,
    "text": "no one of them is winning too much.\nAll of them are surprisingly equal.\u00a0"
  },
  {
    "start": 1608.88,
    "duration": 5.92,
    "text": "This has been the trend for a while.\nSo to answer your question, I expect differences\u00a0\u00a0"
  },
  {
    "start": 1614.8,
    "duration": 3.76,
    "text": "algorithmically to what's happening today.\nBut I do also expect that some of the\u00a0\u00a0"
  },
  {
    "start": 1618.56,
    "duration": 3.04,
    "text": "things that have stuck around for a very\u00a0\nlong time will probably still be there.\u00a0"
  },
  {
    "start": 1621.6,
    "duration": 3.44,
    "text": "It's probably still a giant neural network trained\u00a0\nwith gradient descent. That would be my guess.\u00a0"
  },
  {
    "start": 1625.04,
    "duration": 8.8,
    "text": "It's surprising that all of those things\u00a0\ntogether only halved the error, 30 years\u00a0\u00a0"
  },
  {
    "start": 1633.84,
    "duration": 4.16,
    "text": "of progress\u2026. Maybe half is a lot. Because if\u00a0\nyou halve the error, that actually means that\u2026\u00a0"
  },
  {
    "start": 1638.0,
    "duration": 6.24,
    "text": "Half is a lot. But I guess what was shocking to me\u00a0\nis everything needs to improve across the board:\u00a0\u00a0"
  },
  {
    "start": 1644.24,
    "duration": 4.24,
    "text": "architecture, optimizer, loss function.\nIt also has improved across the board forever.\u00a0"
  },
  {
    "start": 1648.48,
    "duration": 2.96,
    "text": "So I expect all those\u00a0\nchanges to be alive and well.\u00a0"
  },
  {
    "start": 1651.44,
    "duration": 2.88,
    "text": "Yeah. I was about to ask you a very\u00a0\nsimilar question about nanochat.\u00a0"
  },
  {
    "start": 1655.04,
    "duration": 5.04,
    "text": "Since you just coded it up recently,\u00a0\nevery single step in the process of\u00a0\u00a0"
  },
  {
    "start": 1660.08,
    "duration": 5.28,
    "text": "building a chatbot is fresh in your RAM.\nI'm curious if you had similar thoughts about,\u00a0\u00a0"
  },
  {
    "start": 1666.0,
    "duration": 6.0,
    "text": "\"Oh, there was no one thing that was\u00a0\nrelevant to going from GPT-2 to nanochat.\"\u00a0"
  },
  {
    "start": 1672.0,
    "duration": 3.92,
    "text": "What are some surprising\u00a0\ntakeaways from the experience?\u00a0"
  },
  {
    "start": 1675.92,
    "duration": 3.12,
    "text": "Of building nanochat? So nanochat\u00a0\nis a repository I released.\u00a0"
  },
  {
    "start": 1679.04,
    "duration": 2.96,
    "text": "Was it yesterday or the day\u00a0\nbefore? I can't remember.\u00a0"
  },
  {
    "start": 1683.12,
    "duration": 2.96,
    "text": "We can see the sleep\u00a0\ndeprivation that went into the\u2026\u00a0"
  },
  {
    "start": 1689.04,
    "duration": 3.84,
    "text": "It's trying to be the simplest complete\u00a0\nrepository that covers the whole pipeline\u00a0\u00a0"
  },
  {
    "start": 1692.88,
    "duration": 5.52,
    "text": "end-to-end of building a ChatGPT clone.\nSo you have all of the steps, not just\u00a0\u00a0"
  },
  {
    "start": 1698.4,
    "duration": 3.76,
    "text": "any individual step, which is a bunch.\nI worked on all the individual steps\u00a0\u00a0"
  },
  {
    "start": 1702.16,
    "duration": 3.76,
    "text": "in the past and released small pieces\u00a0\nof code that show you how that's done\u00a0\u00a0"
  },
  {
    "start": 1705.92,
    "duration": 5.44,
    "text": "in an algorithmic sense, in simple code.\nBut this handles the entire pipeline.\u00a0"
  },
  {
    "start": 1712.4,
    "duration": 3.76,
    "text": "In terms of learning, I don't\u00a0\nknow that I necessarily found\u00a0\u00a0"
  },
  {
    "start": 1716.16,
    "duration": 4.56,
    "text": "something that I learned from it.\nI already had in my mind how you build it.\u00a0"
  },
  {
    "start": 1720.72,
    "duration": 7.76,
    "text": "This is just the process of mechanically building\u00a0\nit and making it clean enough so that people can\u00a0\u00a0"
  },
  {
    "start": 1728.48,
    "duration": 4.64,
    "text": "learn from it and that they find it useful.\nWhat is the best way for somebody\u00a0\u00a0"
  },
  {
    "start": 1733.12,
    "duration": 1.84,
    "text": "to learn from it?\nIs it to just delete\u00a0\u00a0"
  },
  {
    "start": 1734.96,
    "duration": 3.36,
    "text": "all the code and try to reimplement from\u00a0\nscratch, try to add modifications to it?\u00a0"
  },
  {
    "start": 1739.52,
    "duration": 3.84,
    "text": "That's a great question. Basically\u00a0\nit's about 8,000 lines of code\u00a0\u00a0"
  },
  {
    "start": 1743.36,
    "duration": 3.68,
    "text": "that takes you through the entire pipeline.\nI would probably put it on the right monitor.\u00a0"
  },
  {
    "start": 1747.04,
    "duration": 4.4,
    "text": "If you have two monitors, you put it on the right.\nYou want to build it from scratch,\u00a0\u00a0"
  },
  {
    "start": 1751.44,
    "duration": 3.28,
    "text": "you build it from the start.\nYou're not allowed to copy-paste, you're allowed\u00a0\u00a0"
  },
  {
    "start": 1754.72,
    "duration": 2.96,
    "text": "to reference, you're not allowed to copy-paste.\nMaybe that's how I would do it.\u00a0"
  },
  {
    "start": 1758.24,
    "duration": 3.44,
    "text": "But I also think the repository\u00a0\nby itself is a pretty large beast.\u00a0"
  },
  {
    "start": 1763.52,
    "duration": 4.08,
    "text": "When you write this code, you don't go from top\u00a0\nto bottom, you go from chunks and you grow the\u00a0\u00a0"
  },
  {
    "start": 1767.6,
    "duration": 3.84,
    "text": "chunks, and that information is absent.\nYou wouldn't know where to start.\u00a0"
  },
  {
    "start": 1771.44,
    "duration": 4.32,
    "text": "So it's not just a final repository that's\u00a0\nneeded, it's the building of the repository,\u00a0\u00a0"
  },
  {
    "start": 1775.76,
    "duration": 4.64,
    "text": "which is a complicated chunk-growing process.\nSo that part is not there yet.\u00a0"
  },
  {
    "start": 1780.4,
    "duration": 6.48,
    "text": "I would love to add that probably later this week.\nIt's probably a video or something like that.\u00a0"
  },
  {
    "start": 1789.2,
    "duration": 4.24,
    "text": "Roughly speaking, that's what I would try to do.\nBuild the stuff yourself, but don't allow\u00a0\u00a0"
  },
  {
    "start": 1793.44,
    "duration": 2.24,
    "text": "yourself copy-paste.\nI do think that\u00a0\u00a0"
  },
  {
    "start": 1795.68,
    "duration": 4.24,
    "text": "there's two types of knowledge, almost.\nThere's the high-level surface knowledge, but when\u00a0\u00a0"
  },
  {
    "start": 1799.92,
    "duration": 4.24,
    "text": "you build something from scratch, you're forced to\u00a0\ncome to terms with what you don't understand and\u00a0\u00a0"
  },
  {
    "start": 1804.16,
    "duration": 4.0,
    "text": "you don't know that you don't understand it.\nIt always leads to a deeper understanding.\u00a0"
  },
  {
    "start": 1809.68,
    "duration": 4.08,
    "text": "It's the only way to build.\nIf I can't build it, I don't understand it.\u00a0"
  },
  {
    "start": 1813.76,
    "duration": 5.68,
    "text": "That\u2019s a Feynman quote, I believe.\nI 100% have always believed this very\u00a0\u00a0"
  },
  {
    "start": 1819.44,
    "duration": 4.24,
    "text": "strongly, because there are all these micro\u00a0\nthings that are just not properly arranged\u00a0\u00a0"
  },
  {
    "start": 1823.68,
    "duration": 2.0,
    "text": "and you don't really have the knowledge.\nYou just think you have the knowledge.\u00a0"
  },
  {
    "start": 1825.68,
    "duration": 3.28,
    "text": "So don't write blog posts, don't\u00a0\ndo slides, don't do any of that.\u00a0"
  },
  {
    "start": 1828.96,
    "duration": 2.8,
    "text": "Build the code, arrange it, get it to work.\nIt's the only way to go. Otherwise,\u00a0\u00a0"
  },
  {
    "start": 1831.76,
    "duration": 3.68,
    "text": "you're missing knowledge.\nYou tweeted out that coding models\u00a0\u00a0"
  },
  {
    "start": 1835.44,
    "duration": 5.68,
    "text": "were of very little help to you in assembling\u00a0\nthis repository. I'm curious why that was.\u00a0"
  },
  {
    "start": 1843.44,
    "duration": 3.12,
    "text": "I guess I built the repository over\u00a0\na period of a bit more than a month.\u00a0"
  },
  {
    "start": 1846.56,
    "duration": 3.92,
    "text": "I would say there are three major classes\u00a0\nof how people interact with code right now.\u00a0"
  },
  {
    "start": 1850.48,
    "duration": 4.32,
    "text": "Some people completely reject all of LLMs\u00a0\nand they are just writing by scratch.\u00a0"
  },
  {
    "start": 1854.8,
    "duration": 2.16,
    "text": "This is probably not the\u00a0\nright thing to do anymore.\u00a0"
  },
  {
    "start": 1858.16,
    "duration": 4.24,
    "text": "The intermediate part, which is where I am, is\u00a0\nyou still write a lot of things from scratch,\u00a0\u00a0"
  },
  {
    "start": 1862.4,
    "duration": 4.48,
    "text": "but you use the autocomplete that's\u00a0\navailable now from these models.\u00a0"
  },
  {
    "start": 1866.88,
    "duration": 3.36,
    "text": "So when you start writing out a little\u00a0\npiece of it, it will autocomplete for\u00a0\u00a0"
  },
  {
    "start": 1870.24,
    "duration": 2.32,
    "text": "you and you can just tap through.\nMost of the time it's correct,\u00a0\u00a0"
  },
  {
    "start": 1872.56,
    "duration": 2.96,
    "text": "sometimes it's not, and you edit it.\nBut you're still very much the\u00a0\u00a0"
  },
  {
    "start": 1876.64,
    "duration": 5.04,
    "text": "architect of what you're writing.\nThen there's the vibe coding: \"Hi,\u00a0\u00a0"
  },
  {
    "start": 1881.68,
    "duration": 6.4,
    "text": "please implement this or that,\" enter, and then\u00a0\nlet the model do it. That's the agents. I do feel\u00a0\u00a0"
  },
  {
    "start": 1888.08,
    "duration": 5.12,
    "text": "like the agents work in very specific settings,\u00a0\nand I would use them in specific settings.\u00a0"
  },
  {
    "start": 1893.2,
    "duration": 4.08,
    "text": "But these are all tools available to you\u00a0\nand you have to learn what they're good at,\u00a0\u00a0"
  },
  {
    "start": 1897.28,
    "duration": 3.28,
    "text": "what they're not good at, and when to use them.\nSo the agents are pretty good, for example,\u00a0\u00a0"
  },
  {
    "start": 1900.56,
    "duration": 4.0,
    "text": "if you're doing boilerplate stuff.\nBoilerplate code that's just\u00a0\u00a0"
  },
  {
    "start": 1904.56,
    "duration": 3.76,
    "text": "copy-paste stuff, they're very good at that.\nThey're very good at stuff that occurs very often\u00a0\u00a0"
  },
  {
    "start": 1908.32,
    "duration": 5.28,
    "text": "on the Internet because there are lots of examples\u00a0\nof it in the training sets of these models.\u00a0"
  },
  {
    "start": 1915.2,
    "duration": 3.04,
    "text": "There are features of things where\u00a0\nthe models will do very well.\u00a0"
  },
  {
    "start": 1918.24,
    "duration": 4.8,
    "text": "I would say nanochat is not an example of\u00a0\nthose because it's a fairly unique repository.\u00a0"
  },
  {
    "start": 1923.04,
    "duration": 5.84,
    "text": "There's not that much code in the way that\u00a0\nI've structured it. It's not boilerplate code.\u00a0\u00a0"
  },
  {
    "start": 1928.88,
    "duration": 4.32,
    "text": "It's intellectually intense code almost, and\u00a0\neverything has to be very precisely arranged.\u00a0"
  },
  {
    "start": 1933.2,
    "duration": 8.0,
    "text": "The models have so many cognitive deficits.\nOne example, they kept misunderstanding the code\u00a0\u00a0"
  },
  {
    "start": 1942.48,
    "duration": 3.84,
    "text": "because they have too much memory from\u00a0\nall the typical ways of doing things on\u00a0\u00a0"
  },
  {
    "start": 1946.32,
    "duration": 4.72,
    "text": "the Internet that I just wasn't adopting.\nThe models, for example\u2014I don't know if I\u00a0\u00a0"
  },
  {
    "start": 1951.04,
    "duration": 5.92,
    "text": "want to get into the full details\u2014but they kept\u00a0\nthinking I'm writing normal code, and I'm not.\u00a0"
  },
  {
    "start": 1956.96,
    "duration": 4.8,
    "text": "Maybe one example?\nYou have eight GPUs\u00a0\u00a0"
  },
  {
    "start": 1961.76,
    "duration": 2.8,
    "text": "that are all doing forward, backwards.\nThe way to synchronize gradients between\u00a0\u00a0"
  },
  {
    "start": 1964.56,
    "duration": 4.16,
    "text": "them is to use a Distributed Data Parallel\u00a0\ncontainer of PyTorch, which automatically\u00a0\u00a0"
  },
  {
    "start": 1969.52,
    "duration": 2.88,
    "text": "as you're doing the backward, it will start\u00a0\ncommunicating and synchronizing gradients.\u00a0"
  },
  {
    "start": 1972.4,
    "duration": 4.24,
    "text": "I didn't use DDP because I didn't want\u00a0\nto use it, because it's not necessary.\u00a0"
  },
  {
    "start": 1976.64,
    "duration": 5.84,
    "text": "I threw it out and wrote my own synchronization\u00a0\nroutine that's inside the step of the optimizer.\u00a0"
  },
  {
    "start": 1982.48,
    "duration": 4.24,
    "text": "The models were trying to get me to\u00a0\nuse the DDP container. They were very\u00a0\u00a0"
  },
  {
    "start": 1986.72,
    "duration": 4.56,
    "text": "concerned. This gets way too technical,\u00a0\nbut I wasn't using that container because\u00a0\u00a0"
  },
  {
    "start": 1991.28,
    "duration": 2.72,
    "text": "I don't need it and I have a custom\u00a0\nimplementation of something like it.\u00a0"
  },
  {
    "start": 1994.0,
    "duration": 1.76,
    "text": "They just couldn't internalize\u00a0\nthat you had your own.\u00a0"
  },
  {
    "start": 1996.64,
    "duration": 6.56,
    "text": "They couldn't get past that. They kept trying to\u00a0\nmess up the style. They're way too over-defensive.\u00a0\u00a0"
  },
  {
    "start": 2003.2,
    "duration": 3.92,
    "text": "They make all these try-catch statements.\nThey keep trying to make a production code base,\u00a0\u00a0"
  },
  {
    "start": 2007.12,
    "duration": 2.8,
    "text": "and I have a bunch of assumptions\u00a0\nin my code, and it's okay.\u00a0"
  },
  {
    "start": 2011.92,
    "duration": 4.64,
    "text": "I don't need all this extra stuff in there.\nSo I feel like they're bloating the code base,\u00a0\u00a0"
  },
  {
    "start": 2016.56,
    "duration": 2.32,
    "text": "bloating the complexity, they keep\u00a0\nmisunderstanding, they're using\u00a0\u00a0"
  },
  {
    "start": 2018.88,
    "duration": 7.92,
    "text": "deprecated APIs a bunch of times. It's a total\u00a0\nmess. It's just not net useful. I can go in,\u00a0\u00a0"
  },
  {
    "start": 2026.8,
    "duration": 4.16,
    "text": "I can clean it up, but it's not net useful.\nI also feel like it's annoying to have to\u00a0\u00a0"
  },
  {
    "start": 2030.96,
    "duration": 3.2,
    "text": "type out what I want in English\u00a0\nbecause it's too much typing.\u00a0"
  },
  {
    "start": 2034.16,
    "duration": 4.48,
    "text": "If I just navigate to the part of the code that I\u00a0\nwant, and I go where I know the code has to appear\u00a0\u00a0"
  },
  {
    "start": 2038.64,
    "duration": 3.68,
    "text": "and I start typing out the first few letters,\u00a0\nautocomplete gets it and just gives you the code.\u00a0"
  },
  {
    "start": 2043.84,
    "duration": 3.2,
    "text": "This is a very high information\u00a0\nbandwidth to specify what you want.\u00a0"
  },
  {
    "start": 2047.04,
    "duration": 3.04,
    "text": "You point to the code where you want\u00a0\nit, you type out the first few pieces,\u00a0\u00a0"
  },
  {
    "start": 2050.08,
    "duration": 5.36,
    "text": "and the model will complete it.\nSo what I mean is, these models\u00a0\u00a0"
  },
  {
    "start": 2055.44,
    "duration": 6.8,
    "text": "are good in certain parts of the stack.\nThere are two examples where I use the\u00a0\u00a0"
  },
  {
    "start": 2062.24,
    "duration": 4.32,
    "text": "models that I think are illustrative.\nOne was when I generated the report.\u00a0"
  },
  {
    "start": 2066.56,
    "duration": 3.68,
    "text": "That's more boilerplate-y, so I\u00a0\npartially vibe-coded some of that stuff.\u00a0"
  },
  {
    "start": 2070.24,
    "duration": 4.08,
    "text": "That was fine because it's not\u00a0\nmission-critical stuff, and it works fine.\u00a0"
  },
  {
    "start": 2074.32,
    "duration": 3.6,
    "text": "The other part is when I was\u00a0\nrewriting the tokenizer in Rust.\u00a0"
  },
  {
    "start": 2077.92,
    "duration": 3.2,
    "text": "I'm not as good at Rust\u00a0\nbecause I'm fairly new to Rust.\u00a0"
  },
  {
    "start": 2081.12,
    "duration": 4.72,
    "text": "So there's a bit of vibe coding going on\u00a0\nwhen I was writing some of the Rust code.\u00a0"
  },
  {
    "start": 2085.84,
    "duration": 3.36,
    "text": "But I had a Python implementation that I\u00a0\nfully understand, and I'm just making sure\u00a0\u00a0"
  },
  {
    "start": 2089.2,
    "duration": 3.92,
    "text": "I'm making a more efficient version of it, and\u00a0\nI have tests so I feel safer doing that stuff.\u00a0"
  },
  {
    "start": 2096.24,
    "duration": 6.32,
    "text": "They increase accessibility to languages or\u00a0\nparadigms that you might not be as familiar with.\u00a0"
  },
  {
    "start": 2102.56,
    "duration": 4.0,
    "text": "I think they're very helpful there as well.\nThere's a ton of Rust code out there,\u00a0\u00a0"
  },
  {
    "start": 2106.56,
    "duration": 2.72,
    "text": "the models are pretty good at it.\nI happen to not know that much about it,\u00a0\u00a0"
  },
  {
    "start": 2109.28,
    "duration": 3.68,
    "text": "so the models are very useful there.\nThe reason this question is so interesting\u00a0\u00a0"
  },
  {
    "start": 2112.96,
    "duration": 6.24,
    "text": "is because the main story people\u00a0\nhave about AI exploding and getting\u00a0\u00a0"
  },
  {
    "start": 2119.2,
    "duration": 5.6,
    "text": "to superintelligence pretty rapidly is AI\u00a0\nautomating AI engineering and AI research.\u00a0"
  },
  {
    "start": 2125.52,
    "duration": 2.235,
    "text": "They'll look at the fact that you can have\u00a0\nClaude Code and make entire applications,\u00a0\u00a0"
  },
  {
    "start": 2127.755,
    "duration": 5.045,
    "text": "CRUD applications, from scratch and think, \"If\u00a0\nyou had this same capability inside of OpenAI\u00a0\u00a0"
  },
  {
    "start": 2132.8,
    "duration": 5.68,
    "text": "and DeepMind and everything, just imagine\u00a0\na thousand of you or a million of you in\u00a0\u00a0"
  },
  {
    "start": 2138.48,
    "duration": 5.04,
    "text": "parallel, finding little architectural tweaks.\"\nIt's quite interesting to hear you say that this\u00a0\u00a0"
  },
  {
    "start": 2143.52,
    "duration": 4.96,
    "text": "is the thing they're asymmetrically worse at.\nIt's quite relevant to forecasting whether\u00a0\u00a0"
  },
  {
    "start": 2148.48,
    "duration": 4.48,
    "text": "the AI 2027-type explosion is\u00a0\nlikely to happen anytime soon.\u00a0"
  },
  {
    "start": 2153.52,
    "duration": 4.88,
    "text": "That's a good way of putting it, and you're\u00a0\ngetting at why my timelines are a bit longer.\u00a0\u00a0"
  },
  {
    "start": 2158.4,
    "duration": 5.68,
    "text": "You're right. They're not very good at code\u00a0\nthat has never been written before, maybe it's\u00a0\u00a0"
  },
  {
    "start": 2164.08,
    "duration": 3.68,
    "text": "one way to put it, which is what we're trying\u00a0\nto achieve when we're building these models.\u00a0"
  },
  {
    "start": 2167.76,
    "duration": 7.04,
    "text": "Very naive question, but the architectural\u00a0\ntweaks that you're adding to nanochat,\u00a0\u00a0"
  },
  {
    "start": 2174.8,
    "duration": 2.56,
    "text": "they're in a paper somewhere, right?\nThey might even be in a repo somewhere.\u00a0"
  },
  {
    "start": 2180.32,
    "duration": 4.96,
    "text": "Is it surprising that they aren't able to\u00a0\nintegrate that into whenever you're like,\u00a0\u00a0"
  },
  {
    "start": 2185.28,
    "duration": 4.48,
    "text": "\"Add RoPE embeddings\" or something,\u00a0\nthey do that in the wrong way?\u00a0"
  },
  {
    "start": 2189.76,
    "duration": 4.56,
    "text": "It's tough. They know, but they don't fully know.\nThey don't know how to fully integrate it into\u00a0\u00a0"
  },
  {
    "start": 2194.32,
    "duration": 3.12,
    "text": "the repo and your style and your code and\u00a0\nyour place, and some of the custom things\u00a0\u00a0"
  },
  {
    "start": 2197.44,
    "duration": 3.76,
    "text": "that you're doing and how it fits with\u00a0\nall the assumptions of the repository.\u00a0"
  },
  {
    "start": 2202.88,
    "duration": 4.08,
    "text": "They do have some knowledge, but they\u00a0\nhaven't gotten to the place where they\u00a0\u00a0"
  },
  {
    "start": 2206.96,
    "duration": 6.0,
    "text": "can integrate it and make sense of it.\nA lot of the stuff continues to improve.\u00a0"
  },
  {
    "start": 2214.08,
    "duration": 3.2,
    "text": "Currently, the state-of-the-art\u00a0\nmodel that I go to is the GPT-5 Pro,\u00a0\u00a0"
  },
  {
    "start": 2217.92,
    "duration": 3.36,
    "text": "and that's a very powerful model.\nIf I have 20 minutes,\u00a0\u00a0"
  },
  {
    "start": 2221.28,
    "duration": 4.72,
    "text": "I will copy-paste my entire repo and I go to\u00a0\nGPT-5 Pro, the oracle, for some questions.\u00a0"
  },
  {
    "start": 2226.0,
    "duration": 3.68,
    "text": "Often it's not too bad and surprisingly\u00a0\ngood compared to what existed a year ago.\u00a0"
  },
  {
    "start": 2231.76,
    "duration": 9.12,
    "text": "Overall, the models are not there.\nI feel like the industry is making too\u00a0\u00a0"
  },
  {
    "start": 2240.88,
    "duration": 6.24,
    "text": "big of a jump and is trying to pretend like this\u00a0\nis amazing, and it's not. It's slop. They're not\u00a0\u00a0"
  },
  {
    "start": 2247.12,
    "duration": 2.72,
    "text": "coming to terms with it, and maybe they're\u00a0\ntrying to fundraise or something like that.\u00a0"
  },
  {
    "start": 2249.84,
    "duration": 3.92,
    "text": "I'm not sure what's going on, but we're\u00a0\nat this intermediate stage. The models are\u00a0\u00a0"
  },
  {
    "start": 2253.76,
    "duration": 4.56,
    "text": "amazing. They still need a lot of work.\nFor now, autocomplete is my sweet spot.\u00a0"
  },
  {
    "start": 2258.32,
    "duration": 2.88,
    "text": "But sometimes, for some types of\u00a0\ncode, I will go to an LLM agent.\u00a0"
  },
  {
    "start": 2263.04,
    "duration": 5.28,
    "text": "Here's another reason this is really interesting.\nThrough the history of programming, there have\u00a0\u00a0"
  },
  {
    "start": 2268.32,
    "duration": 6.96,
    "text": "been many productivity improvements\u2014compilers,\u00a0\nlinting, better programming languages\u2014which\u00a0\u00a0"
  },
  {
    "start": 2275.92,
    "duration": 3.6,
    "text": "have increased programmer productivity\u00a0\nbut have not led to an explosion.\u00a0"
  },
  {
    "start": 2280.88,
    "duration": 3.52,
    "text": "That sounds very much like the\u00a0\nautocomplete tab, and this other\u00a0\u00a0"
  },
  {
    "start": 2284.4,
    "duration": 4.64,
    "text": "category is just automation of the programmer.\nIt's interesting you're seeing more in the\u00a0\u00a0"
  },
  {
    "start": 2289.04,
    "duration": 4.64,
    "text": "category of the historical analogies\u00a0\nof better compilers or something.\u00a0"
  },
  {
    "start": 2293.68,
    "duration": 5.44,
    "text": "Maybe this gets to one other thought.\nI have a hard time differentiating where\u00a0\u00a0"
  },
  {
    "start": 2299.12,
    "duration": 3.92,
    "text": "AI begins and stops because I see\u00a0\nAI as fundamentally an extension of\u00a0\u00a0"
  },
  {
    "start": 2303.04,
    "duration": 5.6,
    "text": "computing in a pretty fundamental way.\nI see a continuum of this recursive\u00a0\u00a0"
  },
  {
    "start": 2308.64,
    "duration": 6.56,
    "text": "self-improvement or speeding up programmers\u00a0\nall the way from the beginning: code editors,\u00a0\u00a0"
  },
  {
    "start": 2317.44,
    "duration": 6.32,
    "text": "syntax highlighting, or checking even of\u00a0\nthe types, like data type checking\u2014all\u00a0\u00a0"
  },
  {
    "start": 2324.88,
    "duration": 3.28,
    "text": "these tools that we've built for\u00a0\neach other. Even search engines.\u00a0\u00a0"
  },
  {
    "start": 2328.16,
    "duration": 6.88,
    "text": "Why aren't search engines part of AI? Ranking\u00a0\nis AI. At some point, Google, even early on,\u00a0\u00a0"
  },
  {
    "start": 2335.04,
    "duration": 4.08,
    "text": "was thinking of themselves as an AI company doing\u00a0\nGoogle Search engine, which is totally fair.\u00a0"
  },
  {
    "start": 2339.12,
    "duration": 4.96,
    "text": "I see it as a lot more of a continuum than other\u00a0\npeople do, and it's hard for me to draw the line.\u00a0"
  },
  {
    "start": 2344.08,
    "duration": 3.28,
    "text": "I feel like we're now getting a much\u00a0\nbetter autocomplete, and now we're also\u00a0\u00a0"
  },
  {
    "start": 2347.36,
    "duration": 4.4,
    "text": "getting some agents which are these loopy\u00a0\nthings, but they go off-rails sometimes.\u00a0"
  },
  {
    "start": 2353.52,
    "duration": 4.56,
    "text": "What's going on is that the human is progressively\u00a0\ndoing a bit less and less of the low-level stuff.\u00a0"
  },
  {
    "start": 2358.08,
    "duration": 2.8,
    "text": "We're not writing the assembly\u00a0\ncode because we have compilers.\u00a0"
  },
  {
    "start": 2360.88,
    "duration": 2.96,
    "text": "Compilers will take my high-level\u00a0\nlanguage in C and write the assembly code.\u00a0"
  },
  {
    "start": 2363.84,
    "duration": 4.96,
    "text": "We're abstracting ourselves very, very slowly.\nThere's this what I call \"autonomy slider,\" where\u00a0\u00a0"
  },
  {
    "start": 2368.8,
    "duration": 3.84,
    "text": "more and more stuff is automated\u2014of the stuff that\u00a0\ncan be automated at any point in time\u2014and we're\u00a0\u00a0"
  },
  {
    "start": 2372.64,
    "duration": 5.12,
    "text": "doing a bit less and less and raising ourselves\u00a0\nin the layer of abstraction over the automation.\u00a0"
  },
  {
    "start": 2453.92,
    "duration": 2.96,
    "text": "Let's talk about RL a bit.\nYou tweeted some very\u00a0\u00a0"
  },
  {
    "start": 2456.88,
    "duration": 4.0,
    "text": "interesting things about this.\nConceptually, how should we think about\u00a0\u00a0"
  },
  {
    "start": 2460.88,
    "duration": 6.48,
    "text": "the way that humans are able to build a rich world\u00a0\nmodel just from interacting with our environment,\u00a0\u00a0"
  },
  {
    "start": 2467.36,
    "duration": 6.0,
    "text": "and in ways that seem almost irrespective of\u00a0\nthe final reward at the end of the episode?\u00a0"
  },
  {
    "start": 2473.36,
    "duration": 4.48,
    "text": "If somebody is starting a business, and at\u00a0\nthe end of 10 years, she finds out whether\u00a0\u00a0"
  },
  {
    "start": 2477.84,
    "duration": 4.64,
    "text": "the business succeeded or failed, we say that\u00a0\nshe's earned a bunch of wisdom and experience.\u00a0"
  },
  {
    "start": 2482.48,
    "duration": 3.44,
    "text": "But it's not because the log probs of every\u00a0\nsingle thing that happened over the last 10\u00a0\u00a0"
  },
  {
    "start": 2485.92,
    "duration": 3.92,
    "text": "years are up-weighted or down-weighted.\nSomething much more deliberate and\u00a0\u00a0"
  },
  {
    "start": 2489.84,
    "duration": 3.36,
    "text": "rich is happening.\nWhat is the ML analogy, and how does that\u00a0\u00a0"
  },
  {
    "start": 2493.2,
    "duration": 3.44,
    "text": "compare to what we're doing with LLMs right now?\nMaybe the way I would put it is that humans don't\u00a0\u00a0"
  },
  {
    "start": 2496.64,
    "duration": 4.0,
    "text": "use reinforcement learning, as I said.\nI think they do something different.\u00a0"
  },
  {
    "start": 2502.64,
    "duration": 5.52,
    "text": "Reinforcement learning is a lot worse than I\u00a0\nthink the average person thinks. Reinforcement\u00a0\u00a0"
  },
  {
    "start": 2508.16,
    "duration": 4.96,
    "text": "learning is terrible. It just so happens\u00a0\nthat everything that we had before it is\u00a0\u00a0"
  },
  {
    "start": 2513.12,
    "duration": 5.44,
    "text": "much worse because previously we were just\u00a0\nimitating people, so it has all these issues.\u00a0"
  },
  {
    "start": 2519.6,
    "duration": 4.4,
    "text": "In reinforcement learning, say you're solving\u00a0\na math problem, because it's very simple.\u00a0"
  },
  {
    "start": 2524.0,
    "duration": 2.88,
    "text": "You're given a math problem and\u00a0\nyou're trying to find the solution.\u00a0"
  },
  {
    "start": 2528.16,
    "duration": 5.12,
    "text": "In reinforcement learning, you will\u00a0\ntry lots of things in parallel first.\u00a0"
  },
  {
    "start": 2533.92,
    "duration": 4.72,
    "text": "You're given a problem, you try hundreds of\u00a0\ndifferent attempts. These attempts can be complex.\u00a0\u00a0"
  },
  {
    "start": 2538.64,
    "duration": 3.76,
    "text": "They can be like, \"Oh, let me try this, let me try\u00a0\nthat, this didn't work, that didn't work,\" etc.\u00a0"
  },
  {
    "start": 2542.4,
    "duration": 3.44,
    "text": "Then maybe you get an answer.\nNow you check the back of the book and you see,\u00a0\u00a0"
  },
  {
    "start": 2545.84,
    "duration": 4.24,
    "text": "\"Okay, the correct answer is this.\"\nYou can see that this one, this one,\u00a0\u00a0"
  },
  {
    "start": 2550.08,
    "duration": 3.76,
    "text": "and that one got the correct answer,\u00a0\nbut these other 97 of them didn't.\u00a0"
  },
  {
    "start": 2553.84,
    "duration": 4.0,
    "text": "Literally what reinforcement learning does is it\u00a0\ngoes to the ones that worked really well and every\u00a0\u00a0"
  },
  {
    "start": 2557.84,
    "duration": 4.8,
    "text": "single thing you did along the way, every single\u00a0\ntoken gets upweighted like, \"Do more of this.\"\u00a0"
  },
  {
    "start": 2562.64,
    "duration": 3.92,
    "text": "The problem with that is people will say\u00a0\nthat your estimator has high variance,\u00a0\u00a0"
  },
  {
    "start": 2566.56,
    "duration": 6.88,
    "text": "but it's just noisy. It's noisy. It almost assumes\u00a0\nthat every single little piece of the solution\u00a0\u00a0"
  },
  {
    "start": 2573.44,
    "duration": 3.52,
    "text": "that you made that arrived at the right answer\u00a0\nwas the correct thing to do, which is not true.\u00a0"
  },
  {
    "start": 2576.96,
    "duration": 3.68,
    "text": "You may have gone down the wrong alleys\u00a0\nuntil you arrived at the right solution.\u00a0"
  },
  {
    "start": 2580.64,
    "duration": 3.36,
    "text": "Every single one of those incorrect things you\u00a0\ndid, as long as you got to the correct solution,\u00a0\u00a0"
  },
  {
    "start": 2584.0,
    "duration": 4.4,
    "text": "will be upweighted as, \"Do more of this.\"\u00a0\nIt's terrible. It's noise. You've done all\u00a0\u00a0"
  },
  {
    "start": 2588.4,
    "duration": 5.12,
    "text": "this work only to find, at the end, you get a\u00a0\nsingle number of like, \"Oh, you did correct.\"\u00a0"
  },
  {
    "start": 2594.16,
    "duration": 4.0,
    "text": "Based on that, you weigh that entire\u00a0\ntrajectory as like, upweight or downweight.\u00a0"
  },
  {
    "start": 2599.04,
    "duration": 3.36,
    "text": "The way I like to put it is you're\u00a0\nsucking supervision through a straw.\u00a0"
  },
  {
    "start": 2602.4,
    "duration": 2.48,
    "text": "You've done all this work that\u00a0\ncould be a minute of rollout,\u00a0\u00a0"
  },
  {
    "start": 2604.88,
    "duration": 4.32,
    "text": "and you're sucking the bits of supervision of the\u00a0\nfinal reward signal through a straw and you're\u00a0\u00a0"
  },
  {
    "start": 2613.92,
    "duration": 3.2,
    "text": "broadcasting that across the entire trajectory\u00a0\nand using that to upweight or downweight that\u00a0\u00a0"
  },
  {
    "start": 2617.12,
    "duration": 2.8,
    "text": "trajectory. It's just stupid and\u00a0\ncrazy. A human would never do this.\u00a0"
  },
  {
    "start": 2619.92,
    "duration": 3.2,
    "text": "Number one, a human would\u00a0\nnever do hundreds of rollouts.\u00a0"
  },
  {
    "start": 2623.12,
    "duration": 4.48,
    "text": "Number two, when a person finds a solution,\u00a0\nthey will have a pretty complicated process\u00a0\u00a0"
  },
  {
    "start": 2627.6,
    "duration": 4.24,
    "text": "of review of, \"Okay, I think these parts I\u00a0\ndid well, these parts I did not do that well.\u00a0"
  },
  {
    "start": 2631.84,
    "duration": 3.6,
    "text": "I should probably do this or that.\"\u00a0\nThey think through things. There's\u00a0\u00a0"
  },
  {
    "start": 2635.44,
    "duration": 4.8,
    "text": "nothing in current LLMs that does this.\u00a0\nThere's no equivalent of it. But I do see\u00a0\u00a0"
  },
  {
    "start": 2640.24,
    "duration": 3.68,
    "text": "papers popping out that are trying to do this\u00a0\nbecause it's obvious to everyone in the field.\u00a0"
  },
  {
    "start": 2645.28,
    "duration": 3.84,
    "text": "The first imitation learning, by the way, was\u00a0\nextremely surprising and miraculous and amazing,\u00a0\u00a0"
  },
  {
    "start": 2649.12,
    "duration": 4.96,
    "text": "that we can fine-tune by imitation on humans.\u00a0\nThat was incredible. Because in the beginning,\u00a0\u00a0"
  },
  {
    "start": 2654.08,
    "duration": 4.16,
    "text": "all we had was base models. Base models\u00a0\nare autocomplete. It wasn't obvious to\u00a0\u00a0"
  },
  {
    "start": 2658.24,
    "duration": 5.12,
    "text": "me at the time, and I had to learn this.\nThe paper that blew my mind was InstructGPT,\u00a0\u00a0"
  },
  {
    "start": 2664.16,
    "duration": 3.84,
    "text": "because it pointed out that you can take\u00a0\nthe pretrained model, which is autocomplete,\u00a0\u00a0"
  },
  {
    "start": 2668.0,
    "duration": 4.56,
    "text": "and if you just fine-tune it on text that looks\u00a0\nlike conversations, the model will very rapidly\u00a0\u00a0"
  },
  {
    "start": 2672.56,
    "duration": 4.24,
    "text": "adapt to become very conversational, and it\u00a0\nkeeps all the knowledge from pre-training.\u00a0"
  },
  {
    "start": 2676.8,
    "duration": 4.32,
    "text": "This blew my mind because I didn't understand\u00a0\nthat stylistically, it can adjust so quickly\u00a0\u00a0"
  },
  {
    "start": 2681.12,
    "duration": 5.36,
    "text": "and become an assistant to a user through just\u00a0\na few loops of fine-tuning on that kind of data.\u00a0"
  },
  {
    "start": 2686.48,
    "duration": 5.36,
    "text": "It was very miraculous to me that that worked.\u00a0\nSo incredible. That was two to three years of\u00a0\u00a0"
  },
  {
    "start": 2691.84,
    "duration": 6.4,
    "text": "work. Now came RL. And RL allows you to do a bit\u00a0\nbetter than just imitation learning because you\u00a0\u00a0"
  },
  {
    "start": 2698.24,
    "duration": 3.92,
    "text": "can have these reward functions and you\u00a0\ncan hill-climb on the reward functions.\u00a0"
  },
  {
    "start": 2702.72,
    "duration": 3.44,
    "text": "Some problems have just correct answers, you\u00a0\ncan hill-climb on that without getting expert\u00a0\u00a0"
  },
  {
    "start": 2706.16,
    "duration": 4.8,
    "text": "trajectories to imitate. So that's amazing. The\u00a0\nmodel can also discover solutions that a human\u00a0\u00a0"
  },
  {
    "start": 2710.96,
    "duration": 8.24,
    "text": "might never come up with. This is incredible.\u00a0\nYet, it's still stupid. We need more. I saw a\u00a0\u00a0"
  },
  {
    "start": 2719.2,
    "duration": 6.0,
    "text": "paper from Google yesterday that tried to\u00a0\nhave this reflect & review idea in mind.\u00a0"
  },
  {
    "start": 2725.2,
    "duration": 5.12,
    "text": "Was it the memory bank paper or something? I don't\u00a0\nknow. I've seen a few papers along these lines.\u00a0"
  },
  {
    "start": 2730.32,
    "duration": 7.04,
    "text": "So I expect there to be some major update to how\u00a0\nwe do algorithms for LLMs coming in that realm.\u00a0"
  },
  {
    "start": 2737.36,
    "duration": 5.12,
    "text": "I think we need three or four or\u00a0\nfive more, something like that.\u00a0"
  },
  {
    "start": 2742.48,
    "duration": 5.44,
    "text": "You're so good at coming up with evocative\u00a0\nphrases. \"Sucking supervision through a\u00a0\u00a0"
  },
  {
    "start": 2747.92,
    "duration": 8.4,
    "text": "straw.\" It's so good. You're saying the problem\u00a0\nwith outcome-based reward is that you have this\u00a0\u00a0"
  },
  {
    "start": 2756.32,
    "duration": 5.2,
    "text": "huge trajectory, and then at the end, you're\u00a0\ntrying to learn every single possible thing\u00a0\u00a0"
  },
  {
    "start": 2761.52,
    "duration": 3.92,
    "text": "about what you should do and what you should\u00a0\nlearn about the world from that one final bit.\u00a0"
  },
  {
    "start": 2767.28,
    "duration": 4.24,
    "text": "Given the fact that this is obvious, why hasn't\u00a0\nprocess-based supervision as an alternative been\u00a0\u00a0"
  },
  {
    "start": 2771.52,
    "duration": 4.24,
    "text": "a successful way to make models more capable?\nWhat has been preventing us from using\u00a0\u00a0"
  },
  {
    "start": 2775.76,
    "duration": 2.24,
    "text": "this alternative paradigm?\nProcess-based supervision just\u00a0\u00a0"
  },
  {
    "start": 2778.0,
    "duration": 3.2,
    "text": "refers to the fact that we're not going to\u00a0\nhave a reward function only at the very end.\u00a0"
  },
  {
    "start": 2781.2,
    "duration": 3.12,
    "text": "After you've done 10 minutes of work, I'm not\u00a0\ngoing to tell you you did well or not well.\u00a0"
  },
  {
    "start": 2784.32,
    "duration": 2.64,
    "text": "I'm going to tell you at every single\u00a0\nstep of the way how well you're doing.\u00a0"
  },
  {
    "start": 2788.0,
    "duration": 4.08,
    "text": "The reason we don't have that is\u00a0\nit's tricky how you do that properly.\u00a0"
  },
  {
    "start": 2792.08,
    "duration": 2.88,
    "text": "You have partial solutions and you\u00a0\ndon't know how to assign credit.\u00a0"
  },
  {
    "start": 2794.96,
    "duration": 4.32,
    "text": "So when you get the right answer, it's just\u00a0\nan equality match to the answer. It\u2019s very\u00a0\u00a0"
  },
  {
    "start": 2799.28,
    "duration": 5.36,
    "text": "simple to implement. If you're doing process\u00a0\nsupervision, how do you assign in an automatable\u00a0\u00a0"
  },
  {
    "start": 2804.64,
    "duration": 3.2,
    "text": "way, a partial credit assignment?\nIt's not obvious how you do it.\u00a0"
  },
  {
    "start": 2807.84,
    "duration": 3.12,
    "text": "Lots of labs are trying to\u00a0\ndo it with these LLM judges.\u00a0"
  },
  {
    "start": 2810.96,
    "duration": 2.96,
    "text": "You get LLMs to try to do it.\nYou prompt an LLM, \"Hey,\u00a0\u00a0"
  },
  {
    "start": 2813.92,
    "duration": 2.8,
    "text": "look at a partial solution of a student.\nHow well do you think they're doing if the\u00a0\u00a0"
  },
  {
    "start": 2816.72,
    "duration": 5.84,
    "text": "answer is this?\" and they try to tune the prompt.\nThe reason that this is tricky is quite subtle.\u00a0"
  },
  {
    "start": 2822.56,
    "duration": 5.12,
    "text": "It's the fact that anytime you use an LLM to\u00a0\nassign a reward, those LLMs are giant things\u00a0\u00a0"
  },
  {
    "start": 2827.68,
    "duration": 3.68,
    "text": "with billions of parameters, and they're gameable.\nIf you're reinforcement learning with respect to\u00a0\u00a0"
  },
  {
    "start": 2831.36,
    "duration": 3.96,
    "text": "them, you will find adversarial examples\u00a0\nfor your LLM judges, almost guaranteed.\u00a0"
  },
  {
    "start": 2835.32,
    "duration": 3.24,
    "text": "So you can't do this for too long.\nYou do maybe 10 steps or 20 steps, and maybe\u00a0\u00a0"
  },
  {
    "start": 2838.56,
    "duration": 6.48,
    "text": "it will work, but you can't do 100 or 1,000.\nI understand it's not obvious, but basically\u00a0\u00a0"
  },
  {
    "start": 2845.04,
    "duration": 5.6,
    "text": "the model will find little cracks.\nIt will find all these spurious\u00a0\u00a0"
  },
  {
    "start": 2850.64,
    "duration": 4.32,
    "text": "things in the nooks and crannies of the\u00a0\ngiant model and find a way to cheat it.\u00a0"
  },
  {
    "start": 2854.96,
    "duration": 7.68,
    "text": "One example that's prominently in my mind, this\u00a0\nwas probably public, if you're using an LLM judge\u00a0\u00a0"
  },
  {
    "start": 2862.64,
    "duration": 4.64,
    "text": "for a reward, you just give it a solution from a\u00a0\nstudent and ask it if the student did well or not.\u00a0"
  },
  {
    "start": 2867.28,
    "duration": 2.0,
    "text": "We were training with\u00a0\nreinforcement learning against\u00a0\u00a0"
  },
  {
    "start": 2869.28,
    "duration": 6.24,
    "text": "that reward function, and it worked really well.\nThen, suddenly, the reward became extremely large.\u00a0"
  },
  {
    "start": 2875.52,
    "duration": 4.32,
    "text": "It was a massive jump, and it did perfect.\nYou're looking at it like, \"Wow, this means\u00a0\u00a0"
  },
  {
    "start": 2879.84,
    "duration": 5.44,
    "text": "the student is perfect in all these problems.\u00a0\nIt's fully solved math.\" But when you look at\u00a0\u00a0"
  },
  {
    "start": 2885.28,
    "duration": 3.2,
    "text": "the completions that you're getting from\u00a0\nthe model, they are complete nonsense.\u00a0"
  },
  {
    "start": 2888.48,
    "duration": 1.92,
    "text": "They start out okay, and then\u00a0\nthey change to \"dhdhdhdh.\"\u00a0"
  },
  {
    "start": 2891.36,
    "duration": 3.36,
    "text": "It's just like, \"Oh, okay, let's take two plus\u00a0\nthree and we do this and this, and then dhdhdhdh.\"\u00a0"
  },
  {
    "start": 2895.84,
    "duration": 1.36,
    "text": "You're looking at it, and\u00a0\nit's like, this is crazy.\u00a0"
  },
  {
    "start": 2897.2,
    "duration": 4.56,
    "text": "How is it getting a reward of one or 100%?\nYou look at the LLM judge, and it turns out\u00a0\u00a0"
  },
  {
    "start": 2901.76,
    "duration": 5.44,
    "text": "that \"dhdhdhdh\" is an adversarial example for\u00a0\nthe model, and it assigns 100% probability to it.\u00a0"
  },
  {
    "start": 2907.2,
    "duration": 3.04,
    "text": "It's just because this is an\u00a0\nout-of-sample example to the LLM.\u00a0"
  },
  {
    "start": 2910.24,
    "duration": 3.76,
    "text": "It's never seen it during training,\u00a0\nand you're in pure generalization land.\u00a0"
  },
  {
    "start": 2914.0,
    "duration": 3.44,
    "text": "It's never seen it during training, and in\u00a0\nthe pure generalization land, you can find\u00a0\u00a0"
  },
  {
    "start": 2917.44,
    "duration": 4.8,
    "text": "these examples that break it.\nYou're basically training\u00a0\u00a0"
  },
  {
    "start": 2922.24,
    "duration": 4.56,
    "text": "the LLM to be a prompt injection model.\nNot even that. Prompt injection is way too fancy.\u00a0"
  },
  {
    "start": 2926.8,
    "duration": 2.16,
    "text": "You're finding adversarial\u00a0\nexamples, as they're called.\u00a0"
  },
  {
    "start": 2928.96,
    "duration": 6.48,
    "text": "These are nonsensical solutions that are obviously\u00a0\nwrong, but the model thinks they are amazing.\u00a0"
  },
  {
    "start": 2935.44,
    "duration": 4.08,
    "text": "To the extent you think this is the\u00a0\nbottleneck to making RL more functional,\u00a0\u00a0"
  },
  {
    "start": 2939.52,
    "duration": 4.88,
    "text": "then that will require making LLMs better judges,\u00a0\nif you want to do this in an automated way.\u00a0"
  },
  {
    "start": 2945.2,
    "duration": 2.16,
    "text": "Is it just going to be some sort\u00a0\nof GAN-like approach where you\u00a0\u00a0"
  },
  {
    "start": 2947.36,
    "duration": 4.0,
    "text": "have to train models to be more robust?\nThe labs are probably doing all that.\u00a0"
  },
  {
    "start": 2951.92,
    "duration": 2.8,
    "text": "The obvious thing is, \"dhdhdhdh\"\u00a0\nshould not get 100% reward.\u00a0"
  },
  {
    "start": 2954.72,
    "duration": 3.04,
    "text": "Okay, well, take \"dhdhdhdh,\" put it\u00a0\nin the training set of the LLM judge,\u00a0\u00a0"
  },
  {
    "start": 2957.76,
    "duration": 3.92,
    "text": "and say this is not 100%, this is 0%.\nYou can do this, but every time you do\u00a0\u00a0"
  },
  {
    "start": 2961.68,
    "duration": 3.12,
    "text": "this, you get a new LLM, and it\u00a0\nstill has adversarial examples.\u00a0"
  },
  {
    "start": 2964.8,
    "duration": 4.56,
    "text": "There's an infinity of adversarial examples.\nProbably if you iterate this a few times, it'll\u00a0\u00a0"
  },
  {
    "start": 2969.36,
    "duration": 3.44,
    "text": "probably be harder and harder to find adversarial\u00a0\nexamples, but I'm not 100% sure because this thing\u00a0\u00a0"
  },
  {
    "start": 2972.8,
    "duration": 5.76,
    "text": "has a trillion parameters or whatnot.\nI bet you the labs are trying.\u00a0"
  },
  {
    "start": 2981.92,
    "duration": 4.24,
    "text": "I still think we need other ideas.\nInteresting. Do you have some shape\u00a0\u00a0"
  },
  {
    "start": 2986.16,
    "duration": 8.08,
    "text": "of what the other idea could be?\nThis idea of a review solution\u00a0\u00a0"
  },
  {
    "start": 2994.24,
    "duration": 4.08,
    "text": "encompassing synthetic examples such that\u00a0\nwhen you train on them, you get better,\u00a0\u00a0"
  },
  {
    "start": 2998.32,
    "duration": 2.48,
    "text": "and meta-learn it in some way.\nI think there are some papers\u00a0\u00a0"
  },
  {
    "start": 3000.8,
    "duration": 3.52,
    "text": "that I'm starting to see pop out.\nI am only at a stage of reading abstracts\u00a0\u00a0"
  },
  {
    "start": 3004.32,
    "duration": 4.56,
    "text": "because a lot of these papers are just ideas.\nSomeone has to make it work on a frontier\u00a0\u00a0"
  },
  {
    "start": 3008.88,
    "duration": 4.4,
    "text": "LLM lab scale in full generality\u00a0\nbecause when you see these papers,\u00a0\u00a0"
  },
  {
    "start": 3013.28,
    "duration": 4.56,
    "text": "they pop up, and it's just a bit noisy.\nThey're cool ideas, but I haven't seen\u00a0\u00a0"
  },
  {
    "start": 3017.84,
    "duration": 5.84,
    "text": "anyone convincingly show that this is possible.\nThat said, the LLM labs are fairly closed,\u00a0\u00a0"
  },
  {
    "start": 3023.68,
    "duration": 9.52,
    "text": "so who knows what they're doing now.\nI can conceptualize how you would be able\u00a0\u00a0"
  },
  {
    "start": 3033.2,
    "duration": 3.6,
    "text": "to train on synthetic examples or synthetic\u00a0\nproblems that you have made for yourself.\u00a0"
  },
  {
    "start": 3036.8,
    "duration": 3.92,
    "text": "But there seems to be another thing humans\u00a0\ndo\u2014maybe sleep is this, maybe daydreaming is\u00a0\u00a0"
  },
  {
    "start": 3040.72,
    "duration": 5.44,
    "text": "this\u2014which is not necessarily to come up\u00a0\nwith fake problems, but just to reflect.\u00a0"
  },
  {
    "start": 3047.28,
    "duration": 4.24,
    "text": "I'm not sure what the ML analogy is for\u00a0\ndaydreaming or sleeping, or just reflecting.\u00a0"
  },
  {
    "start": 3051.52,
    "duration": 3.12,
    "text": "I haven't come up with a new problem.\nObviously, the very basic analogy would just\u00a0\u00a0"
  },
  {
    "start": 3054.64,
    "duration": 4.96,
    "text": "be fine-tuning on reflection bits, but I feel like\u00a0\nin practice that probably wouldn't work that well.\u00a0"
  },
  {
    "start": 3060.24,
    "duration": 4.88,
    "text": "Do you have some take on what\u00a0\nthe analogy of this thing is?\u00a0"
  },
  {
    "start": 3065.12,
    "duration": 4.48,
    "text": "I do think that we're missing some aspects there.\nAs an example, let\u2019s take reading a book.\u00a0"
  },
  {
    "start": 3071.36,
    "duration": 4.32,
    "text": "Currently when LLMs are reading a book, what that\u00a0\nmeans is we stretch out the sequence of text,\u00a0\u00a0"
  },
  {
    "start": 3075.68,
    "duration": 3.44,
    "text": "and the model is predicting the next token,\u00a0\nand it's getting some knowledge from that.\u00a0"
  },
  {
    "start": 3079.12,
    "duration": 2.4,
    "text": "That's not really what humans do.\nWhen you're reading a book,\u00a0\u00a0"
  },
  {
    "start": 3081.52,
    "duration": 4.32,
    "text": "I don't even feel like the book is exposition\u00a0\nI'm supposed to be attending to and training on.\u00a0"
  },
  {
    "start": 3085.84,
    "duration": 4.24,
    "text": "The book is a set of prompts for\u00a0\nme to do synthetic data generation,\u00a0\u00a0"
  },
  {
    "start": 3090.08,
    "duration": 3.12,
    "text": "or for you to get to a book club\u00a0\nand talk about it with your friends.\u00a0"
  },
  {
    "start": 3093.2,
    "duration": 3.76,
    "text": "It's by manipulating that information\u00a0\nthat you actually gain that knowledge.\u00a0"
  },
  {
    "start": 3097.68,
    "duration": 4.32,
    "text": "We have no equivalent of that with LLMs. They\u00a0\ndon't really do that. I'd love to see during\u00a0\u00a0"
  },
  {
    "start": 3102.0,
    "duration": 4.32,
    "text": "pre-training some stage that thinks through\u00a0\nthe material and tries to reconcile it with\u00a0\u00a0"
  },
  {
    "start": 3106.32,
    "duration": 5.84,
    "text": "what it already knows, and thinks through it\u00a0\nfor some amount of time and gets that to work.\u00a0"
  },
  {
    "start": 3112.16,
    "duration": 2.56,
    "text": "There's no equivalence of any of this.\u00a0\nThis is all research. There are some\u00a0\u00a0"
  },
  {
    "start": 3114.72,
    "duration": 4.8,
    "text": "subtle\u2014very subtle that I think are very hard\u00a0\nto understand\u2014reasons why it's not trivial.\u00a0"
  },
  {
    "start": 3119.52,
    "duration": 4.64,
    "text": "If I can just describe one: why can't we\u00a0\njust synthetically generate and train on it?\u00a0"
  },
  {
    "start": 3124.16,
    "duration": 3.2,
    "text": "Because every synthetic example, if\u00a0\nI just give synthetic generation of\u00a0\u00a0"
  },
  {
    "start": 3127.36,
    "duration": 3.44,
    "text": "the model thinking about a book, you look\u00a0\nat it and you're like, \"This looks great.\u00a0"
  },
  {
    "start": 3130.8,
    "duration": 2.16,
    "text": "Why can't I train on it?\"\nYou could try, but the model\u00a0\u00a0"
  },
  {
    "start": 3132.96,
    "duration": 4.08,
    "text": "will get much worse if you continue trying.\nThat's because all of the samples you get\u00a0\u00a0"
  },
  {
    "start": 3137.04,
    "duration": 4.48,
    "text": "from models are silently collapsed.\nSilently\u2014it is not obvious if you look\u00a0\u00a0"
  },
  {
    "start": 3141.52,
    "duration": 4.96,
    "text": "at any individual example of it\u2014they occupy\u00a0\na very tiny manifold of the possible space of\u00a0\u00a0"
  },
  {
    "start": 3147.2,
    "duration": 3.28,
    "text": "thoughts about content.\nThe LLMs, when they come off,\u00a0\u00a0"
  },
  {
    "start": 3150.48,
    "duration": 2.8,
    "text": "they're what we call \"collapsed.\"\nThey have a collapsed data distribution.\u00a0"
  },
  {
    "start": 3154.96,
    "duration": 3.44,
    "text": "One easy way to see it is to go to\u00a0\nChatGPT and ask it, \"Tell me a joke.\"\u00a0"
  },
  {
    "start": 3158.4,
    "duration": 3.04,
    "text": "It only has like three jokes.\nIt's not giving you the whole breadth\u00a0\u00a0"
  },
  {
    "start": 3161.44,
    "duration": 5.92,
    "text": "of possible jokes. It knows like three jokes.\u00a0\nThey're silently collapsed. You're not getting\u00a0\u00a0"
  },
  {
    "start": 3167.36,
    "duration": 5.2,
    "text": "the richness and the diversity and the entropy\u00a0\nfrom these models as you would get from humans.\u00a0"
  },
  {
    "start": 3172.56,
    "duration": 3.04,
    "text": "Humans are a lot noisier, but\u00a0\nat least they're not biased,\u00a0\u00a0"
  },
  {
    "start": 3176.56,
    "duration": 4.16,
    "text": "in a statistical sense. They're not silently\u00a0\ncollapsed. They maintain a huge amount of entropy.\u00a0"
  },
  {
    "start": 3180.72,
    "duration": 4.8,
    "text": "So how do you get synthetic data generation to\u00a0\nwork despite the collapse and while maintaining\u00a0\u00a0"
  },
  {
    "start": 3185.52,
    "duration": 4.32,
    "text": "the entropy? That\u2019s a research problem.\nJust to make sure I understood, the reason\u00a0\u00a0"
  },
  {
    "start": 3189.84,
    "duration": 3.36,
    "text": "that the collapse is relevant to synthetic data\u00a0\ngeneration is because you want to be able to\u00a0\u00a0"
  },
  {
    "start": 3193.2,
    "duration": 6.88,
    "text": "come up with synthetic problems or reflections\u00a0\nwhich are not already in your data distribution?\u00a0"
  },
  {
    "start": 3200.08,
    "duration": 5.6,
    "text": "I guess what I'm saying is, say we have a chapter\u00a0\nof a book and I ask an LLM to think about it,\u00a0\u00a0"
  },
  {
    "start": 3206.56,
    "duration": 1.84,
    "text": "it will give you something\u00a0\nthat looks very reasonable.\u00a0"
  },
  {
    "start": 3208.4,
    "duration": 3.52,
    "text": "But if I ask it 10 times, you'll\u00a0\nnotice that all of them are the same.\u00a0"
  },
  {
    "start": 3211.92,
    "duration": 7.92,
    "text": "You can't just keep scaling \"reflection\"\u00a0\non the same amount of prompt information\u00a0\u00a0"
  },
  {
    "start": 3219.84,
    "duration": 4.0,
    "text": "and then get returns from that.\nAny individual sample will look okay,\u00a0\u00a0"
  },
  {
    "start": 3223.84,
    "duration": 4.0,
    "text": "but the distribution of it is quite terrible.\nIt's quite terrible in such a way that if\u00a0\u00a0"
  },
  {
    "start": 3227.84,
    "duration": 3.04,
    "text": "you continue training on too much of\u00a0\nyour own stuff, you actually collapse.\u00a0"
  },
  {
    "start": 3230.88,
    "duration": 2.8,
    "text": "I think that there's possibly\u00a0\nno fundamental solution to this.\u00a0"
  },
  {
    "start": 3234.24,
    "duration": 5.76,
    "text": "I also think humans collapse over time.\u00a0\nThese analogies are surprisingly good.\u00a0\u00a0"
  },
  {
    "start": 3240.0,
    "duration": 6.0,
    "text": "Humans collapse during the course of their lives.\nThis is why children, they haven't overfit yet.\u00a0"
  },
  {
    "start": 3246.0,
    "duration": 3.84,
    "text": "They will say stuff that will shock you\u00a0\nbecause you can see where they're coming from,\u00a0\u00a0"
  },
  {
    "start": 3249.84,
    "duration": 4.64,
    "text": "but it's just not the thing people say,\u00a0\nbecause they're not yet collapsed. But we're\u00a0\u00a0"
  },
  {
    "start": 3254.48,
    "duration": 5.68,
    "text": "collapsed. We end up revisiting the same thoughts.\nWe end up saying more and more of the same stuff,\u00a0\u00a0"
  },
  {
    "start": 3260.16,
    "duration": 3.84,
    "text": "and the learning rates go down, and\u00a0\nthe collapse continues to get worse,\u00a0\u00a0"
  },
  {
    "start": 3264.0,
    "duration": 4.48,
    "text": "and then everything deteriorates.\nHave you seen this super interesting\u00a0\u00a0"
  },
  {
    "start": 3268.48,
    "duration": 5.68,
    "text": "paper that dreaming is a way of preventing\u00a0\nthis kind of overfitting and collapse?\u00a0"
  },
  {
    "start": 3274.16,
    "duration": 7.12,
    "text": "The reason dreaming is evolutionary adaptive\u00a0\nis to put you in weird situations that are\u00a0\u00a0"
  },
  {
    "start": 3281.84,
    "duration": 2.72,
    "text": "very unlike your day-to-day reality, so\u00a0\nas to prevent this kind of overfitting.\u00a0"
  },
  {
    "start": 3284.56,
    "duration": 3.52,
    "text": "It's an interesting idea. I do think\u00a0\nthat when you're generating things\u00a0\u00a0"
  },
  {
    "start": 3288.08,
    "duration": 3.28,
    "text": "in your head and then you're attending to\u00a0\nit, you're training on your own samples,\u00a0\u00a0"
  },
  {
    "start": 3291.36,
    "duration": 2.32,
    "text": "you're training on your synthetic data.\nIf you do it for too long,\u00a0\u00a0"
  },
  {
    "start": 3293.68,
    "duration": 6.24,
    "text": "you go off-rails and you collapse way too much.\nYou always have to seek entropy in your life.\u00a0"
  },
  {
    "start": 3301.36,
    "duration": 3.76,
    "text": "Talking to other people is a great\u00a0\nsource of entropy, and things like that.\u00a0"
  },
  {
    "start": 3305.12,
    "duration": 4.72,
    "text": "So maybe the brain has also built some internal\u00a0\nmechanisms for increasing the amount of entropy\u00a0\u00a0"
  },
  {
    "start": 3311.36,
    "duration": 5.36,
    "text": "in that process. That's an interesting idea.\nThis is a very ill-formed thought so I\u2019ll\u00a0\u00a0"
  },
  {
    "start": 3316.72,
    "duration": 3.92,
    "text": "just put it out and let you react to it.\nThe best learners that we are aware of,\u00a0\u00a0"
  },
  {
    "start": 3320.64,
    "duration": 5.04,
    "text": "which are children, are extremely\u00a0\nbad at recollecting information.\u00a0"
  },
  {
    "start": 3325.68,
    "duration": 3.52,
    "text": "In fact, at the very earliest stages of\u00a0\nchildhood, you will forget everything.\u00a0"
  },
  {
    "start": 3329.2,
    "duration": 3.44,
    "text": "You're just an amnesiac about everything\u00a0\nthat happens before a certain year date.\u00a0"
  },
  {
    "start": 3332.64,
    "duration": 3.44,
    "text": "But you're extremely good at picking up\u00a0\nnew languages and learning from the world.\u00a0"
  },
  {
    "start": 3336.08,
    "duration": 2.88,
    "text": "Maybe there's some element of being\u00a0\nable to see the forest for the trees.\u00a0"
  },
  {
    "start": 3338.96,
    "duration": 5.12,
    "text": "Whereas if you compare it to the opposite end\u00a0\nof the spectrum, you have LLM pre-training,\u00a0\u00a0"
  },
  {
    "start": 3344.08,
    "duration": 3.84,
    "text": "where these models will literally be\u00a0\nable to regurgitate word-for-word what\u00a0\u00a0"
  },
  {
    "start": 3347.92,
    "duration": 4.48,
    "text": "is the next thing in a Wikipedia page.\nBut their ability to learn abstract\u00a0\u00a0"
  },
  {
    "start": 3352.4,
    "duration": 3.52,
    "text": "concepts really quickly, the way\u00a0\na child can, is much more limited.\u00a0"
  },
  {
    "start": 3355.92,
    "duration": 3.92,
    "text": "Then adults are somewhere in between, where\u00a0\nthey don't have the flexibility of childhood\u00a0\u00a0"
  },
  {
    "start": 3359.84,
    "duration": 6.08,
    "text": "learning, but they can memorize facts and\u00a0\ninformation in a way that is harder for kids.\u00a0"
  },
  {
    "start": 3365.92,
    "duration": 2.24,
    "text": "I don't know if there's something\u00a0\ninteresting about that spectrum.\u00a0"
  },
  {
    "start": 3368.16,
    "duration": 2.48,
    "text": "I think there's something very\u00a0\ninteresting about that, 100%.\u00a0"
  },
  {
    "start": 3370.64,
    "duration": 5.84,
    "text": "I do think that humans have a lot more of\u00a0\nan element, compared to LLMs, of seeing\u00a0\u00a0"
  },
  {
    "start": 3376.48,
    "duration": 2.72,
    "text": "the forest for the trees.\nWe're not actually that good\u00a0\u00a0"
  },
  {
    "start": 3379.2,
    "duration": 6.08,
    "text": "at memorization, which is actually a feature.\nBecause we're not that good at memorization, we're\u00a0\u00a0"
  },
  {
    "start": 3385.92,
    "duration": 7.92,
    "text": "forced to find patterns in a more general sense.\nLLMs in comparison are extremely good\u00a0\u00a0"
  },
  {
    "start": 3393.84,
    "duration": 1.6,
    "text": "at memorization.\nThey will recite\u00a0\u00a0"
  },
  {
    "start": 3395.44,
    "duration": 4.8,
    "text": "passages from all these training sources.\nYou can give them completely nonsensical data.\u00a0"
  },
  {
    "start": 3401.28,
    "duration": 3.36,
    "text": "You can hash some amount of text or something\u00a0\nlike that, you get a completely random sequence.\u00a0"
  },
  {
    "start": 3404.64,
    "duration": 4.0,
    "text": "If you train on it, even just for a single\u00a0\niteration or two, it can suddenly regurgitate\u00a0\u00a0"
  },
  {
    "start": 3408.64,
    "duration": 3.04,
    "text": "the entire thing. It will memorize it.\u00a0\nThere's no way a person can read a single\u00a0\u00a0"
  },
  {
    "start": 3411.68,
    "duration": 6.72,
    "text": "sequence of random numbers and recite it to you.\nThat's a feature, not a bug, because it forces\u00a0\u00a0"
  },
  {
    "start": 3418.4,
    "duration": 4.96,
    "text": "you to only learn the generalizable components.\nWhereas LLMs are distracted by all the memory\u00a0\u00a0"
  },
  {
    "start": 3423.36,
    "duration": 3.2,
    "text": "that they have of the pre-training\u00a0\ndocuments, and it's probably very\u00a0\u00a0"
  },
  {
    "start": 3426.56,
    "duration": 4.0,
    "text": "distracting to them in a certain sense.\nSo that's why when I talk about the\u00a0\u00a0"
  },
  {
    "start": 3430.56,
    "duration": 3.12,
    "text": "cognitive core, I want to remove the\u00a0\nmemory, which is what we talked about.\u00a0"
  },
  {
    "start": 3433.68,
    "duration": 3.92,
    "text": "I'd love to have them have less memory\u00a0\nso that they have to look things up,\u00a0\u00a0"
  },
  {
    "start": 3437.6,
    "duration": 5.2,
    "text": "and they only maintain the algorithms for\u00a0\nthought, and the idea of an experiment,\u00a0\u00a0"
  },
  {
    "start": 3442.8,
    "duration": 5.52,
    "text": "and all this cognitive glue of acting.\nAnd this is also relevant to preventing\u00a0\u00a0"
  },
  {
    "start": 3448.32,
    "duration": 6.72,
    "text": "model collapse?\nLet me think. I'm\u00a0\u00a0"
  },
  {
    "start": 3455.04,
    "duration": 5.28,
    "text": "not sure. It's almost like a separate axis.\nThe models are way too good at memorization,\u00a0\u00a0"
  },
  {
    "start": 3460.32,
    "duration": 6.16,
    "text": "and somehow we should remove that.\nPeople are much worse, but it's a good thing.\u00a0"
  },
  {
    "start": 3466.48,
    "duration": 4.24,
    "text": "What is a solution to model collapse?\nThere are very naive things you could attempt.\u00a0"
  },
  {
    "start": 3472.4,
    "duration": 3.36,
    "text": "The distribution over logits\u00a0\nshould be wider or something.\u00a0"
  },
  {
    "start": 3475.76,
    "duration": 2.72,
    "text": "There are many naive things you could try.\nWhat ends up being the problem\u00a0\u00a0"
  },
  {
    "start": 3478.48,
    "duration": 4.4,
    "text": "with the naive approaches?\nThat's a great question. You can imagine having\u00a0\u00a0"
  },
  {
    "start": 3482.88,
    "duration": 3.92,
    "text": "a regularization for entropy and things like that.\nI guess they just don't work as well empirically\u00a0\u00a0"
  },
  {
    "start": 3486.8,
    "duration": 6.24,
    "text": "because right now the models are collapsed.\nBut I will say most of the tasks that we\u00a0\u00a0"
  },
  {
    "start": 3493.04,
    "duration": 5.76,
    "text": "want from them don't actually demand diversity.\nThat\u2019s probably the answer to what's going on.\u00a0"
  },
  {
    "start": 3500.0,
    "duration": 2.72,
    "text": "The frontier labs are trying\u00a0\nto make the models useful.\u00a0"
  },
  {
    "start": 3502.72,
    "duration": 3.6,
    "text": "I feel like the diversity of\u00a0\nthe outputs is not so much...\u00a0"
  },
  {
    "start": 3506.32,
    "duration": 3.2,
    "text": "Number one, it's much harder to work with and\u00a0\nevaluate and all this stuff, but maybe it's not\u00a0\u00a0"
  },
  {
    "start": 3509.52,
    "duration": 4.0,
    "text": "what's capturing most of the value.\nIn fact, it's actively penalized.\u00a0\u00a0"
  },
  {
    "start": 3514.16,
    "duration": 5.2,
    "text": "If you're super creative in RL, it's not good.\nYeah. Or maybe if you're doing a lot of writing,\u00a0\u00a0"
  },
  {
    "start": 3519.36,
    "duration": 3.12,
    "text": "help from LLMs and stuff like that, it's probably\u00a0\nbad because the models will silently give\u00a0\u00a0"
  },
  {
    "start": 3522.48,
    "duration": 6.0,
    "text": "you all the same stuff.\nThey won't explore lots\u00a0\u00a0"
  },
  {
    "start": 3528.48,
    "duration": 7.76,
    "text": "of different ways of answering a question.\nMaybe this diversity, not as many applications\u00a0\u00a0"
  },
  {
    "start": 3536.24,
    "duration": 2.16,
    "text": "need it so the models don't have it.\nBut then it's a problem at\u00a0\u00a0"
  },
  {
    "start": 3538.4,
    "duration": 3.28,
    "text": "synthetic data generation time, et cetera.\nSo we're shooting ourselves in the foot by not\u00a0\u00a0"
  },
  {
    "start": 3541.68,
    "duration": 5.2,
    "text": "allowing this entropy to maintain in the model.\nPossibly the labs should try harder.\u00a0"
  },
  {
    "start": 3546.88,
    "duration": 4.48,
    "text": "I think you hinted that it's a very\u00a0\nfundamental problem, it won't be easy\u00a0\u00a0"
  },
  {
    "start": 3551.36,
    "duration": 5.68,
    "text": "to solve. What's your intuition for that?\nI don't know if it's super fundamental.\u00a0"
  },
  {
    "start": 3557.04,
    "duration": 6.4,
    "text": "I don't know if I intended to say that.\nI do think that I haven't done these experiments,\u00a0\u00a0"
  },
  {
    "start": 3563.44,
    "duration": 3.44,
    "text": "but I do think that you could probably\u00a0\nregularize the entropy to be higher.\u00a0"
  },
  {
    "start": 3566.88,
    "duration": 4.48,
    "text": "So you're encouraging the model to give you more\u00a0\nand more solutions, but you don't want it to\u00a0\u00a0"
  },
  {
    "start": 3571.36,
    "duration": 3.12,
    "text": "start deviating too much from the training data.\nIt's going to start making up its own language.\u00a0"
  },
  {
    "start": 3574.48,
    "duration": 4.0,
    "text": "It's going to start using words that are\u00a0\nextremely rare, so it's going to drift too\u00a0\u00a0"
  },
  {
    "start": 3578.48,
    "duration": 2.56,
    "text": "much from the distribution.\nSo I think controlling\u00a0\u00a0"
  },
  {
    "start": 3581.04,
    "duration": 6.56,
    "text": "the distribution is just tricky.\nIt's probably not trivial in that sense.\u00a0"
  },
  {
    "start": 3587.6,
    "duration": 6.48,
    "text": "How many bits should the optimal core\u00a0\nof intelligence end up being if you\u00a0\u00a0"
  },
  {
    "start": 3594.08,
    "duration": 2.48,
    "text": "just had to make a guess?\nThe thing we put on the\u00a0\u00a0"
  },
  {
    "start": 3596.56,
    "duration": 5.2,
    "text": "von Neumann probes, how big does it have to be?\nIt's really interesting in the history of the\u00a0\u00a0"
  },
  {
    "start": 3601.76,
    "duration": 4.0,
    "text": "field because at one point everything was\u00a0\nvery scaling-pilled in terms of like, \"Oh,\u00a0\u00a0"
  },
  {
    "start": 3605.76,
    "duration": 3.12,
    "text": "we're gonna make much bigger models,\u00a0\ntrillions of parameter models.\"\u00a0"
  },
  {
    "start": 3608.88,
    "duration": 3.12,
    "text": "What the models have done in size\u00a0\nis they've gone up and now they've\u00a0\u00a0"
  },
  {
    "start": 3614.64,
    "duration": 4.72,
    "text": "come down. State-of-the-art models are smaller.\u00a0\nEven then, I think they memorized way too much.\u00a0"
  },
  {
    "start": 3620.16,
    "duration": 4.48,
    "text": "So I had a prediction a while back that I almost\u00a0\nfeel like we can get cognitive cores that are\u00a0\u00a0"
  },
  {
    "start": 3624.64,
    "duration": 6.32,
    "text": "very good at even a billion parameters.\nIf you talk to a billion parameter model,\u00a0\u00a0"
  },
  {
    "start": 3630.96,
    "duration": 3.28,
    "text": "I think in 20 years, you can have\u00a0\na very productive conversation.\u00a0"
  },
  {
    "start": 3634.24,
    "duration": 5.04,
    "text": "It thinks and it's a lot more like a human.\nBut if you ask it some factual question, it might\u00a0\u00a0"
  },
  {
    "start": 3639.28,
    "duration": 2.96,
    "text": "have to look it up, but it knows that it doesn't\u00a0\nknow and it might have to look it up and it will\u00a0\u00a0"
  },
  {
    "start": 3642.24,
    "duration": 2.56,
    "text": "just do all the reasonable things.\nThat's surprising that you think\u00a0\u00a0"
  },
  {
    "start": 3644.8,
    "duration": 2.72,
    "text": "it'll take a billion parameters.\nBecause already we have billion\u00a0\u00a0"
  },
  {
    "start": 3647.52,
    "duration": 3.92,
    "text": "parameter models or a couple billion\u00a0\nparameter models that are very intelligent.\u00a0"
  },
  {
    "start": 3651.44,
    "duration": 2.24,
    "text": "Well, state-of-the-art models\u00a0\nare like a trillion parameters.\u00a0"
  },
  {
    "start": 3653.68,
    "duration": 5.04,
    "text": "But they remember so much stuff.\nYeah, but I'm surprised that in 10 years,\u00a0\u00a0"
  },
  {
    "start": 3658.72,
    "duration": 8.32,
    "text": "given the pace\u2026 We have gpt-oss-20b.\nThat's way better than GPT-4 original,\u00a0\u00a0"
  },
  {
    "start": 3667.04,
    "duration": 4.56,
    "text": "which was a trillion plus parameters.\nGiven that trend, I'm surprised you\u00a0\u00a0"
  },
  {
    "start": 3671.6,
    "duration": 3.52,
    "text": "think in 10 years the cognitive\u00a0\ncore is still a billion parameters.\u00a0"
  },
  {
    "start": 3675.12,
    "duration": 4.88,
    "text": "I'm surprised you're not like, \"Oh it's\u00a0\ngonna be like tens of millions or millions.\"\u00a0"
  },
  {
    "start": 3682.16,
    "duration": 3.84,
    "text": "Here's the issue, the training data is\u00a0\nthe internet, which is really terrible.\u00a0"
  },
  {
    "start": 3686.0,
    "duration": 2.48,
    "text": "There's a huge amount of gains to be\u00a0\nmade because the internet is terrible.\u00a0"
  },
  {
    "start": 3689.52,
    "duration": 2.56,
    "text": "Even the internet, when you and I think of\u00a0\nthe internet, you're thinking of like The\u00a0\u00a0"
  },
  {
    "start": 3692.08,
    "duration": 4.16,
    "text": "Wall Street Journal. That's not what this\u00a0\nis. When you're looking at a pre-training\u00a0\u00a0"
  },
  {
    "start": 3696.24,
    "duration": 4.48,
    "text": "dataset in the frontier lab and you look at a\u00a0\nrandom internet document, it's total garbage.\u00a0"
  },
  {
    "start": 3700.72,
    "duration": 4.88,
    "text": "I don't even know how this works at all.\nIt's some like stock tickers, symbols,\u00a0\u00a0"
  },
  {
    "start": 3706.56,
    "duration": 3.6,
    "text": "it's a huge amount of slop and garbage\u00a0\nfrom like all the corners of the internet.\u00a0"
  },
  {
    "start": 3710.16,
    "duration": 3.04,
    "text": "It's not like your Wall Street Journal\u00a0\narticle, that's extremely rare.\u00a0"
  },
  {
    "start": 3713.92,
    "duration": 6.48,
    "text": "So because the internet is so terrible, we have\u00a0\nto build really big models to compress all that.\u00a0"
  },
  {
    "start": 3720.4,
    "duration": 3.68,
    "text": "Most of that compression is memory\u00a0\nwork instead of cognitive work.\u00a0"
  },
  {
    "start": 3724.08,
    "duration": 2.88,
    "text": "But what we really want is the\u00a0\ncognitive part, delete the memory.\u00a0"
  },
  {
    "start": 3728.24,
    "duration": 4.24,
    "text": "I guess what I'm saying is that we need\u00a0\nintelligent models to help us refine even\u00a0\u00a0"
  },
  {
    "start": 3732.48,
    "duration": 3.44,
    "text": "the pre-training set to just narrow\u00a0\nit down to the cognitive components.\u00a0"
  },
  {
    "start": 3735.92,
    "duration": 2.48,
    "text": "Then I think you get away with a\u00a0\nmuch smaller model because it's a\u00a0\u00a0"
  },
  {
    "start": 3738.4,
    "duration": 3.84,
    "text": "much better dataset and you could train it on it.\nBut probably it's not trained directly on it, it's\u00a0\u00a0"
  },
  {
    "start": 3742.24,
    "duration": 4.8,
    "text": "probably distilled from a much better model still.\nBut why is the distilled version still a billion?\u00a0"
  },
  {
    "start": 3748.56,
    "duration": 1.92,
    "text": "I just feel like distillation\u00a0\nworks extremely well.\u00a0"
  },
  {
    "start": 3750.48,
    "duration": 4.0,
    "text": "So almost every small model, if you have a\u00a0\nsmall model, it's almost certainly distilled.\u00a0"
  },
  {
    "start": 3755.52,
    "duration": 4.16,
    "text": "Right, but why is the distillation in\u00a0\n10 years not getting below 1 billion?\u00a0"
  },
  {
    "start": 3759.68,
    "duration": 3.84,
    "text": "Oh, you think it should be smaller than a\u00a0\nbillion? I mean, come on, right? I don't\u00a0\u00a0"
  },
  {
    "start": 3763.52,
    "duration": 6.0,
    "text": "know. At some point it should take at least\u00a0\na billion knobs to do something interesting.\u00a0"
  },
  {
    "start": 3769.52,
    "duration": 3.68,
    "text": "You're thinking it should be even smaller?\nYeah. If you look at the trend over the last\u00a0\u00a0"
  },
  {
    "start": 3773.2,
    "duration": 4.64,
    "text": "few years of just finding low-hanging fruit and\u00a0\ngoing from trillion plus models to models that\u00a0\u00a0"
  },
  {
    "start": 3777.84,
    "duration": 5.68,
    "text": "are literally two orders of magnitude smaller in a\u00a0\nmatter of two years and having better performance,\u00a0\u00a0"
  },
  {
    "start": 3783.52,
    "duration": 5.84,
    "text": "it makes me think the sort of core of\u00a0\nintelligence might be even way, way smaller.\u00a0"
  },
  {
    "start": 3789.36,
    "duration": 2.32,
    "text": "Plenty of room at the bottom,\u00a0\nto paraphrase Feynman.\u00a0"
  },
  {
    "start": 3791.68,
    "duration": 2.96,
    "text": "I feel like I'm already contrarian\u00a0\nby talking about a billion parameter\u00a0\u00a0"
  },
  {
    "start": 3794.64,
    "duration": 8.72,
    "text": "cognitive core and you're outdoing me.\nMaybe we could get a little bit smaller.\u00a0"
  },
  {
    "start": 3803.36,
    "duration": 3.04,
    "text": "I do think that practically speaking, you\u00a0\nwant the model to have some knowledge.\u00a0"
  },
  {
    "start": 3806.4,
    "duration": 3.68,
    "text": "You don't want it to be looking up everything\u00a0\nbecause then you can't think in your head.\u00a0"
  },
  {
    "start": 3810.08,
    "duration": 1.44,
    "text": "You're looking up way too much stuff all the time.\u00a0"
  },
  {
    "start": 3812.72,
    "duration": 5.04,
    "text": "Some basic curriculum needs to be there for\u00a0\nknowledge, but it doesn't have esoteric knowledge.\u00a0"
  },
  {
    "start": 3818.64,
    "duration": 2.72,
    "text": "We're discussing what plausibly\u00a0\ncould be the cognitive core.\u00a0"
  },
  {
    "start": 3821.36,
    "duration": 5.6,
    "text": "There's a separate question which is what\u00a0\nwill be the size of frontier models over time?\u00a0"
  },
  {
    "start": 3826.96,
    "duration": 4.88,
    "text": "I'm curious if you have predictions.\nWe had increasing scale up to maybe GPT 4.5 and\u00a0\u00a0"
  },
  {
    "start": 3831.84,
    "duration": 4.88,
    "text": "now we're seeing decreasing or plateauing scale.\nThere are many reasons this could be going on.\u00a0"
  },
  {
    "start": 3836.72,
    "duration": 4.16,
    "text": "Do you have a prediction going forward?\nWill the biggest models be bigger,\u00a0\u00a0"
  },
  {
    "start": 3840.88,
    "duration": 5.2,
    "text": "will they be smaller, will they be the same?\nI don't have a super strong prediction.\u00a0"
  },
  {
    "start": 3847.2,
    "duration": 3.76,
    "text": "The labs are just being practical.\nThey have a flops budget and a cost budget.\u00a0"
  },
  {
    "start": 3850.96,
    "duration": 3.28,
    "text": "It just turns out that pre-training is not where\u00a0\nyou want to put most of your flops or your cost.\u00a0"
  },
  {
    "start": 3854.24,
    "duration": 3.6,
    "text": "That's why the models have gotten smaller.\nThey are a bit smaller, the pre-training\u00a0\u00a0"
  },
  {
    "start": 3857.84,
    "duration": 2.64,
    "text": "stage is smaller, but they make\u00a0\nit up in reinforcement learning,\u00a0\u00a0"
  },
  {
    "start": 3861.2,
    "duration": 4.08,
    "text": "mid-training, and all this stuff that follows.\nThey're just being practical in terms of all the\u00a0\u00a0"
  },
  {
    "start": 3865.28,
    "duration": 5.36,
    "text": "stages and how you get the most bang for the buck.\nForecasting that trend is quite hard.\u00a0"
  },
  {
    "start": 3870.64,
    "duration": 5.04,
    "text": "I do still expect that there's so much\u00a0\nlow-hanging fruit. That's my basic expectation.\u00a0\u00a0"
  },
  {
    "start": 3878.24,
    "duration": 4.24,
    "text": "I have a very wide distribution here.\nDo you expect the low-hanging fruit to be\u00a0\u00a0"
  },
  {
    "start": 3882.48,
    "duration": 5.92,
    "text": "similar in kind to the kinds of things that have\u00a0\nbeen happening over the last two to five years?\u00a0"
  },
  {
    "start": 3889.52,
    "duration": 4.56,
    "text": "If I look at nanochat versus nanoGPT\u00a0\nand the architectural tweaks you made,\u00a0\u00a0"
  },
  {
    "start": 3894.08,
    "duration": 2.8,
    "text": "is that the flavor of things you\u00a0\nexpect to continue to keep happening?\u00a0"
  },
  {
    "start": 3898.0,
    "duration": 3.2,
    "text": "You're not expecting any giant paradigm shifts.\nFor the most part, yeah. I expect the\u00a0\u00a0"
  },
  {
    "start": 3901.2,
    "duration": 2.56,
    "text": "datasets to get much, much better.\nWhen you look at the average datasets,\u00a0\u00a0"
  },
  {
    "start": 3903.76,
    "duration": 1.76,
    "text": "they're extremely terrible.\nThey\u2019re so bad that I\u00a0\u00a0"
  },
  {
    "start": 3905.52,
    "duration": 5.2,
    "text": "don't even know how anything works.\nLook at the average example in the training set:\u00a0\u00a0"
  },
  {
    "start": 3910.72,
    "duration": 5.12,
    "text": "factual mistakes, errors, nonsensical things.\nSomehow when you do it at scale,\u00a0\u00a0"
  },
  {
    "start": 3916.4,
    "duration": 4.72,
    "text": "the noise washes away and you're left with\u00a0\nsome of the signal. Datasets will improve\u00a0\u00a0"
  },
  {
    "start": 3921.12,
    "duration": 8.16,
    "text": "a ton. Everything gets better. Our hardware,\u00a0\nall the kernels for running the hardware and\u00a0\u00a0"
  },
  {
    "start": 3929.28,
    "duration": 4.32,
    "text": "maximizing what you get with the hardware.\nNvidia is slowly tuning the hardware itself,\u00a0\u00a0"
  },
  {
    "start": 3933.6,
    "duration": 3.36,
    "text": "Tensor Cores, all that needs to\u00a0\nhappen and will continue to happen.\u00a0"
  },
  {
    "start": 3936.96,
    "duration": 2.88,
    "text": "All the kernels will get better and\u00a0\nutilize the chip to the max extent.\u00a0"
  },
  {
    "start": 3939.84,
    "duration": 5.28,
    "text": "All the algorithms will probably improve over\u00a0\noptimization, architecture, and all the modeling\u00a0\u00a0"
  },
  {
    "start": 3945.12,
    "duration": 3.76,
    "text": "components of how everything is done and what\u00a0\nthe algorithms are that we're even training with.\u00a0"
  },
  {
    "start": 3948.88,
    "duration": 10.4,
    "text": "I do expect that nothing dominates. Everything\u00a0\nplus 20%. This is roughly what I've seen.\u00a0"
  },
  {
    "start": 4033.92,
    "duration": 7.52,
    "text": "People have proposed different ways of charting\u00a0\nhow much progress we've made towards full AGI.\u00a0"
  },
  {
    "start": 4041.44,
    "duration": 3.84,
    "text": "If you can come up with some line, then you\u00a0\ncan see where that line intersects with AGI\u00a0\u00a0"
  },
  {
    "start": 4045.28,
    "duration": 4.64,
    "text": "and where that would happen on the x-axis.\nPeople have proposed it's the education level.\u00a0"
  },
  {
    "start": 4049.92,
    "duration": 4.32,
    "text": "We had a high schooler, and then they went to\u00a0\ncollege with RL, and they're going to get a Ph.D.\u00a0"
  },
  {
    "start": 4054.24,
    "duration": 2.56,
    "text": "I don't like that one.\nOr they'll propose horizon\u00a0\u00a0"
  },
  {
    "start": 4056.8,
    "duration": 4.96,
    "text": "length. Maybe they can do tasks that take\u00a0\na minute, they can do those autonomously.\u00a0"
  },
  {
    "start": 4061.76,
    "duration": 3.6,
    "text": "Then they can autonomously do tasks that take\u00a0\nan hour, a human an hour, a human a week.\u00a0"
  },
  {
    "start": 4066.32,
    "duration": 6.8,
    "text": "How do you think about the relevant y-axis here?\nHow should we think about how\u00a0\u00a0"
  },
  {
    "start": 4073.12,
    "duration": 2.8,
    "text": "AI is making progress?\nI have two answers to that.\u00a0"
  },
  {
    "start": 4075.92,
    "duration": 3.44,
    "text": "Number one, I'm almost tempted to\u00a0\nreject the question entirely because\u00a0\u00a0"
  },
  {
    "start": 4079.36,
    "duration": 3.52,
    "text": "I see this as an extension of computing.\nHave we talked about how to chart progress\u00a0\u00a0"
  },
  {
    "start": 4082.88,
    "duration": 3.52,
    "text": "in computing, or how do you chart progress\u00a0\nin computing since the 1970s or whatever?\u00a0\u00a0"
  },
  {
    "start": 4086.4,
    "duration": 4.8,
    "text": "What is the y-axis? The whole question is\u00a0\nfunny from that perspective a little bit.\u00a0"
  },
  {
    "start": 4093.44,
    "duration": 5.52,
    "text": "When people talk about AI and the original AGI\u00a0\nand how we spoke about it when OpenAI started,\u00a0\u00a0"
  },
  {
    "start": 4098.96,
    "duration": 8.96,
    "text": "AGI was a system you could go to that can do any\u00a0\neconomically valuable task at human performance\u00a0\u00a0"
  },
  {
    "start": 4107.92,
    "duration": 4.0,
    "text": "or better. That was the definition. I\u00a0\nwas pretty happy with that at the time.\u00a0"
  },
  {
    "start": 4112.88,
    "duration": 3.6,
    "text": "I've stuck to that definition forever, and\u00a0\nthen people have made up all kinds of other\u00a0\u00a0"
  },
  {
    "start": 4116.48,
    "duration": 6.88,
    "text": "definitions. But I like that definition. The first\u00a0\nconcession that people make all the time is they\u00a0\u00a0"
  },
  {
    "start": 4123.36,
    "duration": 4.8,
    "text": "just take out all the physical stuff because\u00a0\nwe're just talking about digital knowledge work.\u00a0"
  },
  {
    "start": 4128.16,
    "duration": 3.92,
    "text": "That's a pretty major concession compared to\u00a0\nthe original definition, which was any task\u00a0\u00a0"
  },
  {
    "start": 4132.08,
    "duration": 5.6,
    "text": "a human can do. I can lift things, etc. AI\u00a0\ncan't do that, obviously, but we'll take it.\u00a0"
  },
  {
    "start": 4137.68,
    "duration": 5.04,
    "text": "What fraction of the economy are we taking away\u00a0\nby saying, \"Oh, only knowledge work?\" I don't know\u00a0\u00a0"
  },
  {
    "start": 4142.72,
    "duration": 7.2,
    "text": "the numbers. I feel about 10% to 20%, if I had to\u00a0\nguess, is only knowledge work, someone could work\u00a0\u00a0"
  },
  {
    "start": 4149.92,
    "duration": 4.96,
    "text": "from home and perform tasks, something like that.\nIt's still a really large market.\u00a0"
  },
  {
    "start": 4156.64,
    "duration": 2.72,
    "text": "What is the size of the\u00a0\neconomy, and what is 10% or 20%?\u00a0"
  },
  {
    "start": 4159.36,
    "duration": 6.72,
    "text": "We're still talking about a few trillion\u00a0\ndollars, even in the US, of market share or work.\u00a0"
  },
  {
    "start": 4166.08,
    "duration": 4.08,
    "text": "So it's still a very massive bucket.\nGoing back to the definition,\u00a0\u00a0"
  },
  {
    "start": 4170.16,
    "duration": 3.68,
    "text": "what I would be looking for is to\u00a0\nwhat extent is that definition true?\u00a0"
  },
  {
    "start": 4175.2,
    "duration": 5.2,
    "text": "Are there jobs or lots of tasks?\nIf we think of tasks as not jobs but tasks.\u00a0"
  },
  {
    "start": 4180.4,
    "duration": 6.0,
    "text": "It's difficult because the problem is society will\u00a0\nrefactor based on the tasks that make up jobs,\u00a0\u00a0"
  },
  {
    "start": 4187.44,
    "duration": 4.72,
    "text": "based on what's automatable or not.\nToday, what jobs are replaceable by AI?\u00a0"
  },
  {
    "start": 4192.16,
    "duration": 4.96,
    "text": "A good example recently was Geoff Hinton's\u00a0\nprediction that radiologists would not be\u00a0\u00a0"
  },
  {
    "start": 4197.12,
    "duration": 2.88,
    "text": "a job anymore, and this turned out\u00a0\nto be very wrong in a bunch of ways.\u00a0"
  },
  {
    "start": 4200.72,
    "duration": 3.52,
    "text": "Radiologists are alive and well and growing,\u00a0\neven though computer vision is really,\u00a0\u00a0"
  },
  {
    "start": 4204.24,
    "duration": 3.28,
    "text": "really good at recognizing all the different\u00a0\nthings that they have to recognize in images.\u00a0"
  },
  {
    "start": 4207.52,
    "duration": 4.08,
    "text": "It's just a messy, complicated job with a\u00a0\nlot of surfaces and dealing with patients\u00a0\u00a0"
  },
  {
    "start": 4211.6,
    "duration": 6.08,
    "text": "and all this stuff in the context of it.\nI don't know that by that definition\u00a0\u00a0"
  },
  {
    "start": 4217.68,
    "duration": 5.12,
    "text": "AI has made a huge dent yet.\nSome of the jobs that I would\u00a0\u00a0"
  },
  {
    "start": 4222.8,
    "duration": 4.32,
    "text": "be looking for have some features that make it\u00a0\nvery amenable to automation earlier than later.\u00a0"
  },
  {
    "start": 4227.12,
    "duration": 3.84,
    "text": "As an example, call center employees\u00a0\noften come up, and I think rightly so.\u00a0"
  },
  {
    "start": 4230.96,
    "duration": 4.8,
    "text": "Call center employees have a number of simplifying\u00a0\nproperties with respect to what's automatable\u00a0\u00a0"
  },
  {
    "start": 4235.76,
    "duration": 6.56,
    "text": "today. Their jobs are pretty simple. It's a\u00a0\nsequence of tasks, and every task looks similar.\u00a0"
  },
  {
    "start": 4242.32,
    "duration": 3.44,
    "text": "You take a phone call with a person, it's\u00a0\n10 minutes of interaction or whatever it is,\u00a0\u00a0"
  },
  {
    "start": 4245.76,
    "duration": 3.2,
    "text": "probably a bit longer.\nIn my experience, a lot longer.\u00a0"
  },
  {
    "start": 4249.6,
    "duration": 3.68,
    "text": "You complete some task in some scheme,\u00a0\nand you change some database entries\u00a0\u00a0"
  },
  {
    "start": 4253.28,
    "duration": 2.32,
    "text": "around or something like that.\nSo you keep repeating something\u00a0\u00a0"
  },
  {
    "start": 4255.6,
    "duration": 5.6,
    "text": "over and over again, and that's your job.\nYou do want to bring in the task horizon\u2014how\u00a0\u00a0"
  },
  {
    "start": 4261.2,
    "duration": 3.84,
    "text": "long it takes to perform a task\u2014and\u00a0\nthen you want to also remove context.\u00a0"
  },
  {
    "start": 4265.04,
    "duration": 3.84,
    "text": "You're not dealing with different parts of\u00a0\nservices of companies or other customers.\u00a0"
  },
  {
    "start": 4268.88,
    "duration": 2.96,
    "text": "It's just the database, you,\u00a0\nand a person you're serving.\u00a0"
  },
  {
    "start": 4271.84,
    "duration": 3.68,
    "text": "It's more closed, it's more\u00a0\nunderstandable, it's purely digital.\u00a0"
  },
  {
    "start": 4275.52,
    "duration": 3.28,
    "text": "So I would be looking for those things.\nBut even there, I'm not looking\u00a0\u00a0"
  },
  {
    "start": 4278.8,
    "duration": 2.96,
    "text": "at full automation yet.\nI'm looking for an autonomy slider.\u00a0"
  },
  {
    "start": 4281.76,
    "duration": 3.84,
    "text": "I expect that we are not going\u00a0\nto instantly replace people.\u00a0"
  },
  {
    "start": 4285.6,
    "duration": 3.44,
    "text": "We're going to be swapping in\u00a0\nAIs that do 80% of the volume.\u00a0"
  },
  {
    "start": 4289.04,
    "duration": 4.32,
    "text": "They delegate 20% of the volume to humans,\u00a0\nand humans are supervising teams of five AIs\u00a0\u00a0"
  },
  {
    "start": 4293.36,
    "duration": 6.24,
    "text": "doing the call center work that's more rote.\nI would be looking for new interfaces or new\u00a0\u00a0"
  },
  {
    "start": 4299.6,
    "duration": 4.8,
    "text": "companies that provide some\u00a0\nlayer that allows you to manage\u00a0\u00a0"
  },
  {
    "start": 4304.4,
    "duration": 4.48,
    "text": "some of these AIs that are not yet perfect.\nThen I would expect that across the economy.\u00a0"
  },
  {
    "start": 4308.88,
    "duration": 2.64,
    "text": "A lot of jobs are a lot harder\u00a0\nthan a call center employee.\u00a0"
  },
  {
    "start": 4312.08,
    "duration": 4.24,
    "text": "With radiologists, I'm totally\u00a0\nspeculating and I have no idea what\u00a0\u00a0"
  },
  {
    "start": 4316.32,
    "duration": 7.6,
    "text": "the actual workflow of a radiologist involves.\nBut one analogy that might be applicable is when\u00a0\u00a0"
  },
  {
    "start": 4323.92,
    "duration": 5.36,
    "text": "Waymos were first being rolled out, there'd be a\u00a0\nperson sitting in the front seat, and you just had\u00a0\u00a0"
  },
  {
    "start": 4329.28,
    "duration": 3.36,
    "text": "to have them there to make sure that if something\u00a0\nwent really wrong, they're there to monitor.\u00a0"
  },
  {
    "start": 4332.64,
    "duration": 2.8,
    "text": "Even today, people are still watching\u00a0\nto make sure things are going well.\u00a0"
  },
  {
    "start": 4335.44,
    "duration": 3.68,
    "text": "Robotaxi, which was just deployed,\u00a0\nstill has a person inside it.\u00a0"
  },
  {
    "start": 4339.12,
    "duration": 6.24,
    "text": "Now we could be in a similar situation where\u00a0\nif you automate 99% of a job, that last 1%\u00a0\u00a0"
  },
  {
    "start": 4345.36,
    "duration": 4.4,
    "text": "the human has to do is incredibly valuable\u00a0\nbecause it's bottlenecking everything else.\u00a0"
  },
  {
    "start": 4349.76,
    "duration": 5.28,
    "text": "If it were the case with radiologists, where\u00a0\nthe person sitting in the front of Waymo has\u00a0\u00a0"
  },
  {
    "start": 4355.04,
    "duration": 4.16,
    "text": "to be specially trained for years in order\u00a0\nto provide the last 1%, their wages should\u00a0\u00a0"
  },
  {
    "start": 4359.2,
    "duration": 4.48,
    "text": "go up tremendously because they're the\u00a0\none thing bottlenecking wide deployment.\u00a0"
  },
  {
    "start": 4363.68,
    "duration": 2.96,
    "text": "Radiologists, I think their wages have\u00a0\ngone up for similar reasons, if you're\u00a0\u00a0"
  },
  {
    "start": 4366.64,
    "duration": 5.36,
    "text": "the last bottleneck and you're not fungible.\nA Waymo driver might be fungible with others.\u00a0"
  },
  {
    "start": 4373.52,
    "duration": 4.32,
    "text": "So you might see this thing where your wages\u00a0\ngo up until you get to 99% and then fall just\u00a0\u00a0"
  },
  {
    "start": 4377.84,
    "duration": 5.12,
    "text": "like that when the last 1% is gone.\nAnd I wonder if we're seeing similar\u00a0\u00a0"
  },
  {
    "start": 4382.96,
    "duration": 3.6,
    "text": "things with radiology or salaries of call\u00a0\ncenter workers or anything like that.\u00a0"
  },
  {
    "start": 4387.44,
    "duration": 4.56,
    "text": "That's an interesting question. I don't think\u00a0\nwe're currently seeing that with radiology.\u00a0"
  },
  {
    "start": 4395.6,
    "duration": 3.52,
    "text": "I think radiology is not a good example.\nI don't know why Geoff Hinton picked\u00a0\u00a0"
  },
  {
    "start": 4399.12,
    "duration": 5.28,
    "text": "on radiology because I think it's an\u00a0\nextremely messy, complicated profession.\u00a0"
  },
  {
    "start": 4405.04,
    "duration": 2.96,
    "text": "I would be a lot more interested in what's\u00a0\nhappening with call center employees today,\u00a0\u00a0"
  },
  {
    "start": 4408.0,
    "duration": 4.64,
    "text": "for example, because I would expect a lot\u00a0\nof the rote stuff to be automatable today.\u00a0"
  },
  {
    "start": 4412.64,
    "duration": 3.36,
    "text": "I don't have first-level access to it but\u00a0\nI would be looking for trends of what's\u00a0\u00a0"
  },
  {
    "start": 4416.0,
    "duration": 4.08,
    "text": "happening with the call center employees.\nSome of the things I would also expect\u00a0\u00a0"
  },
  {
    "start": 4420.08,
    "duration": 4.96,
    "text": "is that maybe they are swapping in AI, but\u00a0\nthen I would still wait for a year or two\u00a0\u00a0"
  },
  {
    "start": 4425.04,
    "duration": 4.48,
    "text": "because I would potentially expect them to\u00a0\npull back and rehire some of the people.\u00a0"
  },
  {
    "start": 4429.52,
    "duration": 3.6,
    "text": "There's been evidence that that's already been\u00a0\nhappening generally in companies that have been\u00a0\u00a0"
  },
  {
    "start": 4433.12,
    "duration": 6.72,
    "text": "adopting AI, which I think is quite surprising.\nI also found what was really surprising. AGI,\u00a0\u00a0"
  },
  {
    "start": 4439.84,
    "duration": 4.48,
    "text": "right? A thing which would do everything.\nWe'll take out physical work,\u00a0\u00a0"
  },
  {
    "start": 4444.32,
    "duration": 4.72,
    "text": "but it should be able to do all knowledge work.\nWhat you would have naively anticipated is that\u00a0\u00a0"
  },
  {
    "start": 4449.04,
    "duration": 5.2,
    "text": "the way this progression would happen is\u00a0\nthat you take a little task that a consultant\u00a0\u00a0"
  },
  {
    "start": 4454.24,
    "duration": 5.04,
    "text": "is doing, you take that out of the bucket.\nYou take a little task that an accountant is\u00a0\u00a0"
  },
  {
    "start": 4459.28,
    "duration": 3.04,
    "text": "doing, you take that out of the bucket.\nThen you're just doing this\u00a0\u00a0"
  },
  {
    "start": 4462.32,
    "duration": 2.96,
    "text": "across all knowledge work.\nBut instead, if we do believe we're\u00a0\u00a0"
  },
  {
    "start": 4465.28,
    "duration": 3.6,
    "text": "on the path of AGI with the current paradigm,\u00a0\nthe progression is very much not like that.\u00a0"
  },
  {
    "start": 4470.48,
    "duration": 3.52,
    "text": "It does not seem like consultants and accountants\u00a0\nare getting huge productivity improvements.\u00a0"
  },
  {
    "start": 4474.0,
    "duration": 5.92,
    "text": "It's very much like programmers are getting\u00a0\nmore and more chiseled away at their work.\u00a0"
  },
  {
    "start": 4479.92,
    "duration": 6.88,
    "text": "If you look at the revenues of these companies,\u00a0\ndiscounting normal chat revenue\u2014which is similar\u00a0\u00a0"
  },
  {
    "start": 4486.8,
    "duration": 4.88,
    "text": "to Google or something\u2014just looking at\u00a0\nAPI revenues, it's dominated by coding.\u00a0"
  },
  {
    "start": 4491.68,
    "duration": 4.48,
    "text": "So this thing which is \"general\", which\u00a0\nshould be able to do any knowledge work,\u00a0\u00a0"
  },
  {
    "start": 4496.16,
    "duration": 4.08,
    "text": "is just overwhelmingly doing only coding.\nIt's a surprising way that you would\u00a0\u00a0"
  },
  {
    "start": 4500.24,
    "duration": 4.48,
    "text": "expect the AGI to be deployed.\nThere's an interesting point\u00a0\u00a0"
  },
  {
    "start": 4504.72,
    "duration": 7.68,
    "text": "here. I do believe coding is the perfect\u00a0\nfirst thing for these LLMs and agents.\u00a0"
  },
  {
    "start": 4512.4,
    "duration": 4.64,
    "text": "That\u2019s because coding has always\u00a0\nfundamentally worked around text.\u00a0"
  },
  {
    "start": 4517.04,
    "duration": 3.52,
    "text": "It's computer terminals and text,\u00a0\nand everything is based around text.\u00a0"
  },
  {
    "start": 4520.56,
    "duration": 3.92,
    "text": "LLMs, the way they're trained\u00a0\non the Internet, love text.\u00a0"
  },
  {
    "start": 4524.48,
    "duration": 4.24,
    "text": "They're perfect text processors, and there's\u00a0\nall this data out there. It's a perfect fit.\u00a0\u00a0"
  },
  {
    "start": 4529.52,
    "duration": 4.0,
    "text": "We also have a lot of infrastructure\u00a0\npre-built for handling code and text.\u00a0"
  },
  {
    "start": 4533.52,
    "duration": 7.52,
    "text": "For example, we have Visual Studio Code\u00a0\nor your favorite IDE showing you code,\u00a0\u00a0"
  },
  {
    "start": 4541.04,
    "duration": 4.24,
    "text": "and an agent can plug into that.\nIf an agent has a diff where it made some change,\u00a0\u00a0"
  },
  {
    "start": 4545.28,
    "duration": 5.76,
    "text": "we suddenly have all this code already that shows\u00a0\nall the differences to a code base using a diff.\u00a0"
  },
  {
    "start": 4551.04,
    "duration": 4.88,
    "text": "It's almost like we've pre-built a\u00a0\nlot of the infrastructure for code.\u00a0"
  },
  {
    "start": 4555.92,
    "duration": 2.96,
    "text": "Contrast that with some of the\u00a0\nthings that don't enjoy that at all.\u00a0"
  },
  {
    "start": 4558.88,
    "duration": 4.96,
    "text": "As an example, there are people trying to build\u00a0\nautomation not for coding, but for slides.\u00a0"
  },
  {
    "start": 4563.84,
    "duration": 3.52,
    "text": "I saw a company doing slides. That's\u00a0\nmuch, much harder. The reason it's\u00a0\u00a0"
  },
  {
    "start": 4567.36,
    "duration": 4.48,
    "text": "much harder is because slides are not text.\nSlides are little graphics, they're arranged\u00a0\u00a0"
  },
  {
    "start": 4571.84,
    "duration": 7.04,
    "text": "spatially, and there's a visual component to it.\nSlides don't have this pre-built infrastructure.\u00a0"
  },
  {
    "start": 4578.88,
    "duration": 5.44,
    "text": "For example, if an agent is to make a change to\u00a0\nyour slides, how does a thing show you the diff?\u00a0"
  },
  {
    "start": 4584.32,
    "duration": 2.64,
    "text": "How do you see the diff?\nThere's nothing that shows diffs\u00a0\u00a0"
  },
  {
    "start": 4586.96,
    "duration": 7.44,
    "text": "for slides. Someone has to build it. Some of these\u00a0\nthings are not amenable to AIs as they are, which\u00a0\u00a0"
  },
  {
    "start": 4594.4,
    "duration": 5.84,
    "text": "are text processors, and code surprisingly is.\nI\u2019m not sure that alone explains it.\u00a0"
  },
  {
    "start": 4602.16,
    "duration": 7.12,
    "text": "I personally have tried to get LLMs to be useful\u00a0\nin domains which are just pure language-in,\u00a0\u00a0"
  },
  {
    "start": 4609.28,
    "duration": 6.08,
    "text": "language-out, like rewriting transcripts,\u00a0\ncoming up with clips based on transcripts.\u00a0"
  },
  {
    "start": 4617.6,
    "duration": 3.12,
    "text": "It's very plausible that I didn't do\u00a0\nevery single possible thing I could do.\u00a0"
  },
  {
    "start": 4620.72,
    "duration": 5.2,
    "text": "I put a bunch of good examples in context, but\u00a0\nmaybe I should have done some kind of fine-tuning.\u00a0"
  },
  {
    "start": 4626.56,
    "duration": 7.12,
    "text": "Our mutual friend, Andy Matuschak, told me that\u00a0\nhe tried 50 billion things to try to get models\u00a0\u00a0"
  },
  {
    "start": 4633.68,
    "duration": 5.76,
    "text": "to be good at writing spaced repetition prompts.\nAgain, very much language-in, language-out tasks,\u00a0\u00a0"
  },
  {
    "start": 4639.44,
    "duration": 3.12,
    "text": "the kind of thing that should be dead\u00a0\ncenter in the repertoire of these LLMs.\u00a0"
  },
  {
    "start": 4642.56,
    "duration": 3.36,
    "text": "He tried in-context learning\u00a0\nwith a few-shot examples.\u00a0"
  },
  {
    "start": 4645.92,
    "duration": 9.6,
    "text": "He tried supervised fine-tuning and retrieval.\nHe could not get them to make\u00a0\u00a0"
  },
  {
    "start": 4655.52,
    "duration": 4.24,
    "text": "cards to his satisfaction.\nSo I find it striking that even in language-out\u00a0\u00a0"
  },
  {
    "start": 4659.76,
    "duration": 5.84,
    "text": "domains, it's very hard to get a lot of economic\u00a0\nvalue out of these models separate from coding.\u00a0"
  },
  {
    "start": 4665.6,
    "duration": 6.4,
    "text": "I don't know what explains it.\nThat makes sense. I'm not\u00a0\u00a0"
  },
  {
    "start": 4672.0,
    "duration": 5.28,
    "text": "saying that anything text is trivial.\nI do think that code is pretty structured.\u00a0"
  },
  {
    "start": 4678.72,
    "duration": 6.16,
    "text": "Text is maybe a lot more flowery, and there's\u00a0\na lot more entropy in text, I would say.\u00a0"
  },
  {
    "start": 4684.88,
    "duration": 5.52,
    "text": "I don't know how else to put it.\nAlso code is hard, and so people feel quite\u00a0\u00a0"
  },
  {
    "start": 4690.4,
    "duration": 8.88,
    "text": "empowered by LLMs, even from simple knowledge.\nI don't know that I have a very good answer.\u00a0"
  },
  {
    "start": 4699.28,
    "duration": 5.04,
    "text": "Obviously, text makes it much, much easier,\u00a0\nbut it doesn't mean that all text is trivial.\u00a0"
  },
  {
    "start": 4705.2,
    "duration": 4.56,
    "text": "How do you think about superintelligence?\nDo you expect it to feel qualitatively different\u00a0\u00a0"
  },
  {
    "start": 4709.76,
    "duration": 7.28,
    "text": "from normal humans or human companies?\nI see it as a progression\u00a0\u00a0"
  },
  {
    "start": 4717.04,
    "duration": 5.28,
    "text": "of automation in society.\nExtrapolating the trend of computing, there will\u00a0\u00a0"
  },
  {
    "start": 4722.32,
    "duration": 4.32,
    "text": "be a gradual automation of a lot of things, and\u00a0\nsuperintelligence will an extrapolation of that.\u00a0"
  },
  {
    "start": 4727.6,
    "duration": 2.96,
    "text": "We expect more and more autonomous\u00a0\nentities over time that are doing a lot\u00a0\u00a0"
  },
  {
    "start": 4730.56,
    "duration": 5.52,
    "text": "of the digital work and then eventually even\u00a0\nthe physical work some amount of time later.\u00a0"
  },
  {
    "start": 4736.08,
    "duration": 4.48,
    "text": "Basically I see it as just\u00a0\nautomation, roughly speaking.\u00a0"
  },
  {
    "start": 4740.56,
    "duration": 2.56,
    "text": "But automation includes the things humans\u00a0\ncan already do, and superintelligence\u00a0\u00a0"
  },
  {
    "start": 4743.12,
    "duration": 2.8,
    "text": "implies things humans can\u2019t do.\nBut one of the things that people\u00a0\u00a0"
  },
  {
    "start": 4745.92,
    "duration": 4.0,
    "text": "do is invent new things, which I would just\u00a0\nput into the automation if that makes sense.\u00a0"
  },
  {
    "start": 4750.96,
    "duration": 7.44,
    "text": "But I guess, less abstractly and more\u00a0\nqualitatively, do you expect something\u00a0\u00a0"
  },
  {
    "start": 4758.4,
    "duration": 8.4,
    "text": "to feel like\u2026 Because this thing can either think\u00a0\nso fast, or has so many copies, or the copies can\u00a0\u00a0"
  },
  {
    "start": 4766.8,
    "duration": 8.4,
    "text": "merge back into themselves, or is much smarter,\u00a0\nany number of advantages an AI might have, will\u00a0\u00a0"
  },
  {
    "start": 4776.0,
    "duration": 3.92,
    "text": "the civilization in which these AIs exist will\u00a0\njust feel qualitatively different from humans?\u00a0"
  },
  {
    "start": 4779.92,
    "duration": 5.04,
    "text": "I think it will. It is fundamentally automation,\u00a0\nbut it will be extremely foreign. It will look\u00a0\u00a0"
  },
  {
    "start": 4784.96,
    "duration": 6.72,
    "text": "really strange. Like you mentioned, we can run\u00a0\nall of this on a computer cluster and much faster.\u00a0"
  },
  {
    "start": 4793.04,
    "duration": 5.36,
    "text": "Some of the scenarios that I start to get\u00a0\nnervous about when the world looks like\u00a0\u00a0"
  },
  {
    "start": 4798.4,
    "duration": 3.36,
    "text": "that is this gradual loss of control\u00a0\nand understanding of what's happening.\u00a0"
  },
  {
    "start": 4801.76,
    "duration": 4.32,
    "text": "I think that's the most likely outcome, that\u00a0\nthere will be a gradual loss of understanding.\u00a0"
  },
  {
    "start": 4807.52,
    "duration": 2.88,
    "text": "We'll gradually layer all this stuff\u00a0\neverywhere, and there will be fewer\u00a0\u00a0"
  },
  {
    "start": 4810.4,
    "duration": 4.24,
    "text": "and fewer people who understand it.\nThen there will be a gradual loss of\u00a0\u00a0"
  },
  {
    "start": 4814.64,
    "duration": 4.72,
    "text": "control and understanding of what's happening.\nThat to me seems the most likely outcome of how\u00a0\u00a0"
  },
  {
    "start": 4819.36,
    "duration": 2.64,
    "text": "all this stuff will go down.\nLet me probe on that a bit.\u00a0"
  },
  {
    "start": 4822.0,
    "duration": 5.52,
    "text": "It's not clear to me that loss of control and\u00a0\nloss of understanding are the same things.\u00a0"
  },
  {
    "start": 4827.52,
    "duration": 9.52,
    "text": "A board of directors at TSMC, Intel\u2014name a random\u00a0\ncompany\u2014they're just prestigious 80-year-olds.\u00a0"
  },
  {
    "start": 4837.04,
    "duration": 3.76,
    "text": "They have very little understanding, and maybe\u00a0\nthey don't practically actually have control.\u00a0"
  },
  {
    "start": 4843.2,
    "duration": 0.56,
    "text": "A better example\u00a0\u00a0"
  },
  {
    "start": 4843.76,
    "duration": 3.84,
    "text": "is the President of the United States.\nThe President has a lot of fucking power.\u00a0"
  },
  {
    "start": 4848.8,
    "duration": 4.24,
    "text": "I'm not trying to make a good statement\u00a0\nabout the current operant, or maybe I am,\u00a0\u00a0"
  },
  {
    "start": 4853.04,
    "duration": 3.44,
    "text": "but the actual level of understanding is\u00a0\nvery different from the level of control.\u00a0"
  },
  {
    "start": 4856.48,
    "duration": 8.72,
    "text": "I think that's fair. That's a good\u00a0\npushback. I think I expect loss of both.\u00a0"
  },
  {
    "start": 4865.2,
    "duration": 3.68,
    "text": "How come? Loss of understanding is\u00a0\nobvious, but why loss of control?\u00a0"
  },
  {
    "start": 4870.16,
    "duration": 4.24,
    "text": "We're really far into a territory where I\u00a0\ndon't know what this looks like, but if I\u00a0\u00a0"
  },
  {
    "start": 4874.4,
    "duration": 7.6,
    "text": "were to write sci-fi novels, they would look along\u00a0\nthe lines of not even a single entity that takes\u00a0\u00a0"
  },
  {
    "start": 4882.0,
    "duration": 4.48,
    "text": "over everything, but multiple competing entities\u00a0\nthat gradually become more and more autonomous.\u00a0"
  },
  {
    "start": 4887.36,
    "duration": 2.64,
    "text": "Some of them go rogue and\u00a0\nthe others fight them off.\u00a0"
  },
  {
    "start": 4891.04,
    "duration": 6.56,
    "text": "It's this hot pot of completely autonomous\u00a0\nactivity that we've delegated to.\u00a0"
  },
  {
    "start": 4897.6,
    "duration": 6.0,
    "text": "I feel it would have that flavor.\nIt is not the fact that they are smarter\u00a0\u00a0"
  },
  {
    "start": 4903.6,
    "duration": 4.24,
    "text": "than us that is resulting in the loss of control.\nIt's the fact that they are competing with each\u00a0\u00a0"
  },
  {
    "start": 4907.84,
    "duration": 6.56,
    "text": "other, and whatever arises out of that\u00a0\ncompetition leads to the loss of control.\u00a0"
  },
  {
    "start": 4918.96,
    "duration": 6.0,
    "text": "A lot of these things, they will be\u00a0\ntools to people, they're acting on\u00a0\u00a0"
  },
  {
    "start": 4924.96,
    "duration": 2.48,
    "text": "behalf of people or something like that.\nSo maybe those people are in control,\u00a0\u00a0"
  },
  {
    "start": 4927.44,
    "duration": 4.8,
    "text": "but maybe it's a loss of control overall for\u00a0\nsociety in the sense of outcomes we want.\u00a0"
  },
  {
    "start": 4933.84,
    "duration": 5.52,
    "text": "You have entities acting on behalf of individuals\u00a0\nthat are still roughly seen as out of control.\u00a0"
  },
  {
    "start": 4940.08,
    "duration": 3.92,
    "text": "This is a question I should have asked earlier.\nWe were talking about how currently it feels like\u00a0\u00a0"
  },
  {
    "start": 4944.0,
    "duration": 5.2,
    "text": "when you're doing AI engineering or AI research,\u00a0\nthese models are more in the category of compiler\u00a0\u00a0"
  },
  {
    "start": 4949.2,
    "duration": 5.44,
    "text": "rather than in the category of a replacement.\nAt some point, if you have AGI,\u00a0\u00a0"
  },
  {
    "start": 4954.64,
    "duration": 4.08,
    "text": "it should be able to do what you do.\nDo you feel like having a million\u00a0\u00a0"
  },
  {
    "start": 4958.72,
    "duration": 4.32,
    "text": "copies of you in parallel results in\u00a0\nsome huge speed-up of AI progress?\u00a0"
  },
  {
    "start": 4963.76,
    "duration": 5.596,
    "text": "If that does happen, do you expect to see an\u00a0\nintelligence explosion once we have a true AGI?\u00a0"
  },
  {
    "start": 4969.356,
    "duration": 6.724,
    "text": "I'm not talking about LLMs today.\nI do, but it's business as usual because\u00a0\u00a0"
  },
  {
    "start": 4976.08,
    "duration": 2.96,
    "text": "we're in an intelligence explosion\u00a0\nalready and have been for decades.\u00a0"
  },
  {
    "start": 4980.0,
    "duration": 3.6,
    "text": "It's basically the GDP curve that is\u00a0\nan exponential weighted sum over so\u00a0\u00a0"
  },
  {
    "start": 4983.6,
    "duration": 2.56,
    "text": "many aspects of the industry.\nEverything is gradually being\u00a0\u00a0"
  },
  {
    "start": 4986.16,
    "duration": 4.24,
    "text": "automated and has been for hundreds of years.\nThe Industrial Revolution is automation and\u00a0\u00a0"
  },
  {
    "start": 4990.4,
    "duration": 2.64,
    "text": "some of the physical components and\u00a0\ntool building and all this stuff.\u00a0"
  },
  {
    "start": 4993.04,
    "duration": 3.68,
    "text": "Compilers are early software\u00a0\nautomation, et cetera.\u00a0"
  },
  {
    "start": 4996.72,
    "duration": 5.04,
    "text": "We've been recursively self-improving\u00a0\nand exploding for a long time.\u00a0"
  },
  {
    "start": 5001.76,
    "duration": 5.68,
    "text": "Another way to see it is that Earth was a pretty\u00a0\nboring place if you don't look at the biomechanics\u00a0\u00a0"
  },
  {
    "start": 5007.44,
    "duration": 6.0,
    "text": "and so on, and looked very similar.\nIf you look from space, we're in the\u00a0\u00a0"
  },
  {
    "start": 5013.44,
    "duration": 4.08,
    "text": "middle of this firecracker event,\u00a0\nbut we're seeing it in slow motion.\u00a0"
  },
  {
    "start": 5018.32,
    "duration": 4.48,
    "text": "I definitely feel like this has\u00a0\nalready happened for a very long time.\u00a0"
  },
  {
    "start": 5022.8,
    "duration": 4.4,
    "text": "Again, I don't see AI as a distinct\u00a0\ntechnology with respect to what has\u00a0\u00a0"
  },
  {
    "start": 5027.2,
    "duration": 3.6,
    "text": "already been happening for a long time.\nYou think it's continuous with this\u00a0\u00a0"
  },
  {
    "start": 5030.8,
    "duration": 3.04,
    "text": "hyper-exponential trend?\nYes. That's why this was\u00a0\u00a0"
  },
  {
    "start": 5033.84,
    "duration": 3.68,
    "text": "very interesting to me, because I was\u00a0\ntrying to find AI in the GDP for a while.\u00a0"
  },
  {
    "start": 5037.52,
    "duration": 3.6,
    "text": "I thought that GDP should go up.\nBut then I looked at some of the\u00a0\u00a0"
  },
  {
    "start": 5041.12,
    "duration": 2.96,
    "text": "other technologies that I thought\u00a0\nwere very transformative, like\u00a0\u00a0"
  },
  {
    "start": 5044.72,
    "duration": 3.76,
    "text": "computers or mobile phones or et cetera.\nYou can't find them in GDP. GDP is the same\u00a0\u00a0"
  },
  {
    "start": 5048.48,
    "duration": 5.36,
    "text": "exponential. Even the early iPhone didn't have the\u00a0\nApp Store, and it didn't have a lot of the bells\u00a0\u00a0"
  },
  {
    "start": 5053.84,
    "duration": 4.16,
    "text": "and whistles that the modern iPhone has.\nSo even though we think of 2008,\u00a0\u00a0"
  },
  {
    "start": 5058.0,
    "duration": 3.52,
    "text": "when the iPhone came out, as this major\u00a0\nseismic change, it's actually not.\u00a0"
  },
  {
    "start": 5061.52,
    "duration": 3.84,
    "text": "Everything is so spread out and it so\u00a0\nslowly diffuses that everything ends up\u00a0\u00a0"
  },
  {
    "start": 5065.36,
    "duration": 3.36,
    "text": "being averaged up into the same exponential.\nIt's the exact same thing with computers.\u00a0"
  },
  {
    "start": 5068.72,
    "duration": 2.72,
    "text": "You can't find them in the GDP\u00a0\nlike, \"Oh, we have computers now.\"\u00a0"
  },
  {
    "start": 5071.44,
    "duration": 2.32,
    "text": "That's not what happened, because\u00a0\nit's such slow progression.\u00a0"
  },
  {
    "start": 5073.76,
    "duration": 3.84,
    "text": "With AI we're going to see the exact same thing.\u00a0\nIt's just more automation. It allows us to write\u00a0\u00a0"
  },
  {
    "start": 5077.6,
    "duration": 4.24,
    "text": "different kinds of programs that we couldn't write\u00a0\nbefore, but AI is still fundamentally a program.\u00a0"
  },
  {
    "start": 5082.56,
    "duration": 4.32,
    "text": "It's a new kind of computer and\u00a0\na new kind of computing system.\u00a0"
  },
  {
    "start": 5086.88,
    "duration": 2.96,
    "text": "But it has all these problems,\u00a0\nit's going to diffuse over time,\u00a0\u00a0"
  },
  {
    "start": 5089.84,
    "duration": 2.24,
    "text": "and it's still going to add\u00a0\nup to the same exponential.\u00a0"
  },
  {
    "start": 5092.08,
    "duration": 4.32,
    "text": "We're still going to have an exponential\u00a0\nthat's going to get extremely vertical.\u00a0"
  },
  {
    "start": 5096.4,
    "duration": 3.12,
    "text": "It's going to be very foreign to\u00a0\nlive in that kind of an environment.\u00a0"
  },
  {
    "start": 5099.52,
    "duration": 6.08,
    "text": "Are you saying that, if you look at the trend\u00a0\nbefore the Industrial Revolution to now,\u00a0\u00a0"
  },
  {
    "start": 5105.6,
    "duration": 5.44,
    "text": "you have a hyper-exponential where you go\u00a0\nfrom 0% growth to then 10,000 years ago,\u00a0\u00a0"
  },
  {
    "start": 5111.04,
    "duration": 4.24,
    "text": "0.02% growth, and to now when we're at 2%\u00a0\ngrowth. That's a hyper-exponential. Are you\u00a0\u00a0"
  },
  {
    "start": 5115.28,
    "duration": 4.88,
    "text": "saying if you're charting AI on there, then\u00a0\nAI takes you to 20% growth or 200% growth?\u00a0"
  },
  {
    "start": 5120.16,
    "duration": 3.44,
    "text": "Or are you saying that if you look at\u00a0\nthe last 300 years, what you've been\u00a0\u00a0"
  },
  {
    "start": 5123.6,
    "duration": 3.6,
    "text": "seeing is that you have technology after\u00a0\ntechnology\u2014computers, electrification,\u00a0\u00a0"
  },
  {
    "start": 5127.2,
    "duration": 6.32,
    "text": "steam engines, railways, et cetera\u2014but the\u00a0\nrate of growth is the exact same, it's 2%.\u00a0"
  },
  {
    "start": 5133.52,
    "duration": 5.36,
    "text": "Are you saying the rate of growth will go up?\nThe rate of growth has also stayed\u00a0\u00a0"
  },
  {
    "start": 5138.88,
    "duration": 2.64,
    "text": "roughly constant, right?\nOnly over the last 200, 300 years.\u00a0"
  },
  {
    "start": 5141.52,
    "duration": 2.8,
    "text": "But over the course of\u00a0\nhuman history it's exploded.\u00a0"
  },
  {
    "start": 5144.32,
    "duration": 5.52,
    "text": "It's gone from 0% to faster, faster,\u00a0\nfaster. Industrial explosion, 2%.\u00a0"
  },
  {
    "start": 5151.12,
    "duration": 3.36,
    "text": "For a while I tried to find AI\u00a0\nor look for AI in the GDP curve,\u00a0\u00a0"
  },
  {
    "start": 5154.48,
    "duration": 3.76,
    "text": "and I've convinced myself that this is false.\nEven when people talk about recursive\u00a0\u00a0"
  },
  {
    "start": 5158.24,
    "duration": 3.44,
    "text": "self-improvement and labs and stuff\u00a0\nlike that, this is business as usual.\u00a0"
  },
  {
    "start": 5161.68,
    "duration": 4.0,
    "text": "Of course it's going to recursively self-improve,\u00a0\nand it's been recursively self-improving.\u00a0"
  },
  {
    "start": 5165.68,
    "duration": 5.44,
    "text": "LLMs allow the engineers to work much more\u00a0\nefficiently to build the next round of LLM,\u00a0\u00a0"
  },
  {
    "start": 5171.12,
    "duration": 3.76,
    "text": "and a lot more of the components are\u00a0\nbeing automated and tuned and et cetera.\u00a0"
  },
  {
    "start": 5174.88,
    "duration": 4.24,
    "text": "All the engineers having access\u00a0\nto Google Search is part of it.\u00a0"
  },
  {
    "start": 5179.12,
    "duration": 3.92,
    "text": "All the engineers having an IDE, all of them\u00a0\nhaving autocomplete or having Claude code,\u00a0\u00a0"
  },
  {
    "start": 5183.04,
    "duration": 7.84,
    "text": "et cetera, it's all just part of the same\u00a0\nspeed-up of the whole thing. It's just so smooth.\u00a0"
  },
  {
    "start": 5190.88,
    "duration": 2.96,
    "text": "Just to clarify, you're saying that\u00a0\nthe rate of growth will not change.\u00a0"
  },
  {
    "start": 5195.44,
    "duration": 4.0,
    "text": "The intelligence explosion will show up as\u00a0\nit just enabled us to continue staying on the\u00a0\u00a0"
  },
  {
    "start": 5199.44,
    "duration": 3.12,
    "text": "2% growth trajectory, just as the Internet\u00a0\nhelped us stay on the 2% growth trajectory.\u00a0"
  },
  {
    "start": 5202.56,
    "duration": 2.96,
    "text": "Yes, my expectation is that\u00a0\nit stays in the same pattern.\u00a0"
  },
  {
    "start": 5207.92,
    "duration": 7.84,
    "text": "Just to throw the opposite argument against you,\u00a0\nmy expectation is that it blows up because I think\u00a0\u00a0"
  },
  {
    "start": 5215.76,
    "duration": 5.04,
    "text": "true AGI\u2014and I'm not talking about LLM coding\u00a0\nbots, I'm talking about actual replacement of a\u00a0\u00a0"
  },
  {
    "start": 5220.8,
    "duration": 6.48,
    "text": "human in a server\u2014is qualitatively different\u00a0\nfrom these other productivity-improving\u00a0\u00a0"
  },
  {
    "start": 5227.28,
    "duration": 5.84,
    "text": "technologies because it's labor itself.\nI think we live in a very labor-constrained world.\u00a0"
  },
  {
    "start": 5233.12,
    "duration": 4.08,
    "text": "If you talk to any startup founder or any person,\u00a0\nyou can be like, what do you need more of? You\u00a0\u00a0"
  },
  {
    "start": 5237.2,
    "duration": 5.6,
    "text": "need really talented people. And if you have\u00a0\nbillions of extra people who are inventing stuff,\u00a0\u00a0"
  },
  {
    "start": 5242.8,
    "duration": 5.6,
    "text": "integrating themselves, making companies bottom\u00a0\nstart to finish, that feels qualitatively\u00a0\u00a0"
  },
  {
    "start": 5248.4,
    "duration": 4.0,
    "text": "different from a single technology.\nIt's as if you get 10 billion\u00a0\u00a0"
  },
  {
    "start": 5252.4,
    "duration": 5.36,
    "text": "extra people on the planet.\nMaybe a counterpoint. I'm pretty willing\u00a0\u00a0"
  },
  {
    "start": 5257.76,
    "duration": 4.24,
    "text": "to be convinced one way or another on this point.\nBut I will say, for example, computing is labor.\u00a0\u00a0"
  },
  {
    "start": 5262.0,
    "duration": 3.52,
    "text": "Computing was labor. Computers, a lot\u00a0\nof jobs disappeared because computers\u00a0\u00a0"
  },
  {
    "start": 5265.52,
    "duration": 4.72,
    "text": "are automating a bunch of digital information\u00a0\nprocessing that you now don't need a human for.\u00a0"
  },
  {
    "start": 5270.24,
    "duration": 7.6,
    "text": "So computers are labor, and that has played out.\nSelf-driving as an example is also computers doing\u00a0\u00a0"
  },
  {
    "start": 5277.84,
    "duration": 4.24,
    "text": "labor. That's already been playing\u00a0\nout. It's still business as usual.\u00a0"
  },
  {
    "start": 5282.8,
    "duration": 5.2,
    "text": "You have a machine which is spitting out more\u00a0\nthings like that at potentially faster pace.\u00a0"
  },
  {
    "start": 5288.0,
    "duration": 3.12,
    "text": "Historically, we have examples\u00a0\nof the growth regime changing\u00a0\u00a0"
  },
  {
    "start": 5291.12,
    "duration": 7.28,
    "text": "where you went from 0.2% growth to 2% growth.\nIt seems very plausible to me that a machine which\u00a0\u00a0"
  },
  {
    "start": 5298.4,
    "duration": 4.8,
    "text": "is then spitting out the next self-driving\u00a0\ncar and the next Internet and whatever\u2026\u00a0"
  },
  {
    "start": 5303.2,
    "duration": 4.32,
    "text": "I see where it's coming from.\nAt the same time, I do feel like\u00a0\u00a0"
  },
  {
    "start": 5307.52,
    "duration": 4.32,
    "text": "people make this assumption of, \"We\u00a0\nhave God in a box, and now it can do\u00a0\u00a0"
  },
  {
    "start": 5311.84,
    "duration": 4.4,
    "text": "everything,\" and it just won't look like that.\nIt's going to be able to do some of the things.\u00a0"
  },
  {
    "start": 5316.24,
    "duration": 3.12,
    "text": "It's going to fail at some other things.\nIt's going to be gradually put into society,\u00a0\u00a0"
  },
  {
    "start": 5319.36,
    "duration": 4.56,
    "text": "and we'll end up with the same pattern. That\u00a0\nis my prediction. This assumption of suddenly\u00a0\u00a0"
  },
  {
    "start": 5323.92,
    "duration": 5.28,
    "text": "having a completely intelligent, fully flexible,\u00a0\nfully general human in a box, and we can dispense\u00a0\u00a0"
  },
  {
    "start": 5329.2,
    "duration": 6.64,
    "text": "it at arbitrary problems in society, I don't\u00a0\nthink that we will have this discrete change.\u00a0"
  },
  {
    "start": 5337.6,
    "duration": 5.28,
    "text": "I think we'll arrive at the same kind of\u00a0\ngradual diffusion of this across the industry.\u00a0"
  },
  {
    "start": 5343.44,
    "duration": 4.64,
    "text": "It often ends up being misleading\u00a0\nin these conversations.\u00a0"
  },
  {
    "start": 5349.28,
    "duration": 3.68,
    "text": "I don't like to use the word intelligence in\u00a0\nthis context because intelligence implies you\u00a0\u00a0"
  },
  {
    "start": 5352.96,
    "duration": 5.92,
    "text": "think there'll be a single superintelligence\u00a0\nsitting in a server and it'll divine how\u00a0\u00a0"
  },
  {
    "start": 5358.88,
    "duration": 3.92,
    "text": "to come up with new technologies and\u00a0\ninventions that cause this explosion.\u00a0"
  },
  {
    "start": 5362.8,
    "duration": 3.04,
    "text": "That's not what I'm imagining\u00a0\nwhen I'm imagining 20% growth.\u00a0"
  },
  {
    "start": 5365.84,
    "duration": 7.44,
    "text": "I'm imagining that there are billions of\u00a0\nvery smart human-like minds, potentially,\u00a0\u00a0"
  },
  {
    "start": 5373.28,
    "duration": 3.2,
    "text": "or that's all that's required.\nBut the fact that there's hundreds\u00a0\u00a0"
  },
  {
    "start": 5376.48,
    "duration": 5.28,
    "text": "of millions of them, billions of them, each\u00a0\nindividually making new products, figuring\u00a0\u00a0"
  },
  {
    "start": 5381.76,
    "duration": 4.88,
    "text": "out how to integrate themselves into the economy.\nIf a highly experienced smart immigrant came to\u00a0\u00a0"
  },
  {
    "start": 5386.64,
    "duration": 2.72,
    "text": "the country, you wouldn't need to figure out how\u00a0\nwe integrate them in the economy. They figure it\u00a0\u00a0"
  },
  {
    "start": 5389.36,
    "duration": 6.08,
    "text": "out. They could start a company, they could make\u00a0\ninventions, or increase productivity in the world.\u00a0"
  },
  {
    "start": 5395.44,
    "duration": 6.0,
    "text": "We have examples, even in the current regime,\u00a0\nof places that have had 10-20% economic growth.\u00a0"
  },
  {
    "start": 5401.44,
    "duration": 4.4,
    "text": "If you just have a lot of people and\u00a0\nless capital in comparison to the people,\u00a0\u00a0"
  },
  {
    "start": 5405.84,
    "duration": 6.24,
    "text": "you can have Hong Kong or Shenzhen or\u00a0\nwhatever with decades of 10% plus growth.\u00a0"
  },
  {
    "start": 5413.12,
    "duration": 4.56,
    "text": "There's a lot of really smart people who are\u00a0\nready to make use of the resources and do\u00a0\u00a0"
  },
  {
    "start": 5417.68,
    "duration": 4.8,
    "text": "this period of catch-up because we've had this\u00a0\ndiscontinuity, and I think AI might be similar.\u00a0"
  },
  {
    "start": 5424.48,
    "duration": 3.6,
    "text": "I understand, but I still think that\u00a0\nyou're presupposing some discrete jump.\u00a0"
  },
  {
    "start": 5428.08,
    "duration": 3.36,
    "text": "There's some unlock that we're waiting to claim.\nAnd suddenly we're going to have\u00a0\u00a0"
  },
  {
    "start": 5431.44,
    "duration": 3.52,
    "text": "geniuses in data centers.\nI still think you're presupposing\u00a0\u00a0"
  },
  {
    "start": 5434.96,
    "duration": 4.64,
    "text": "some discrete jump that has no historical\u00a0\nprecedent that I can't find in any of the\u00a0\u00a0"
  },
  {
    "start": 5439.6,
    "duration": 4.08,
    "text": "statistics and that I think probably won't happen.\nI mean, the Industrial Revolution is such a jump.\u00a0"
  },
  {
    "start": 5443.68,
    "duration": 5.76,
    "text": "You went from 0.2% growth to 2% growth.\nI'm just saying you'll see another jump like that.\u00a0"
  },
  {
    "start": 5449.44,
    "duration": 4.32,
    "text": "I'm a little bit suspicious,\u00a0\nI would have to take a look.\u00a0"
  },
  {
    "start": 5453.76,
    "duration": 4.32,
    "text": "For example, some of the logs are not very\u00a0\ngood from before the Industrial Revolution.\u00a0"
  },
  {
    "start": 5459.92,
    "duration": 4.72,
    "text": "I'm a bit suspicious of it but\u00a0\nI don't have strong opinions.\u00a0"
  },
  {
    "start": 5464.64,
    "duration": 2.8,
    "text": "You're saying that this was a singular\u00a0\nevent that was extremely magical.\u00a0"
  },
  {
    "start": 5467.44,
    "duration": 1.84,
    "text": "You're saying that maybe there's going\u00a0\nto be another event that's going to\u00a0\u00a0"
  },
  {
    "start": 5469.28,
    "duration": 3.68,
    "text": "be just like that, extremely magical.\nIt will break the paradigm, and so on.\u00a0"
  },
  {
    "start": 5472.96,
    "duration": 4.16,
    "text": "I actually don't think\u2026 The crucial thing with the\u00a0\nIndustrial Revolution was that it was not magical.\u00a0"
  },
  {
    "start": 5478.48,
    "duration": 9.2,
    "text": "If you just zoomed in, what you would see in 1770\u00a0\nor 1870 is not that there was some key invention.\u00a0"
  },
  {
    "start": 5488.4,
    "duration": 4.56,
    "text": "But at the same time, you did move the\u00a0\neconomy to a regime where the progress\u00a0\u00a0"
  },
  {
    "start": 5492.96,
    "duration": 4.88,
    "text": "was much faster and the exponential 10x'd.\nI expect a similar thing from AI where it's\u00a0\u00a0"
  },
  {
    "start": 5497.84,
    "duration": 4.63,
    "text": "not like there's going to be a single moment\u00a0\nwhere we've made the crucial invention.\u00a0"
  },
  {
    "start": 5502.47,
    "duration": 3.29,
    "text": "It\u2019s an overhang that's being unlocked.\nLike maybe there's a new energy source.\u00a0"
  },
  {
    "start": 5505.76,
    "duration": 4.0,
    "text": "There's some unlock\u2014in this case, some kind of\u00a0\na cognitive capacity\u2014and there's an overhang of\u00a0\u00a0"
  },
  {
    "start": 5509.76,
    "duration": 2.4,
    "text": "cognitive work to do.\nThat's right.\u00a0"
  },
  {
    "start": 5512.16,
    "duration": 3.6,
    "text": "You're expecting that overhang to be filled by\u00a0\nthis new technology when it crosses the threshold.\u00a0"
  },
  {
    "start": 5516.64,
    "duration": 4.48,
    "text": "Maybe one way to think about it is\u00a0\nthroughout history, a lot of growth\u00a0\u00a0"
  },
  {
    "start": 5521.12,
    "duration": 5.28,
    "text": "comes because people come up with ideas,\u00a0\nand then people are out there doing stuff to\u00a0\u00a0"
  },
  {
    "start": 5526.4,
    "duration": 4.64,
    "text": "execute those ideas and make valuable output.\nThrough most of this time, the population has\u00a0\u00a0"
  },
  {
    "start": 5531.04,
    "duration": 3.12,
    "text": "been exploding. That has been driving\u00a0\ngrowth. For the last 50 years, people\u00a0\u00a0"
  },
  {
    "start": 5534.16,
    "duration": 3.36,
    "text": "have argued that growth has stagnated.\nThe population in frontier countries\u00a0\u00a0"
  },
  {
    "start": 5537.52,
    "duration": 2.32,
    "text": "has also stagnated.\nI think we go back to\u00a0\u00a0"
  },
  {
    "start": 5539.84,
    "duration": 7.2,
    "text": "the exponential growth in population that\u00a0\ncauses hyper-exponential growth in output.\u00a0"
  },
  {
    "start": 5548.96,
    "duration": 3.2,
    "text": "It's really hard to tell. I\u00a0\nunderstand that viewpoint. I\u00a0\u00a0"
  },
  {
    "start": 5552.16,
    "duration": 68.4,
    "text": "don't intuitively feel that viewpoint.\nYou recommended Nick Lane's book to me.\u00a0"
  },
  {
    "start": 5620.56,
    "duration": 3.84,
    "text": "On that basis, I also found it super\u00a0\ninteresting and I interviewed him.\u00a0"
  },
  {
    "start": 5625.2,
    "duration": 3.92,
    "text": "I have some questions about thinking about\u00a0\nintelligence and evolutionary history.\u00a0"
  },
  {
    "start": 5629.12,
    "duration": 5.28,
    "text": "Now that you, over the last 20 years of doing AI\u00a0\nresearch, you maybe have a more tangible sense of\u00a0\u00a0"
  },
  {
    "start": 5634.4,
    "duration": 7.28,
    "text": "what intelligence is, what it takes to develop it.\nAre you more or less surprised as a result that\u00a0\u00a0"
  },
  {
    "start": 5641.68,
    "duration": 10.48,
    "text": "evolution just spontaneously stumbled upon it?\nI love Nick Lane's books. I was just listening\u00a0\u00a0"
  },
  {
    "start": 5652.16,
    "duration": 3.12,
    "text": "to his podcast on the way up here.\nWith respect to intelligence and its\u00a0\u00a0"
  },
  {
    "start": 5655.28,
    "duration": 6.96,
    "text": "evolution, it's very, very recent.\nI am surprised that it evolved.\u00a0"
  },
  {
    "start": 5663.04,
    "duration": 2.08,
    "text": "I find it fascinating to think\u00a0\nabout all the worlds out there.\u00a0"
  },
  {
    "start": 5665.12,
    "duration": 2.32,
    "text": "Say there's a thousand planets\u00a0\nlike Earth and what they look like.\u00a0"
  },
  {
    "start": 5667.44,
    "duration": 2.16,
    "text": "I think Nick Lane was here talking\u00a0\nabout some of the earliest parts.\u00a0"
  },
  {
    "start": 5670.88,
    "duration": 3.2,
    "text": "He expects very similar life\u00a0\nforms, roughly speaking,\u00a0\u00a0"
  },
  {
    "start": 5674.08,
    "duration": 4.4,
    "text": "and bacteria-like things in most of them.\nThere are a few breaks in there.\u00a0"
  },
  {
    "start": 5679.92,
    "duration": 4.0,
    "text": "The evolution of intelligence intuitively feels\u00a0\nto me like it should be a fairly rare event.\u00a0"
  },
  {
    "start": 5685.92,
    "duration": 3.36,
    "text": "Maybe you should base it on\u00a0\nhow long something has existed.\u00a0"
  },
  {
    "start": 5689.28,
    "duration": 3.6,
    "text": "If bacteria were around for 2 billion years\u00a0\nand nothing happened, then going to eukaryote\u00a0\u00a0"
  },
  {
    "start": 5692.88,
    "duration": 7.12,
    "text": "is probably pretty hard because bacteria came\u00a0\nup quite early in Earth's evolution or history.\u00a0"
  },
  {
    "start": 5702.24,
    "duration": 2.48,
    "text": "How long have we had animals?\nMaybe a couple hundred million years,\u00a0\u00a0"
  },
  {
    "start": 5704.72,
    "duration": 3.6,
    "text": "multicellular animals that\u00a0\nrun around, crawl, et cetera.\u00a0"
  },
  {
    "start": 5708.32,
    "duration": 7.52,
    "text": "That\u2019s maybe 10% of Earth's lifespan.\nMaybe on that timescale it's not too tricky.\u00a0"
  },
  {
    "start": 5718.0,
    "duration": 2.48,
    "text": "It's still surprising to me,\u00a0\nintuitively, that it developed.\u00a0"
  },
  {
    "start": 5720.48,
    "duration": 4.16,
    "text": "I would maybe expect just a lot of animal-like\u00a0\nlife forms doing animal-like things.\u00a0"
  },
  {
    "start": 5724.64,
    "duration": 3.44,
    "text": "The fact that you can get something\u00a0\nthat creates culture and knowledge\u00a0\u00a0"
  },
  {
    "start": 5728.08,
    "duration": 7.36,
    "text": "and accumulates it is surprising to me.\nThere's a couple of interesting follow-ups.\u00a0"
  },
  {
    "start": 5735.44,
    "duration": 6.48,
    "text": "If you buy the Sutton perspective that the\u00a0\ncrux of intelligence is animal intelligence\u2026\u00a0\u00a0"
  },
  {
    "start": 5741.92,
    "duration": 3.2,
    "text": "The quote he said is \"If you got to the\u00a0\nsquirrel, you'd be most of the way to AGI.\"\u00a0"
  },
  {
    "start": 5746.56,
    "duration": 4.64,
    "text": "We got to squirrel intelligence right after\u00a0\nthe Cambrian explosion 600 million years ago.\u00a0"
  },
  {
    "start": 5751.2,
    "duration": 4.8,
    "text": "It seems like what instigated that was the\u00a0\noxygenation event 600 million years ago.\u00a0"
  },
  {
    "start": 5756.0,
    "duration": 5.6,
    "text": "But immediately the intelligence algorithm\u00a0\nwas there to make the squirrel intelligence.\u00a0"
  },
  {
    "start": 5762.48,
    "duration": 3.76,
    "text": "It's suggestive that animal\u00a0\nintelligence was like that.\u00a0"
  },
  {
    "start": 5766.24,
    "duration": 2.88,
    "text": "As soon as you had the oxygen in the\u00a0\nenvironment, you had the eukaryote,\u00a0\u00a0"
  },
  {
    "start": 5769.12,
    "duration": 5.28,
    "text": "you could just get the algorithm.\nMaybe it was an accident that\u00a0\u00a0"
  },
  {
    "start": 5774.4,
    "duration": 3.04,
    "text": "evolution stumbled upon it so fast,\u00a0\nbut I don't know if that suggests that\u00a0\u00a0"
  },
  {
    "start": 5778.48,
    "duration": 4.72,
    "text": "at the end it's going to be quite simple.\nIt's so hard to tell with any of this stuff.\u00a0"
  },
  {
    "start": 5783.2,
    "duration": 2.96,
    "text": "You can base it a bit on how long\u00a0\nsomething has existed or how long\u00a0\u00a0"
  },
  {
    "start": 5786.16,
    "duration": 4.08,
    "text": "it feels like something has been bottlenecked.\nNick Lane is very good about describing this very\u00a0\u00a0"
  },
  {
    "start": 5790.24,
    "duration": 4.0,
    "text": "apparent bottleneck in bacteria and archaea.\nFor two billion years, nothing happened.\u00a0"
  },
  {
    "start": 5794.24,
    "duration": 6.96,
    "text": "There\u2019s extreme diversity of biochemistry,\u00a0\nand yet nothing grows to become animals.\u00a0\u00a0"
  },
  {
    "start": 5801.2,
    "duration": 5.2,
    "text": "Two billion years. I don't know that we've\u00a0\nseen exactly that kind of an equivalent with\u00a0\u00a0"
  },
  {
    "start": 5806.4,
    "duration": 4.8,
    "text": "animals and intelligence, to your point.\nWe could also look at it with respect\u00a0\u00a0"
  },
  {
    "start": 5811.2,
    "duration": 4.4,
    "text": "to how many times we think certain\u00a0\nintelligence has individually sprung up.\u00a0"
  },
  {
    "start": 5815.6,
    "duration": 7.52,
    "text": "That's a really good thing to investigate.\nOne thought on that. There's hominid intelligence,\u00a0\u00a0"
  },
  {
    "start": 5823.12,
    "duration": 4.72,
    "text": "and then there's bird intelligence.\nRavens, etc., are extremely clever,\u00a0\u00a0"
  },
  {
    "start": 5827.84,
    "duration": 5.92,
    "text": "but their brain parts are quite distinct,\u00a0\nand we don't have that much in common.\u00a0"
  },
  {
    "start": 5833.76,
    "duration": 5.12,
    "text": "That's a slight indication of maybe\u00a0\nintelligence springing up a few times.\u00a0"
  },
  {
    "start": 5838.88,
    "duration": 7.68,
    "text": "In that case, you'd expect it more frequently.\nA former guest, Gwern, and Carl Shulman, they\u2019ve\u00a0\u00a0"
  },
  {
    "start": 5846.56,
    "duration": 6.16,
    "text": "made a really interesting point about that.\nTheir perspective is that the scalable algorithm\u00a0\u00a0"
  },
  {
    "start": 5852.72,
    "duration": 6.88,
    "text": "which humans have and primates have, arose in\u00a0\nbirds as well, and maybe other times as well.\u00a0"
  },
  {
    "start": 5859.6,
    "duration": 6.48,
    "text": "But humans found an evolutionary niche which\u00a0\nrewarded marginal increases in intelligence\u00a0\u00a0"
  },
  {
    "start": 5867.28,
    "duration": 5.04,
    "text": "and also had a scalable brain algorithm that\u00a0\ncould achieve those increases in intelligence.\u00a0"
  },
  {
    "start": 5873.44,
    "duration": 3.76,
    "text": "For example, if a bird had a bigger brain,\u00a0\nit would just collapse out of the air.\u00a0"
  },
  {
    "start": 5877.2,
    "duration": 3.36,
    "text": "It's very smart for the size of\u00a0\nits brain, but it's not in a niche\u00a0\u00a0"
  },
  {
    "start": 5880.56,
    "duration": 5.52,
    "text": "which rewards the brain getting bigger.\nIt\u2019s maybe similar to some really smart\u2026\u00a0"
  },
  {
    "start": 5888.88,
    "duration": 1.28,
    "text": "Like dolphins?\nExaclty, humans, we have hands that\u00a0\u00a0"
  },
  {
    "start": 5890.16,
    "duration": 4.16,
    "text": "reward being able to learn how to do tool use.\nWe can externalize digestion, more energy to\u00a0\u00a0"
  },
  {
    "start": 5894.32,
    "duration": 5.12,
    "text": "the brain, and that kicks off the flywheel.\nAlso stuff to work with. I'm guessing it would\u00a0\u00a0"
  },
  {
    "start": 5899.44,
    "duration": 8.64,
    "text": "be harder if I were a dolphin. How do you have\u00a0\nfire? The universe of things you can do in water,\u00a0\u00a0"
  },
  {
    "start": 5908.88,
    "duration": 4.08,
    "text": "inside water, is probably lower than\u00a0\nwhat you can do on land, just chemically.\u00a0"
  },
  {
    "start": 5913.84,
    "duration": 4.8,
    "text": "I do agree with this viewpoint of these niches\u00a0\nand what's being incentivized. I still find it\u00a0\u00a0"
  },
  {
    "start": 5918.64,
    "duration": 7.28,
    "text": "miraculous. I would have expected things to\u00a0\nget stuck on animals with bigger muscles.\u00a0"
  },
  {
    "start": 5927.04,
    "duration": 4.48,
    "text": "Going through intelligence is a\u00a0\nreally fascinating breaking point.\u00a0"
  },
  {
    "start": 5931.52,
    "duration": 4.64,
    "text": "The way Gwern put it is the reason it was so hard\u00a0\nis that it's a very tight line between being in\u00a0\u00a0"
  },
  {
    "start": 5936.16,
    "duration": 7.04,
    "text": "a situation where something is so important\u00a0\nto learn that it's not worth distilling the\u00a0\u00a0"
  },
  {
    "start": 5943.2,
    "duration": 6.8,
    "text": "exact right circuits directly back into your DNA,\u00a0\nversus it's not important enough to learn at all.\u00a0"
  },
  {
    "start": 5950.0,
    "duration": 7.2,
    "text": "It has to be something that incentivizes\u00a0\nbuilding the algorithm to learn in a lifetime.\u00a0"
  },
  {
    "start": 5957.2,
    "duration": 4.64,
    "text": "You have to incentivize some kind of adaptability.\nYou want environments that are unpredictable\u00a0\u00a0"
  },
  {
    "start": 5961.84,
    "duration": 2.8,
    "text": "so evolution can't bake your\u00a0\nalgorithms into your weights.\u00a0"
  },
  {
    "start": 5964.64,
    "duration": 5.36,
    "text": "A lot of animals are pre-baked in this sense.\nHumans have to figure it out at test\u00a0\u00a0"
  },
  {
    "start": 5970.0,
    "duration": 5.36,
    "text": "time when they get born.\nYou want these environments\u00a0\u00a0"
  },
  {
    "start": 5975.36,
    "duration": 4.72,
    "text": "that change really rapidly, where you\u00a0\ncan't foresee what will work well.\u00a0"
  },
  {
    "start": 5982.0,
    "duration": 2.08,
    "text": "You create intelligence to\u00a0\nfigure it out at test time.\u00a0"
  },
  {
    "start": 5985.2,
    "duration": 3.2,
    "text": "Quintin Pope had this interesting blog post\u00a0\nwhere he's saying the reason he doesn't\u00a0\u00a0"
  },
  {
    "start": 5988.4,
    "duration": 6.88,
    "text": "expect a sharp takeoff is that humans had the\u00a0\nsharp takeoff where 60,000 years ago we seem\u00a0\u00a0"
  },
  {
    "start": 5995.28,
    "duration": 4.48,
    "text": "to have had the cognitive architectures\u00a0\nthat we have today. 10,000 years ago,\u00a0\u00a0"
  },
  {
    "start": 5999.76,
    "duration": 4.8,
    "text": "agricultural revolution, modernity.\nWhat was happening in that 50,000 years?\u00a0"
  },
  {
    "start": 6004.56,
    "duration": 6.4,
    "text": "You had to build this cultural scaffold where\u00a0\nyou can accumulate knowledge over generations.\u00a0"
  },
  {
    "start": 6011.52,
    "duration": 4.866,
    "text": "This is an ability that exists for\u00a0\nfree in the way we do AI training.\u00a0"
  },
  {
    "start": 6016.386,
    "duration": 5.134,
    "text": "In many cases they are literally distilled.\nIf you retrain a model, they can be trained\u00a0\u00a0"
  },
  {
    "start": 6021.52,
    "duration": 3.28,
    "text": "on each other, they can be trained\u00a0\non the same pre-training corpus,\u00a0\u00a0"
  },
  {
    "start": 6025.36,
    "duration": 5.92,
    "text": "they don't literally have to start from scratch.\nThere's a sense in which it took humans a long\u00a0\u00a0"
  },
  {
    "start": 6031.28,
    "duration": 4.24,
    "text": "time to get this cultural loop going, but it just\u00a0\ncomes for free with the way we do LLM training.\u00a0"
  },
  {
    "start": 6036.24,
    "duration": 3.2,
    "text": "Yes and no. Because LLMs don't really\u00a0\nhave the equivalent of culture.\u00a0"
  },
  {
    "start": 6039.44,
    "duration": 2.56,
    "text": "Maybe we're giving them way too\u00a0\nmuch and incentivizing not to\u00a0\u00a0"
  },
  {
    "start": 6042.0,
    "duration": 3.12,
    "text": "create it or something like that.\nBut the invention of culture and of\u00a0\u00a0"
  },
  {
    "start": 6045.12,
    "duration": 3.68,
    "text": "written record and of passing down notes\u00a0\nbetween each other, I don't think there's\u00a0\u00a0"
  },
  {
    "start": 6048.8,
    "duration": 4.4,
    "text": "an equivalent of that with LLMs right now.\nLLMs don't really have culture right now and\u00a0\u00a0"
  },
  {
    "start": 6053.2,
    "duration": 5.2,
    "text": "it's one of the impediments I would say.\nCan you give me some sense of what\u00a0\u00a0"
  },
  {
    "start": 6058.4,
    "duration": 3.44,
    "text": "LLM culture might look like?\nIn the simplest case it would be a\u00a0\u00a0"
  },
  {
    "start": 6061.84,
    "duration": 5.04,
    "text": "giant scratchpad that the LLM can edit and as it's\u00a0\nreading stuff or as it's helping out with work,\u00a0\u00a0"
  },
  {
    "start": 6066.88,
    "duration": 3.68,
    "text": "it's editing the scratchpad for itself.\nWhy can't an LLM write a book for the other\u00a0\u00a0"
  },
  {
    "start": 6070.56,
    "duration": 5.68,
    "text": "LLMs? That would be cool. Why can't other\u00a0\nLLMs read this LLM's book and be inspired\u00a0\u00a0"
  },
  {
    "start": 6076.24,
    "duration": 4.16,
    "text": "by it or shocked by it or something like that?\nThere's no equivalence for any of this stuff.\u00a0"
  },
  {
    "start": 6080.4,
    "duration": 2.88,
    "text": "Interesting. When would you expect\u00a0\nthat kind of thing to start happening?\u00a0"
  },
  {
    "start": 6084.64,
    "duration": 6.48,
    "text": "Also, multi-agent systems and a sort of\u00a0\nindependent AI civilization and culture?\u00a0"
  },
  {
    "start": 6091.12,
    "duration": 3.28,
    "text": "There are two powerful ideas in the\u00a0\nrealm of multi-agent that have both\u00a0\u00a0"
  },
  {
    "start": 6094.4,
    "duration": 4.24,
    "text": "not been really claimed or so on.\nThe first one I would say is culture\u00a0\u00a0"
  },
  {
    "start": 6098.64,
    "duration": 5.68,
    "text": "and LLMs having a growing repertoire\u00a0\nof knowledge for their own purposes.\u00a0"
  },
  {
    "start": 6104.32,
    "duration": 3.2,
    "text": "The second one looks a lot more\u00a0\nlike the powerful idea of self-play.\u00a0"
  },
  {
    "start": 6107.52,
    "duration": 5.52,
    "text": "In my mind it\u2019s extremely powerful.\nEvolution has a lot of competition\u00a0\u00a0"
  },
  {
    "start": 6113.04,
    "duration": 6.56,
    "text": "driving intelligence and evolution.\nIn AlphaGo more algorithmically,\u00a0\u00a0"
  },
  {
    "start": 6119.6,
    "duration": 4.16,
    "text": "AlphaGo is playing against itself and that's\u00a0\nhow it learns to get really good at Go.\u00a0"
  },
  {
    "start": 6123.76,
    "duration": 3.76,
    "text": "There's no equivalent of self-playing LLMs,\u00a0\nbut I would expect that to also exist.\u00a0"
  },
  {
    "start": 6127.52,
    "duration": 3.12,
    "text": "No one has done it yet.\nWhy can't an LLM for example, create a bunch\u00a0\u00a0"
  },
  {
    "start": 6130.64,
    "duration": 5.68,
    "text": "of problems that another LLM is learning to solve?\nThen the LLM is always trying to serve more and\u00a0\u00a0"
  },
  {
    "start": 6136.32,
    "duration": 5.2,
    "text": "more difficult problems, stuff like that.\nThere's a bunch of ways to organize it.\u00a0"
  },
  {
    "start": 6142.16,
    "duration": 4.4,
    "text": "It's a realm of research, but I haven't\u00a0\nseen anything that convincingly claims\u00a0\u00a0"
  },
  {
    "start": 6146.56,
    "duration": 5.04,
    "text": "both of those multi-agent improvements.\nWe're mostly in the realm of a single\u00a0\u00a0"
  },
  {
    "start": 6151.6,
    "duration": 5.68,
    "text": "individual agent, but that will change.\nIn the realm of culture also,\u00a0\u00a0"
  },
  {
    "start": 6157.28,
    "duration": 3.92,
    "text": "I would also bucket organizations.\nWe haven't seen anything like that convincingly\u00a0\u00a0"
  },
  {
    "start": 6161.2,
    "duration": 4.64,
    "text": "either. That's why we're still early.\nCan you identify the key bottleneck\u00a0\u00a0"
  },
  {
    "start": 6165.84,
    "duration": 4.32,
    "text": "that's preventing this kind\u00a0\nof collaboration between LLMs?\u00a0"
  },
  {
    "start": 6170.16,
    "duration": 5.92,
    "text": "Maybe the way I would put it is,\u00a0\nsome of these analogies work and\u00a0\u00a0"
  },
  {
    "start": 6176.08,
    "duration": 5.04,
    "text": "they shouldn't, but somehow, remarkably, they do.\nA lot of the smaller models, or the dumber models,\u00a0\u00a0"
  },
  {
    "start": 6181.12,
    "duration": 5.6,
    "text": "remarkably resemble a kindergarten student, or an\u00a0\nelementary school student or high school student.\u00a0"
  },
  {
    "start": 6187.52,
    "duration": 3.2,
    "text": "Somehow, we still haven't graduated\u00a0\nenough where this stuff can take over.\u00a0"
  },
  {
    "start": 6192.0,
    "duration": 5.04,
    "text": "My Claude Code or Codex, they still\u00a0\nfeel like this elementary-grade student.\u00a0"
  },
  {
    "start": 6197.04,
    "duration": 4.48,
    "text": "I know that they can take PhD quizzes,\u00a0\nbut they still cognitively feel like a\u00a0\u00a0"
  },
  {
    "start": 6201.52,
    "duration": 3.44,
    "text": "kindergarten or an elementary school student.\nI don't think they can create culture because\u00a0\u00a0"
  },
  {
    "start": 6204.96,
    "duration": 7.36,
    "text": "they're still kids. They're savant kids.\u00a0\nThey have perfect memory of all this stuff.\u00a0"
  },
  {
    "start": 6213.2,
    "duration": 3.52,
    "text": "They can convincingly create all\u00a0\nkinds of slop that looks really good.\u00a0"
  },
  {
    "start": 6216.72,
    "duration": 2.08,
    "text": "But I still think they don't really know\u00a0\nwhat they're doing and they don't really\u00a0\u00a0"
  },
  {
    "start": 6218.8,
    "duration": 4.24,
    "text": "have the cognition across all these little\u00a0\ncheckboxes that we still have to collect.\u00a0"
  },
  {
    "start": 6223.76,
    "duration": 6.4,
    "text": "You've talked about how you were at Tesla\u00a0\nleading self-driving from 2017 to 2022.\u00a0"
  },
  {
    "start": 6230.16,
    "duration": 8.24,
    "text": "And you firsthand saw this progress from cool\u00a0\ndemos to now thousands of cars out there actually\u00a0\u00a0"
  },
  {
    "start": 6238.4,
    "duration": 2.88,
    "text": "autonomously doing drives.\nWhy did that take a decade?\u00a0"
  },
  {
    "start": 6241.28,
    "duration": 5.28,
    "text": "What was happening through that time?\nOne thing I will almost instantly push\u00a0\u00a0"
  },
  {
    "start": 6246.56,
    "duration": 5.6,
    "text": "back on is that this is not even near done,\u00a0\nin a bunch of ways that I'm going to get to.\u00a0"
  },
  {
    "start": 6253.28,
    "duration": 3.52,
    "text": "Self-driving is very interesting because\u00a0\nit's definitely where I get a lot of my\u00a0\u00a0"
  },
  {
    "start": 6256.8,
    "duration": 5.68,
    "text": "intuitions because I spent five years on it.\nIt has this entire history where the first demos\u00a0\u00a0"
  },
  {
    "start": 6262.48,
    "duration": 5.68,
    "text": "of self-driving go all the way to the 1980s.\nYou can see a demo from CMU in 1986.\u00a0"
  },
  {
    "start": 6268.16,
    "duration": 6.0,
    "text": "There's a truck that's driving itself on\u00a0\nroads. Fast forward. When I was joining Tesla,\u00a0\u00a0"
  },
  {
    "start": 6274.16,
    "duration": 6.0,
    "text": "I had a very early demo of Waymo.\nIt basically gave me a perfect drive\u00a0\u00a0"
  },
  {
    "start": 6280.16,
    "duration": 6.08,
    "text": "in 2014 or something like that, so\u00a0\na perfect Waymo drive a decade ago.\u00a0"
  },
  {
    "start": 6286.24,
    "duration": 3.36,
    "text": "It took us around Palo Alto and so on\u00a0\nbecause I had a friend who worked there.\u00a0"
  },
  {
    "start": 6290.64,
    "duration": 2.8,
    "text": "I thought it was very close and\u00a0\nthen it still took a long time.\u00a0"
  },
  {
    "start": 6294.8,
    "duration": 7.28,
    "text": "For some kinds of tasks and jobs and so on,\u00a0\nthere's a very large demo-to-product gap where the\u00a0\u00a0"
  },
  {
    "start": 6302.08,
    "duration": 5.04,
    "text": "demo is very easy, but the product is very hard.\nIt's especially the case in cases like\u00a0\u00a0"
  },
  {
    "start": 6307.12,
    "duration": 3.68,
    "text": "self-driving where the cost\u00a0\nof failure is too high.\u00a0"
  },
  {
    "start": 6311.76,
    "duration": 4.08,
    "text": "Many industries, tasks, and jobs maybe don't have\u00a0\nthat property, but when you do have that property,\u00a0\u00a0"
  },
  {
    "start": 6315.84,
    "duration": 4.0,
    "text": "that definitely increases the timelines.\nFor example, in software engineering,\u00a0\u00a0"
  },
  {
    "start": 6319.84,
    "duration": 4.72,
    "text": "I do think that property does exist.\nFor a lot of vibe coding, it doesn't.\u00a0"
  },
  {
    "start": 6324.56,
    "duration": 4.08,
    "text": "But if you're writing actual production-grade\u00a0\ncode, that property should exist, because any\u00a0\u00a0"
  },
  {
    "start": 6328.64,
    "duration": 3.44,
    "text": "kind of mistake leads to a security\u00a0\nvulnerability or something like that.\u00a0"
  },
  {
    "start": 6332.08,
    "duration": 4.08,
    "text": "Millions and hundreds of millions of\u00a0\npeople's personal Social Security numbers\u00a0\u00a0"
  },
  {
    "start": 6336.16,
    "duration": 5.44,
    "text": "get leaked or something like that.\nSo in software, people should be careful,\u00a0\u00a0"
  },
  {
    "start": 6342.16,
    "duration": 3.84,
    "text": "kind of like in self-driving.\nIn self-driving, if things go wrong,\u00a0\u00a0"
  },
  {
    "start": 6346.0,
    "duration": 6.96,
    "text": "you might get injured. There are worse\u00a0\noutcomes. But in software, it's almost\u00a0\u00a0"
  },
  {
    "start": 6352.96,
    "duration": 5.68,
    "text": "unbounded how terrible something could be.\nI do think that they share that property.\u00a0"
  },
  {
    "start": 6359.92,
    "duration": 4.96,
    "text": "What takes the long amount of time and the way\u00a0\nto think about it is that it's a march of nines.\u00a0"
  },
  {
    "start": 6364.88,
    "duration": 5.44,
    "text": "Every single nine is a constant amount of work.\nEvery single nine is the same amount of work.\u00a0"
  },
  {
    "start": 6370.32,
    "duration": 6.4,
    "text": "When you get a demo and something works 90%\u00a0\nof the time, that's just the first nine.\u00a0"
  },
  {
    "start": 6376.72,
    "duration": 2.16,
    "text": "Then you need the second nine, a third\u00a0\nnine, a fourth nine, a fifth nine.\u00a0"
  },
  {
    "start": 6378.88,
    "duration": 4.16,
    "text": "While I was at Tesla for five years or so, we\u00a0\nwent through maybe three nines or two nines.\u00a0"
  },
  {
    "start": 6383.04,
    "duration": 2.56,
    "text": "I don't know what it is, but\u00a0\nmultiple nines of iteration.\u00a0"
  },
  {
    "start": 6385.6,
    "duration": 3.92,
    "text": "There are still more nines to go.\nThat's why these things take so long.\u00a0"
  },
  {
    "start": 6391.52,
    "duration": 3.84,
    "text": "It's definitely formative for me, seeing\u00a0\nsomething that was a demo. I'm very\u00a0\u00a0"
  },
  {
    "start": 6395.36,
    "duration": 5.52,
    "text": "unimpressed by demos. Whenever I see demos of\u00a0\nanything, I'm extremely unimpressed by that.\u00a0"
  },
  {
    "start": 6403.44,
    "duration": 2.48,
    "text": "If it's a demo that someone cooked\u00a0\nup as a showing, it's worse.\u00a0"
  },
  {
    "start": 6405.92,
    "duration": 1.68,
    "text": "If you can interact with it, it's a bit better.\u00a0"
  },
  {
    "start": 6407.6,
    "duration": 3.36,
    "text": "But even then, you're not done. You need the\u00a0\nactual product. It's going to face all these\u00a0\u00a0"
  },
  {
    "start": 6410.96,
    "duration": 2.8,
    "text": "challenges when it comes in contact\u00a0\nwith reality and all these different\u00a0\u00a0"
  },
  {
    "start": 6413.76,
    "duration": 3.68,
    "text": "pockets of behavior that need patching.\nWe're going to see all this stuff play\u00a0\u00a0"
  },
  {
    "start": 6417.44,
    "duration": 4.4,
    "text": "out. It's a march of nines. Each nine is\u00a0\nconstant. Demos are encouraging. It\u2019s still\u00a0\u00a0"
  },
  {
    "start": 6421.84,
    "duration": 5.36,
    "text": "a huge amount of work to do.\nIt is a critical safety domain,\u00a0\u00a0"
  },
  {
    "start": 6427.2,
    "duration": 3.68,
    "text": "unless you're doing vibe coding,\u00a0\nwhich is all nice and fun and so on.\u00a0"
  },
  {
    "start": 6431.84,
    "duration": 2.72,
    "text": "That's why this also enforced my\u00a0\ntimelines from that perspective.\u00a0"
  },
  {
    "start": 6436.0,
    "duration": 4.56,
    "text": "It's very interesting to hear you say that, that\u00a0\nthe safety guarantees you need from software\u00a0\u00a0"
  },
  {
    "start": 6440.56,
    "duration": 4.0,
    "text": "are not dissimilar to self-driving.\nWhat people will often say is that\u00a0\u00a0"
  },
  {
    "start": 6444.56,
    "duration": 5.6,
    "text": "self-driving took so long because\u00a0\nthe cost of failure is so high.\u00a0"
  },
  {
    "start": 6450.16,
    "duration": 4.16,
    "text": "A human makes a mistake on average every\u00a0\n400,000 miles or every seven years.\u00a0"
  },
  {
    "start": 6454.32,
    "duration": 5.04,
    "text": "If you had to release a coding agent that\u00a0\ncouldn't make a mistake for at least seven years,\u00a0\u00a0"
  },
  {
    "start": 6459.36,
    "duration": 4.08,
    "text": "it would be much harder to deploy.\nBut your point is that if you made a\u00a0\u00a0"
  },
  {
    "start": 6463.44,
    "duration": 4.4,
    "text": "catastrophic coding mistake, like breaking\u00a0\nsome important system every seven years...\u00a0"
  },
  {
    "start": 6467.84,
    "duration": 3.36,
    "text": "Very easy to do.\nIn fact, in terms of wall clock time,\u00a0\u00a0"
  },
  {
    "start": 6471.2,
    "duration": 3.36,
    "text": "it would be much less than seven years because\u00a0\nyou're constantly outputting code like that.\u00a0"
  },
  {
    "start": 6477.28,
    "duration": 2.8,
    "text": "In terms of tokens, it would be seven years.\nBut in terms of wall clock time...\u00a0"
  },
  {
    "start": 6480.08,
    "duration": 2.8,
    "text": "In some ways, it's a much harder problem.\nSelf-driving is just one of\u00a0\u00a0"
  },
  {
    "start": 6482.88,
    "duration": 4.24,
    "text": "thousands of things that people do.\nIt's almost like a single vertical, I suppose.\u00a0"
  },
  {
    "start": 6487.12,
    "duration": 1.84,
    "text": "Whereas when we're talking about\u00a0\ngeneral software engineering,\u00a0\u00a0"
  },
  {
    "start": 6488.96,
    "duration": 5.76,
    "text": "it's even more... There's more surface area.\nThere's another objection people make to that\u00a0\u00a0"
  },
  {
    "start": 6494.72,
    "duration": 6.4,
    "text": "analogy, which is that with self-driving, what\u00a0\ntook a big fraction of that time was solving\u00a0\u00a0"
  },
  {
    "start": 6501.12,
    "duration": 5.92,
    "text": "the problem of having basic perception\u00a0\nthat's robust, building representations,\u00a0\u00a0"
  },
  {
    "start": 6507.04,
    "duration": 5.04,
    "text": "and having a model that has some common\u00a0\nsense so it can generalize to when it sees\u00a0\u00a0"
  },
  {
    "start": 6512.08,
    "duration": 4.96,
    "text": "something that's slightly out of distribution.\nIf somebody's waving down the road this way,\u00a0\u00a0"
  },
  {
    "start": 6517.04,
    "duration": 3.6,
    "text": "you don't need to train for it.\nThe thing will have some understanding\u00a0\u00a0"
  },
  {
    "start": 6520.64,
    "duration": 3.76,
    "text": "of how to respond to something like that.\nThese are things we're getting for free\u00a0\u00a0"
  },
  {
    "start": 6524.4,
    "duration": 5.28,
    "text": "with LLMs or VLMs today, so we don't have to\u00a0\nsolve these very basic representation problems.\u00a0"
  },
  {
    "start": 6529.68,
    "duration": 4.64,
    "text": "So now deploying AIs across different domains\u00a0\nwill sort of be like deploying a self-driving\u00a0\u00a0"
  },
  {
    "start": 6534.32,
    "duration": 4.24,
    "text": "car with current models to a different city,\u00a0\nwhich is hard but not like a 10-year-long task.\u00a0"
  },
  {
    "start": 6539.2,
    "duration": 3.92,
    "text": "I'm not 100% sure if I fully agree with that.\nI don't know how much we're getting for free.\u00a0"
  },
  {
    "start": 6543.84,
    "duration": 2.72,
    "text": "There's still a lot of gaps in\u00a0\nunderstanding what we are getting.\u00a0"
  },
  {
    "start": 6547.68,
    "duration": 2.56,
    "text": "We're definitely getting more\u00a0\ngeneralizable intelligence in a\u00a0\u00a0"
  },
  {
    "start": 6550.24,
    "duration": 4.24,
    "text": "single entity, whereas self-driving is a\u00a0\nvery special-purpose task that requires.\u00a0"
  },
  {
    "start": 6554.48,
    "duration": 4.08,
    "text": "In some sense building a special-purpose task\u00a0\nis maybe even harder in a certain sense because\u00a0\u00a0"
  },
  {
    "start": 6558.56,
    "duration": 3.68,
    "text": "it doesn't fall out from a more general thing\u00a0\nthat you're doing at scale, if that makes sense.\u00a0"
  },
  {
    "start": 6564.24,
    "duration": 6.16,
    "text": "But the analogy still doesn't fully\u00a0\nresonate because the LLMs are still\u00a0\u00a0"
  },
  {
    "start": 6570.4,
    "duration": 3.12,
    "text": "pretty fallible and they have a lot of\u00a0\ngaps that still need to be filled in.\u00a0"
  },
  {
    "start": 6573.52,
    "duration": 2.16,
    "text": "I don't think that we're\u00a0\ngetting magical generalization\u00a0\u00a0"
  },
  {
    "start": 6575.68,
    "duration": 6.64,
    "text": "completely out of the box, in a certain sense.\nThe other aspect that I wanted to return to is\u00a0\u00a0"
  },
  {
    "start": 6582.96,
    "duration": 7.52,
    "text": "that self-driving cars are nowhere near done\u00a0\nstill. The deployments are pretty minimal.\u00a0\u00a0"
  },
  {
    "start": 6591.36,
    "duration": 3.12,
    "text": "Even Waymo and so on has very few cars.\nThey're doing that roughly speaking\u00a0\u00a0"
  },
  {
    "start": 6594.48,
    "duration": 4.96,
    "text": "because they're not economical.\nThey've built something that lives in the future.\u00a0"
  },
  {
    "start": 6600.32,
    "duration": 3.44,
    "text": "They've had to pull back the future,\u00a0\nbut they had to make it uneconomical.\u00a0"
  },
  {
    "start": 6605.84,
    "duration": 3.44,
    "text": "There are all these costs, not just\u00a0\nmarginal costs for those cars and\u00a0\u00a0"
  },
  {
    "start": 6609.28,
    "duration": 3.12,
    "text": "their operation and maintenance, but\u00a0\nalso the capex of the entire thing.\u00a0"
  },
  {
    "start": 6613.12,
    "duration": 3.04,
    "text": "Making it economical is still\u00a0\ngoing to be a slog for them.\u00a0"
  },
  {
    "start": 6617.92,
    "duration": 3.6,
    "text": "Also, when you look at these cars and\u00a0\nthere's no one driving, I actually think\u00a0\u00a0"
  },
  {
    "start": 6621.52,
    "duration": 4.96,
    "text": "it's a little bit deceiving because there\u00a0\nare very elaborate teleoperation centers\u00a0\u00a0"
  },
  {
    "start": 6626.48,
    "duration": 6.16,
    "text": "of people kind of in a loop with these cars.\nI don't have the full extent of it, but there's\u00a0\u00a0"
  },
  {
    "start": 6632.64,
    "duration": 3.52,
    "text": "more human-in-the-loop than you might expect.\nThere are people somewhere out there\u00a0\u00a0"
  },
  {
    "start": 6636.8,
    "duration": 3.12,
    "text": "beaming in from the sky.\nI don't know if they're\u00a0\u00a0"
  },
  {
    "start": 6639.92,
    "duration": 2.88,
    "text": "fully in the loop with the driving.\nSome of the time they are, but they're\u00a0\u00a0"
  },
  {
    "start": 6642.8,
    "duration": 2.8,
    "text": "certainly involved and there are people.\nIn some sense, we haven't actually removed\u00a0\u00a0"
  },
  {
    "start": 6645.6,
    "duration": 2.96,
    "text": "the person, we've moved them to\u00a0\nsomewhere where you can't see them.\u00a0"
  },
  {
    "start": 6648.56,
    "duration": 3.12,
    "text": "I still think there will be some work, as you\u00a0\nmentioned, going from environment to environment.\u00a0"
  },
  {
    "start": 6652.8,
    "duration": 2.88,
    "text": "There are still challenges\u00a0\nto make self-driving real.\u00a0"
  },
  {
    "start": 6655.68,
    "duration": 3.92,
    "text": "But I do agree that it's definitely crossed\u00a0\na threshold where it kind of feels real,\u00a0\u00a0"
  },
  {
    "start": 6659.6,
    "duration": 3.68,
    "text": "unless it's really teleoperated.\nFor example, Waymo can't go to\u00a0\u00a0"
  },
  {
    "start": 6663.28,
    "duration": 3.84,
    "text": "all the different parts of the city.\nMy suspicion is that it's parts of the city\u00a0\u00a0"
  },
  {
    "start": 6667.12,
    "duration": 4.72,
    "text": "where you don't get good signal.\nAnyway, I don't know anything\u00a0\u00a0"
  },
  {
    "start": 6671.84,
    "duration": 6.08,
    "text": "about the stack. I'm just making stuff up.\nYou led self-driving for five years at Tesla.\u00a0"
  },
  {
    "start": 6677.92,
    "duration": 1.68,
    "text": "Sorry, I don't know anything\u00a0\nabout the specifics of Waymo.\u00a0"
  },
  {
    "start": 6681.12,
    "duration": 1.76,
    "text": "By the way, I love Waymo\u00a0\nand I take it all the time.\u00a0"
  },
  {
    "start": 6684.88,
    "duration": 4.24,
    "text": "I just think that people are sometimes a\u00a0\nlittle bit too naive about some of the progress\u00a0\u00a0"
  },
  {
    "start": 6689.12,
    "duration": 4.32,
    "text": "and there's still a huge amount of work.\nTesla took in my mind a much more scalable\u00a0\u00a0"
  },
  {
    "start": 6693.44,
    "duration": 6.0,
    "text": "approach and the team is doing extremely well.\nI'm kind of on the record for predicting\u00a0\u00a0"
  },
  {
    "start": 6699.44,
    "duration": 2.96,
    "text": "how this thing will go.\nWaymo had an early start\u00a0\u00a0"
  },
  {
    "start": 6702.4,
    "duration": 3.52,
    "text": "because you can package up so many sensors.\nBut I do think Tesla is taking the more\u00a0\u00a0"
  },
  {
    "start": 6705.92,
    "duration": 2.8,
    "text": "scalable strategy and it's going\u00a0\nto look a lot more like that.\u00a0"
  },
  {
    "start": 6708.72,
    "duration": 5.52,
    "text": "So this will still have to play out and hasn't.\nBut I don't want to talk about self-driving as\u00a0\u00a0"
  },
  {
    "start": 6714.24,
    "duration": 5.6,
    "text": "something that took a decade because it\u00a0\ndidn't take it yet, if that makes sense.\u00a0"
  },
  {
    "start": 6719.84,
    "duration": 5.28,
    "text": "Because one, the start is at 1980 and not 10\u00a0\nyears ago, and then two, the end is not here yet.\u00a0"
  },
  {
    "start": 6725.12,
    "duration": 3.28,
    "text": "The end is not near yet because when\u00a0\nwe're talking about self-driving,\u00a0\u00a0"
  },
  {
    "start": 6728.4,
    "duration": 5.36,
    "text": "usually in my mind it's self-driving at scale.\nPeople don't have to get a driver's license, etc.\u00a0"
  },
  {
    "start": 6733.76,
    "duration": 5.36,
    "text": "I'm curious to bounce two other ways in\u00a0\nwhich the analogy might be different.\u00a0"
  },
  {
    "start": 6739.12,
    "duration": 5.36,
    "text": "The reason I'm especially curious about this is\u00a0\nbecause the question of how fast AI is deployed,\u00a0\u00a0"
  },
  {
    "start": 6744.48,
    "duration": 4.24,
    "text": "how valuable it is when it's early\u00a0\non is potentially the most important\u00a0\u00a0"
  },
  {
    "start": 6748.72,
    "duration": 2.4,
    "text": "question in the world right now.\nIf you're trying to model what the\u00a0\u00a0"
  },
  {
    "start": 6751.12,
    "duration": 4.08,
    "text": "year 2030 looks like, this is the question\u00a0\nyou ought to have some understanding of.\u00a0"
  },
  {
    "start": 6756.16,
    "duration": 5.36,
    "text": "Another thing you might think is one, you have\u00a0\nthis latency requirement with self-driving.\u00a0"
  },
  {
    "start": 6762.24,
    "duration": 3.2,
    "text": "I have no idea what the actual models are, but I\u00a0\nassume it\u2019s like tens of millions of parameters\u00a0\u00a0"
  },
  {
    "start": 6765.44,
    "duration": 5.76,
    "text": "or something, which is not the necessary\u00a0\nconstraint for knowledge work with LLMs.\u00a0"
  },
  {
    "start": 6771.2,
    "duration": 5.68,
    "text": "Maybe it might be with computer use and stuff.\nBut the other big one is, maybe more\u00a0\u00a0"
  },
  {
    "start": 6776.88,
    "duration": 5.92,
    "text": "importantly, on this capex question.\nYes, there is additional cost to serving\u00a0\u00a0"
  },
  {
    "start": 6782.8,
    "duration": 9.2,
    "text": "up an additional copy of a model, but the opex\u00a0\nof a session is quite low and you can amortize\u00a0\u00a0"
  },
  {
    "start": 6792.0,
    "duration": 5.28,
    "text": "the cost of AI into the training run itself,\u00a0\ndepending on how inference scaling goes and stuff.\u00a0"
  },
  {
    "start": 6797.28,
    "duration": 6.48,
    "text": "But it's certainly not as much as building a whole\u00a0\nnew car to serve another instance of a model.\u00a0"
  },
  {
    "start": 6803.76,
    "duration": 4.8,
    "text": "So the economics of deploying more\u00a0\nwidely are much more favorable.\u00a0"
  },
  {
    "start": 6808.56,
    "duration": 2.56,
    "text": "I think that's right. If you're\u00a0\nsticking to the realm of bits,\u00a0\u00a0"
  },
  {
    "start": 6811.12,
    "duration": 5.52,
    "text": "bits are a million times easier than anything\u00a0\nthat touches the physical world. I definitely\u00a0\u00a0"
  },
  {
    "start": 6816.64,
    "duration": 6.24,
    "text": "grant that. Bits are completely changeable,\u00a0\narbitrarily reshuffleable at a very rapid speed.\u00a0"
  },
  {
    "start": 6822.88,
    "duration": 7.44,
    "text": "You would expect a much faster adaptation also in\u00a0\nthe industry and so on. What was the first one?\u00a0"
  },
  {
    "start": 6830.32,
    "duration": 3.2,
    "text": "The latency requirements and\u00a0\nits implications for model size?\u00a0"
  },
  {
    "start": 6833.52,
    "duration": 2.56,
    "text": "I think that's roughly right. I also\u00a0\nthink that if we are talking about\u00a0\u00a0"
  },
  {
    "start": 6836.08,
    "duration": 4.0,
    "text": "knowledge work at scale, there will be some\u00a0\nlatency requirements, practically speaking,\u00a0\u00a0"
  },
  {
    "start": 6840.08,
    "duration": 6.08,
    "text": "because we're going to have to create a\u00a0\nhuge amount of compute and serve that.\u00a0"
  },
  {
    "start": 6846.16,
    "duration": 3.92,
    "text": "The last aspect that I very briefly want\u00a0\nto also talk about is all the rest of it.\u00a0"
  },
  {
    "start": 6853.28,
    "duration": 4.24,
    "text": "What does society think about it? What are\u00a0\nthe legal ramifications? How is it working\u00a0\u00a0"
  },
  {
    "start": 6857.52,
    "duration": 6.56,
    "text": "legally? How is it working insurance-wise?\u00a0\nWhat are those layers of it and aspects of it?\u00a0"
  },
  {
    "start": 6865.04,
    "duration": 2.56,
    "text": "What is the equivalent of people\u00a0\nputting a cone on a Waymo?\u00a0"
  },
  {
    "start": 6868.8,
    "duration": 5.68,
    "text": "There are going to be equivalents of all that.\nSo I feel like self-driving is a very nice\u00a0\u00a0"
  },
  {
    "start": 6874.48,
    "duration": 3.52,
    "text": "analogy that you can borrow things from.\nWhat is the equivalent of a cone in the car?\u00a0"
  },
  {
    "start": 6878.0,
    "duration": 7.28,
    "text": "What is the equivalent of a teleoperating worker\u00a0\nwho's hidden away and all the aspects of it.\u00a0"
  },
  {
    "start": 6885.28,
    "duration": 3.2,
    "text": "Do you have any opinions on what this\u00a0\nimplies about the current AI buildout,\u00a0\u00a0"
  },
  {
    "start": 6889.2,
    "duration": 6.96,
    "text": "which would 10x the amount of available compute\u00a0\nin the world in a year or two and maybe more\u00a0\u00a0"
  },
  {
    "start": 6896.16,
    "duration": 4.64,
    "text": "than 100x it by the end of the decade.\nIf the use of AI will be lower than\u00a0\u00a0"
  },
  {
    "start": 6900.8,
    "duration": 3.76,
    "text": "some people naively predict, does\u00a0\nthat mean that we're overbuilding\u00a0\u00a0"
  },
  {
    "start": 6904.56,
    "duration": 4.0,
    "text": "compute or is that a separate question?\nKind of like what happened with railroads.\u00a0"
  },
  {
    "start": 6908.56,
    "duration": 2.32,
    "text": "With what, sorry?\nWas it railroads or?\u00a0"
  },
  {
    "start": 6910.88,
    "duration": 3.2,
    "text": "Yeah, it was.\nYeah. There's historical precedent.\u00a0\u00a0"
  },
  {
    "start": 6914.08,
    "duration": 4.4,
    "text": "Or was it with the telecommunication industry?\nPre-paving the internet that only came a decade\u00a0\u00a0"
  },
  {
    "start": 6918.48,
    "duration": 5.52,
    "text": "later and creating a whole bubble in the\u00a0\ntelecommunications industry in the late '90s.\u00a0"
  },
  {
    "start": 6928.0,
    "duration": 5.2,
    "text": "I understand I'm sounding very pessimistic\u00a0\nhere. I'm actually optimistic. I think this\u00a0\u00a0"
  },
  {
    "start": 6933.2,
    "duration": 3.28,
    "text": "will work. I think it's tractable. I'm\u00a0\nonly sounding pessimistic because when\u00a0\u00a0"
  },
  {
    "start": 6936.48,
    "duration": 3.92,
    "text": "I go on my Twitter timeline, I see all\u00a0\nthis stuff that makes no sense to me.\u00a0"
  },
  {
    "start": 6942.56,
    "duration": 4.8,
    "text": "There's a lot of reasons for why that exists.\nA lot of it is honestly just fundraising.\u00a0\u00a0"
  },
  {
    "start": 6947.36,
    "duration": 3.12,
    "text": "It's just incentive structures.\u00a0\nA lot of it may be fundraising.\u00a0"
  },
  {
    "start": 6950.48,
    "duration": 4.96,
    "text": "A lot of it is just attention, converting\u00a0\nattention to money on the internet,\u00a0\u00a0"
  },
  {
    "start": 6955.44,
    "duration": 4.8,
    "text": "stuff like that.\nThere's a lot of\u00a0\u00a0"
  },
  {
    "start": 6960.24,
    "duration": 5.68,
    "text": "that going on, and I'm only reacting to that.\nBut I'm still overall very bullish on technology.\u00a0"
  },
  {
    "start": 6965.92,
    "duration": 4.0,
    "text": "We're going to work through all this stuff.\nThere's been a rapid amount of progress.\u00a0"
  },
  {
    "start": 6969.92,
    "duration": 5.28,
    "text": "I don't know that there's overbuilding.\nI think we're going to be able to gobble up what,\u00a0\u00a0"
  },
  {
    "start": 6975.2,
    "duration": 5.04,
    "text": "in my understanding, is being built.\nFor example, Claude Code or OpenAI Codex\u00a0\u00a0"
  },
  {
    "start": 6980.24,
    "duration": 2.64,
    "text": "and stuff like that didn't even\u00a0\nexist a year ago. Is that right?\u00a0\u00a0"
  },
  {
    "start": 6984.96,
    "duration": 4.96,
    "text": "This is a miraculous technology that didn't exist.\nThere's going to be a huge amount of demand,\u00a0\u00a0"
  },
  {
    "start": 6989.92,
    "duration": 6.08,
    "text": "as we see the demand in ChatGPT already and so on.\nSo I don't know that there's overbuilding.\u00a0"
  },
  {
    "start": 6997.92,
    "duration": 4.4,
    "text": "I'm just reacting to some of the very fast\u00a0\ntimelines that people continue to say incorrectly.\u00a0"
  },
  {
    "start": 7002.32,
    "duration": 4.24,
    "text": "I've heard many, many times over the course\u00a0\nof my 15 years in AI where very reputable\u00a0\u00a0"
  },
  {
    "start": 7006.56,
    "duration": 6.88,
    "text": "people keep getting this wrong all the time.\nI want this to be properly calibrated, and some\u00a0\u00a0"
  },
  {
    "start": 7013.44,
    "duration": 5.68,
    "text": "of this also has geopolitical ramifications and\u00a0\nthings like that with some of these questions.\u00a0"
  },
  {
    "start": 7019.68,
    "duration": 3.52,
    "text": "I don't want people to make\u00a0\nmistakes in that sphere of things.\u00a0"
  },
  {
    "start": 7024.08,
    "duration": 3.68,
    "text": "I do want us to be grounded in the\u00a0\nreality of what technology is and isn't.\u00a0"
  },
  {
    "start": 7028.88,
    "duration": 6.96,
    "text": "Let's talk about education and Eureka.\nOne thing you could do is start another AI\u00a0\u00a0"
  },
  {
    "start": 7035.84,
    "duration": 5.28,
    "text": "lab and then try to solve those problems.\nI\u2019m curious what you're up to now,\u00a0\u00a0"
  },
  {
    "start": 7041.68,
    "duration": 4.64,
    "text": "and why not AI research itself?\nI guess the way I would put it\u00a0\u00a0"
  },
  {
    "start": 7046.32,
    "duration": 6.0,
    "text": "is I feel some amount of determinism\u00a0\naround the things that AI labs are doing.\u00a0"
  },
  {
    "start": 7053.36,
    "duration": 7.84,
    "text": "I feel like I could help out there, but I\u00a0\ndon't know that I would uniquely improve it.\u00a0"
  },
  {
    "start": 7062.24,
    "duration": 4.24,
    "text": "My personal big fear is that a lot of this\u00a0\nstuff happens on the side of humanity,\u00a0\u00a0"
  },
  {
    "start": 7066.48,
    "duration": 6.32,
    "text": "and that humanity gets disempowered by it.\nI care not just about all the Dyson spheres\u00a0\u00a0"
  },
  {
    "start": 7072.8,
    "duration": 2.64,
    "text": "that we're going to build and that AI is\u00a0\ngoing to build in a fully autonomous way,\u00a0\u00a0"
  },
  {
    "start": 7075.44,
    "duration": 5.04,
    "text": "I care about what happens to humans.\nI want humans to be well off in the future.\u00a0"
  },
  {
    "start": 7080.48,
    "duration": 3.6,
    "text": "I feel like that's where I can a\u00a0\nlot more uniquely add value than\u00a0\u00a0"
  },
  {
    "start": 7084.08,
    "duration": 7.12,
    "text": "an incremental improvement in the frontier lab.\nI'm most afraid of something depicted in movies\u00a0\u00a0"
  },
  {
    "start": 7091.2,
    "duration": 4.72,
    "text": "like WALL-E or Idiocracy or something like that,\u00a0\nwhere humanity is on the side of this stuff.\u00a0"
  },
  {
    "start": 7096.8,
    "duration": 3.12,
    "text": "I want humans to be much,\u00a0\nmuch better in this future.\u00a0"
  },
  {
    "start": 7101.12,
    "duration": 3.52,
    "text": "To me, this is through education\u00a0\nthat you can achieve this.\u00a0"
  },
  {
    "start": 7106.0,
    "duration": 4.56,
    "text": "So what are you working on there?\nThe easiest way I can describe it is\u00a0\u00a0"
  },
  {
    "start": 7110.56,
    "duration": 4.24,
    "text": "we're trying to build the Starfleet Academy.\nI don\u2019t know if you\u2019ve watched Star Trek.\u00a0"
  },
  {
    "start": 7114.8,
    "duration": 2.8,
    "text": "I haven\u2019t.\nStarfleet Academy is\u00a0\u00a0"
  },
  {
    "start": 7117.6,
    "duration": 6.24,
    "text": "this elite institution for frontier technology,\u00a0\nbuilding spaceships, and graduating cadets to be\u00a0\u00a0"
  },
  {
    "start": 7123.84,
    "duration": 4.08,
    "text": "the pilots of these spaceships and whatnot.\nSo I just imagine an elite institution for\u00a0\u00a0"
  },
  {
    "start": 7127.92,
    "duration": 8.56,
    "text": "technical knowledge and a kind of school that's\u00a0\nvery up-to-date and a premier institution.\u00a0"
  },
  {
    "start": 7136.48,
    "duration": 7.04,
    "text": "A category of questions I have for you is\u00a0\nexplaining how one teaches technical or\u00a0\u00a0"
  },
  {
    "start": 7143.52,
    "duration": 4.48,
    "text": "scientific content well, because you\u00a0\nare one of the world masters at it.\u00a0"
  },
  {
    "start": 7148.64,
    "duration": 3.12,
    "text": "I'm curious both about how you think about\u00a0\nit for content you've already put out there\u00a0\u00a0"
  },
  {
    "start": 7151.76,
    "duration": 4.32,
    "text": "on YouTube, but also, to the extent it's any\u00a0\ndifferent, how you think about it for Eureka.\u00a0"
  },
  {
    "start": 7156.88,
    "duration": 3.68,
    "text": "With respect to Eureka, one thing that\u00a0\nis very fascinating to me about education\u00a0\u00a0"
  },
  {
    "start": 7160.56,
    "duration": 4.4,
    "text": "is that I do think education will pretty\u00a0\nfundamentally change with AIs on the side.\u00a0"
  },
  {
    "start": 7164.96,
    "duration": 5.44,
    "text": "It has to be rewired and changed to some extent.\nI still think that we're pretty early.\u00a0"
  },
  {
    "start": 7170.4,
    "duration": 2.24,
    "text": "There's going to be a lot of people who\u00a0\nare going to try to do the obvious things.\u00a0"
  },
  {
    "start": 7173.44,
    "duration": 4.64,
    "text": "Have an LLM and ask it questions.\nDo all the basic things that you would\u00a0\u00a0"
  },
  {
    "start": 7178.08,
    "duration": 2.08,
    "text": "do via prompting right now.\nIt's helpful,\u00a0\u00a0"
  },
  {
    "start": 7180.16,
    "duration": 4.24,
    "text": "but it still feels to me a bit like slop.\nI'd like to do it properly, and I think the\u00a0\u00a0"
  },
  {
    "start": 7184.4,
    "duration": 6.16,
    "text": "capability is not there for what I would want.\nWhat I'd want is an actual tutor experience.\u00a0"
  },
  {
    "start": 7191.36,
    "duration": 5.76,
    "text": "A prominent example in my mind is I was\u00a0\nrecently learning Korean, so language learning.\u00a0"
  },
  {
    "start": 7197.12,
    "duration": 3.2,
    "text": "I went through a phase where I was\u00a0\nlearning Korean by myself on the internet.\u00a0"
  },
  {
    "start": 7200.32,
    "duration": 5.76,
    "text": "I went through a phase where I was part of a small\u00a0\nclass in Korea taking Korean with a bunch of other\u00a0\u00a0"
  },
  {
    "start": 7206.08,
    "duration": 2.0,
    "text": "people, which was really funny.\nWe had a teacher and 10\u00a0\u00a0"
  },
  {
    "start": 7208.08,
    "duration": 4.4,
    "text": "people or so taking Korean.\nThen I switched to a one-on-one tutor.\u00a0"
  },
  {
    "start": 7213.6,
    "duration": 6.0,
    "text": "I guess what was fascinating to me was, I think\u00a0\nI had a really good tutor, but just thinking\u00a0\u00a0"
  },
  {
    "start": 7219.6,
    "duration": 5.6,
    "text": "through what this tutor was doing for me and how\u00a0\nincredible that experience was and how high the\u00a0\u00a0"
  },
  {
    "start": 7225.2,
    "duration": 6.48,
    "text": "bar is for what I want to build eventually.\nInstantly from a very short\u00a0\u00a0"
  },
  {
    "start": 7231.68,
    "duration": 4.08,
    "text": "conversation, she understood where I am\u00a0\nas a student, what I know and don't know.\u00a0"
  },
  {
    "start": 7235.76,
    "duration": 5.28,
    "text": "She was able to probe exactly the kinds of\u00a0\nquestions or things to understand my world model.\u00a0"
  },
  {
    "start": 7241.04,
    "duration": 3.28,
    "text": "No LLM will do that for you\u00a0\n100% right now, not even close.\u00a0"
  },
  {
    "start": 7244.32,
    "duration": 5.04,
    "text": "But a tutor will do that if they're good.\nOnce she understands, she really served\u00a0\u00a0"
  },
  {
    "start": 7249.36,
    "duration": 3.44,
    "text": "me all the things that I needed at\u00a0\nmy current sliver of capability.\u00a0"
  },
  {
    "start": 7252.8,
    "duration": 4.08,
    "text": "I need to be always appropriately challenged.\nI can't be faced with something too hard or\u00a0\u00a0"
  },
  {
    "start": 7256.88,
    "duration": 3.92,
    "text": "too trivial, and a tutor is really good\u00a0\nat serving you just the right stuff.\u00a0"
  },
  {
    "start": 7261.68,
    "duration": 5.92,
    "text": "I felt like I was the only constraint to learning.\nI was always given the perfect information. I'm\u00a0\u00a0"
  },
  {
    "start": 7267.6,
    "duration": 3.52,
    "text": "the only constraint. I felt good because\u00a0\nI'm the only impediment that exists.\u00a0"
  },
  {
    "start": 7271.12,
    "duration": 3.12,
    "text": "It's not that I can't find knowledge or\u00a0\nthat it's not properly explained or etc.\u00a0"
  },
  {
    "start": 7274.24,
    "duration": 4.4,
    "text": "It's just my ability to memorize and so on.\nThis is what I want for people.\u00a0"
  },
  {
    "start": 7278.64,
    "duration": 2.56,
    "text": "How do you automate that?\nVery good question. At\u00a0\u00a0"
  },
  {
    "start": 7281.2,
    "duration": 5.76,
    "text": "the current capability, you don't.\nThat's why I think it's not actually the\u00a0\u00a0"
  },
  {
    "start": 7286.96,
    "duration": 4.4,
    "text": "right time to build this kind of an AI tutor.\nI still think it's a useful product,\u00a0\u00a0"
  },
  {
    "start": 7291.36,
    "duration": 7.12,
    "text": "and lots of people will build it, but the bar\u00a0\nis so high and the capability is not there.\u00a0"
  },
  {
    "start": 7300.4,
    "duration": 5.04,
    "text": "Even today, I would say ChatGPT is an\u00a0\nextremely valuable educational product.\u00a0"
  },
  {
    "start": 7305.44,
    "duration": 3.2,
    "text": "But for me, it was so fascinating\u00a0\nto see how high the bar is.\u00a0"
  },
  {
    "start": 7308.64,
    "duration": 4.4,
    "text": "When I was with her, I almost felt\u00a0\nlike there's no way I can build this.\u00a0"
  },
  {
    "start": 7313.04,
    "duration": 2.16,
    "text": "But you are building it, right?\nAnyone who's had a really good\u00a0\u00a0"
  },
  {
    "start": 7315.2,
    "duration": 6.24,
    "text": "tutor is like, \"How are you going to build\u00a0\nthis?\" I'm waiting for that capability. I\u00a0\u00a0"
  },
  {
    "start": 7324.4,
    "duration": 4.64,
    "text": "did some AI consulting for computer vision.\nA lot of times, the value that I brought to\u00a0\u00a0"
  },
  {
    "start": 7329.04,
    "duration": 4.48,
    "text": "the company was telling them not to use AI.\nI was the AI expert, and they described the\u00a0\u00a0"
  },
  {
    "start": 7333.52,
    "duration": 4.16,
    "text": "problem, and I said, \"Don't use AI.\"\u00a0\nThis is my value add. I feel like it's\u00a0\u00a0"
  },
  {
    "start": 7337.68,
    "duration": 3.92,
    "text": "the same in education right now, where\u00a0\nI feel like for what I have in mind,\u00a0\u00a0"
  },
  {
    "start": 7341.6,
    "duration": 4.56,
    "text": "it's not yet the time, but the time will come.\nFor now, I'm building something that looks\u00a0\u00a0"
  },
  {
    "start": 7346.16,
    "duration": 4.16,
    "text": "maybe a bit more conventional that has a\u00a0\nphysical and digital component and so on.\u00a0"
  },
  {
    "start": 7350.32,
    "duration": 4.72,
    "text": "But it's obvious how this\u00a0\nshould look in the future.\u00a0"
  },
  {
    "start": 7355.04,
    "duration": 2.72,
    "text": "To the extent you're willing to\u00a0\nsay, what is the thing you hope\u00a0\u00a0"
  },
  {
    "start": 7357.76,
    "duration": 5.52,
    "text": "will be released this year or next year?\nI'm building the first course. I want to\u00a0\u00a0"
  },
  {
    "start": 7363.28,
    "duration": 4.72,
    "text": "have a really, really good course, the\u00a0\nobvious state-of-the-art destination\u00a0\u00a0"
  },
  {
    "start": 7368.0,
    "duration": 3.44,
    "text": "you go to to learn, AI in this case.\nThat's just what I'm familiar with, so it's\u00a0\u00a0"
  },
  {
    "start": 7371.44,
    "duration": 4.64,
    "text": "a really good first product to get to be really\u00a0\ngood at it. So that's what I'm building. Nanochat,\u00a0\u00a0"
  },
  {
    "start": 7376.08,
    "duration": 4.64,
    "text": "which you briefly mentioned, is a capstone project\u00a0\nof LLM101N, which is a class that I'm building.\u00a0"
  },
  {
    "start": 7382.48,
    "duration": 2.24,
    "text": "That's a really big piece of it.\nBut now I have to build out a lot of\u00a0\u00a0"
  },
  {
    "start": 7384.72,
    "duration": 6.24,
    "text": "the intermediates, and then I have to hire a small\u00a0\nteam of TAs and so on and build the entire course.\u00a0"
  },
  {
    "start": 7391.52,
    "duration": 3.92,
    "text": "One more thing that I would say is that many\u00a0\ntimes, when people think about education,\u00a0\u00a0"
  },
  {
    "start": 7395.44,
    "duration": 4.8,
    "text": "they think more about what I would say is\u00a0\na softer component of diffusing knowledge.\u00a0"
  },
  {
    "start": 7401.76,
    "duration": 5.2,
    "text": "I have something very hard and technical in mind.\nIn my mind, education is the very difficult\u00a0\u00a0"
  },
  {
    "start": 7406.96,
    "duration": 6.48,
    "text": "technical process of building ramps to knowledge.\nIn my mind, nanochat is a ramp to\u00a0\u00a0"
  },
  {
    "start": 7413.44,
    "duration": 4.56,
    "text": "knowledge because it's very simple.\nIt's the super simplified full-stack thing.\u00a0"
  },
  {
    "start": 7418.0,
    "duration": 3.92,
    "text": "If you give this artifact to someone and they\u00a0\nlook through it, they're learning a ton of stuff.\u00a0"
  },
  {
    "start": 7423.76,
    "duration": 4.32,
    "text": "It's giving you a lot of what I call eurekas\u00a0\nper second, which is understanding per second.\u00a0"
  },
  {
    "start": 7428.08,
    "duration": 4.64,
    "text": "That's what I want, lots of eurekas per second.\nSo to me, this is a technical problem of\u00a0\u00a0"
  },
  {
    "start": 7432.72,
    "duration": 5.92,
    "text": "how do we build these ramps to knowledge.\nSo I almost think of Eureka as maybe not that\u00a0\u00a0"
  },
  {
    "start": 7438.64,
    "duration": 4.48,
    "text": "different from some of the frontier labs\u00a0\nor some of the work that's going on there.\u00a0"
  },
  {
    "start": 7443.12,
    "duration": 4.48,
    "text": "I want to figure out how to build these\u00a0\nramps very efficiently so that people are\u00a0\u00a0"
  },
  {
    "start": 7447.6,
    "duration": 5.92,
    "text": "never stuck and everything is always\u00a0\nnot too hard or not too trivial, and\u00a0\u00a0"
  },
  {
    "start": 7454.24,
    "duration": 4.72,
    "text": "you have just the right material to progress.\nYou're imagining in the short term that instead\u00a0\u00a0"
  },
  {
    "start": 7458.96,
    "duration": 5.68,
    "text": "of a tutor being able to probe your understanding,\u00a0\nif you have enough self-awareness to be able to\u00a0\u00a0"
  },
  {
    "start": 7464.64,
    "duration": 5.04,
    "text": "probe yourself, you're never going to be stuck.\nYou can find the right answer between talking\u00a0\u00a0"
  },
  {
    "start": 7469.68,
    "duration": 3.76,
    "text": "to the TA or talking to an LLM and\u00a0\nlooking at the reference implementation.\u00a0"
  },
  {
    "start": 7473.44,
    "duration": 5.44,
    "text": "It sounds like automation or\u00a0\nAI is not a significant part.\u00a0"
  },
  {
    "start": 7478.88,
    "duration": 7.6,
    "text": "So far, the big alpha here is your\u00a0\nability to explain AI codified\u00a0\u00a0"
  },
  {
    "start": 7486.48,
    "duration": 4.88,
    "text": "in the source material of the class.\nThat's fundamentally what the course is.\u00a0"
  },
  {
    "start": 7491.36,
    "duration": 3.76,
    "text": "You always have to be calibrated to\u00a0\nwhat capability exists in the industry.\u00a0"
  },
  {
    "start": 7495.92,
    "duration": 3.36,
    "text": "A lot of people are going to\u00a0\npursue just asking ChatGPT, etc.\u00a0"
  },
  {
    "start": 7499.28,
    "duration": 4.56,
    "text": "But I think right now, for example, if you go to\u00a0\nChatGPT and you say, teach me AI, there's no way.\u00a0"
  },
  {
    "start": 7503.84,
    "duration": 5.28,
    "text": "It's going to give you some slop.\nAI is never going to write nanochat right now.\u00a0"
  },
  {
    "start": 7509.12,
    "duration": 3.12,
    "text": "But nanochat is a really\u00a0\nuseful intermediate point.\u00a0"
  },
  {
    "start": 7513.52,
    "duration": 2.24,
    "text": "I'm collaborating with AI\u00a0\nto create all this material,\u00a0\u00a0"
  },
  {
    "start": 7515.76,
    "duration": 5.28,
    "text": "so AI is still fundamentally very helpful.\nEarlier on, I built CS231n at Stanford,\u00a0\u00a0"
  },
  {
    "start": 7521.04,
    "duration": 5.28,
    "text": "which I think was the first deep learning\u00a0\nclass at Stanford, which became very popular.\u00a0"
  },
  {
    "start": 7527.68,
    "duration": 4.08,
    "text": "The difference in building out 231n\u00a0\nthen and LLM101N now is quite stark.\u00a0"
  },
  {
    "start": 7533.12,
    "duration": 4.72,
    "text": "I feel really empowered by the LLMs as they\u00a0\nexist right now, but I'm very much in the loop.\u00a0"
  },
  {
    "start": 7537.84,
    "duration": 3.12,
    "text": "They're helping me build the\u00a0\nmaterials, I go much faster.\u00a0"
  },
  {
    "start": 7540.96,
    "duration": 4.16,
    "text": "They're doing a lot of the boring stuff, etc.\nI feel like I'm developing the course much faster,\u00a0\u00a0"
  },
  {
    "start": 7545.12,
    "duration": 5.04,
    "text": "and it's LLM-infused, but it's not yet at a\u00a0\nplace where it can creatively create the content.\u00a0"
  },
  {
    "start": 7550.16,
    "duration": 3.28,
    "text": "I'm still there to do that.\nThe trickiness is always\u00a0\u00a0"
  },
  {
    "start": 7553.44,
    "duration": 3.6,
    "text": "calibrating yourself to what exists.\nWhen you imagine what is available\u00a0\u00a0"
  },
  {
    "start": 7557.04,
    "duration": 5.36,
    "text": "through Eureka in a couple of years, it\u00a0\nseems like the big bottleneck is going to be\u00a0\u00a0"
  },
  {
    "start": 7562.4,
    "duration": 6.0,
    "text": "finding Karpathys in field after field who can\u00a0\nconvert their understanding into these ramps.\u00a0"
  },
  {
    "start": 7569.44,
    "duration": 5.36,
    "text": "It would change over time. Right now,\u00a0\nit would be hiring faculty to help work\u00a0\u00a0"
  },
  {
    "start": 7574.8,
    "duration": 5.6,
    "text": "hand-in-hand with AI and a team of people\u00a0\nprobably to build state-of-the-art courses.\u00a0"
  },
  {
    "start": 7581.44,
    "duration": 6.72,
    "text": "Over time maybe some of the TAs can become AIs.\nYou just take all the course materials and then\u00a0\u00a0"
  },
  {
    "start": 7588.16,
    "duration": 4.96,
    "text": "I think you could serve a very good automated\u00a0\nTA for the student when they have more basic\u00a0\u00a0"
  },
  {
    "start": 7593.12,
    "duration": 2.96,
    "text": "questions or something like that.\nBut I think you'll need faculty\u00a0\u00a0"
  },
  {
    "start": 7596.08,
    "duration": 4.24,
    "text": "for the overall architecture of a\u00a0\ncourse and making sure that it fits.\u00a0"
  },
  {
    "start": 7600.32,
    "duration": 4.72,
    "text": "So I see a progression of how this will evolve.\nMaybe at some future point I'm not even that\u00a0\u00a0"
  },
  {
    "start": 7605.04,
    "duration": 2.72,
    "text": "useful and AI is doing most of the\u00a0\ndesign much better than I could.\u00a0"
  },
  {
    "start": 7607.76,
    "duration": 2.4,
    "text": "But I still think that's going\u00a0\nto take some time to play out.\u00a0"
  },
  {
    "start": 7610.88,
    "duration": 5.84,
    "text": "Are you imagining that people who have expertise\u00a0\nin other fields are then contributing courses,\u00a0\u00a0"
  },
  {
    "start": 7616.72,
    "duration": 4.4,
    "text": "or do you feel like it's quite\u00a0\nessential to the vision that you,\u00a0\u00a0"
  },
  {
    "start": 7621.12,
    "duration": 5.36,
    "text": "given your understanding of how you want to\u00a0\nteach, are the one designing the content?\u00a0"
  },
  {
    "start": 7627.12,
    "duration": 2.4,
    "text": "Sal Khan is narrating all\u00a0\nthe videos on Khan Academy.\u00a0"
  },
  {
    "start": 7629.52,
    "duration": 3.44,
    "text": "Are you imagining something like that?\nNo, I will hire faculty because there\u00a0\u00a0"
  },
  {
    "start": 7632.96,
    "duration": 5.68,
    "text": "are domains in which I'm not an expert.\nThat's the only way to offer the state-of-the-art\u00a0\u00a0"
  },
  {
    "start": 7638.64,
    "duration": 5.52,
    "text": "experience for the student ultimately.\nI do expect that I would hire faculty, but\u00a0\u00a0"
  },
  {
    "start": 7644.16,
    "duration": 5.76,
    "text": "I will probably stick around in AI for some time.\nI do have something more conventional in mind for\u00a0\u00a0"
  },
  {
    "start": 7649.92,
    "duration": 2.96,
    "text": "the current capability than what\u00a0\npeople would probably anticipate.\u00a0"
  },
  {
    "start": 7653.68,
    "duration": 4.96,
    "text": "When I'm building Starfleet Academy, I do probably\u00a0\nimagine a physical institution, and maybe a tier\u00a0\u00a0"
  },
  {
    "start": 7658.64,
    "duration": 5.52,
    "text": "below that a digital offering that is not the\u00a0\nstate-of-the-art experience you would get when\u00a0\u00a0"
  },
  {
    "start": 7664.16,
    "duration": 4.32,
    "text": "someone comes in physically full-time and we\u00a0\nwork through material from start to end and\u00a0\u00a0"
  },
  {
    "start": 7668.48,
    "duration": 5.2,
    "text": "make sure you understand it. That's the physical\u00a0\noffering. The digital offering is a bunch of stuff\u00a0\u00a0"
  },
  {
    "start": 7673.68,
    "duration": 4.0,
    "text": "on the internet and maybe some LLM assistant.\nIt's a bit more gimmicky in a tier below, but\u00a0\u00a0"
  },
  {
    "start": 7677.68,
    "duration": 7.28,
    "text": "at least it's accessible to 8 billion people.\nI think you're basically inventing college\u00a0\u00a0"
  },
  {
    "start": 7684.96,
    "duration": 6.72,
    "text": "from first principles for the tools that\u00a0\nare available today and just selecting\u00a0\u00a0"
  },
  {
    "start": 7691.68,
    "duration": 5.76,
    "text": "for people who have the motivation and the\u00a0\ninterest of really engaging with material.\u00a0"
  },
  {
    "start": 7698.08,
    "duration": 3.28,
    "text": "There's going to have to be a lot of not\u00a0\njust education but also re-education.\u00a0"
  },
  {
    "start": 7701.36,
    "duration": 4.64,
    "text": "I would love to help out there because\u00a0\nthe jobs will probably change quite a bit.\u00a0"
  },
  {
    "start": 7707.36,
    "duration": 2.48,
    "text": "For example, today a lot of people are\u00a0\ntrying to upskill in AI specifically.\u00a0"
  },
  {
    "start": 7709.84,
    "duration": 2.56,
    "text": "I think it's a really good\u00a0\ncourse to teach in this respect.\u00a0"
  },
  {
    "start": 7714.96,
    "duration": 6.16,
    "text": "Motivation-wise, before AGI motivation is very\u00a0\nsimple to solve because people want to make money.\u00a0"
  },
  {
    "start": 7721.12,
    "duration": 5.84,
    "text": "This is how you make money in the industry today.\nPost-AGI is a lot more interesting possibly\u00a0\u00a0"
  },
  {
    "start": 7726.96,
    "duration": 2.8,
    "text": "because if everything is automated\u00a0\nand there's nothing to do for anyone,\u00a0\u00a0"
  },
  {
    "start": 7729.76,
    "duration": 7.92,
    "text": "why would anyone go to a school?\nI often say that pre-AGI education\u00a0\u00a0"
  },
  {
    "start": 7737.68,
    "duration": 7.84,
    "text": "is useful. Post-AGI education is fun. In\u00a0\na similar way, people go to the gym today.\u00a0"
  },
  {
    "start": 7746.56,
    "duration": 3.84,
    "text": "We don't need their physical strength\u00a0\nto manipulate heavy objects because we\u00a0\u00a0"
  },
  {
    "start": 7750.4,
    "duration": 2.0,
    "text": "have machines that do that.\nThey still go to the gym.\u00a0"
  },
  {
    "start": 7752.4,
    "duration": 2.72,
    "text": "Why do they go to the gym?\nBecause it's fun, it's healthy,\u00a0\u00a0"
  },
  {
    "start": 7756.24,
    "duration": 7.76,
    "text": "and you look hot when you have a six-pack.\nIt's attractive for people to do that\u00a0\u00a0"
  },
  {
    "start": 7764.0,
    "duration": 4.88,
    "text": "in a very deep, psychological,\u00a0\nevolutionary sense for humanity.\u00a0"
  },
  {
    "start": 7770.32,
    "duration": 3.44,
    "text": "Education will play out in the same way.\nYou'll go to school like you go to the gym.\u00a0"
  },
  {
    "start": 7776.0,
    "duration": 4.72,
    "text": "Right now, not that many people learn\u00a0\nbecause learning is hard. You bounce\u00a0\u00a0"
  },
  {
    "start": 7780.72,
    "duration": 4.0,
    "text": "from material. Some people overcome that\u00a0\nbarrier, but for most people, it's hard.\u00a0"
  },
  {
    "start": 7786.0,
    "duration": 4.32,
    "text": "It's a technical problem to solve.\nIt's a technical problem to do what my tutor\u00a0\u00a0"
  },
  {
    "start": 7790.32,
    "duration": 3.12,
    "text": "did for me when I was learning Korean.\nIt's tractable and buildable,\u00a0\u00a0"
  },
  {
    "start": 7793.44,
    "duration": 2.32,
    "text": "and someone should build it.\nIt's going to make learning\u00a0\u00a0"
  },
  {
    "start": 7795.76,
    "duration": 4.72,
    "text": "anything trivial and desirable, and people\u00a0\nwill do it for fun because it's trivial.\u00a0"
  },
  {
    "start": 7800.48,
    "duration": 4.96,
    "text": "If I had a tutor like that for any arbitrary piece\u00a0\nof knowledge, it's going to be so much easier to\u00a0\u00a0"
  },
  {
    "start": 7805.44,
    "duration": 2.16,
    "text": "learn anything, and people will do it.\nThey'll do it for the same\u00a0\u00a0"
  },
  {
    "start": 7807.6,
    "duration": 6.0,
    "text": "reasons they go to the gym.\nThat sounds different from using\u2026\u00a0\u00a0"
  },
  {
    "start": 7814.16,
    "duration": 7.12,
    "text": "So post-AGI, you're using this as\u00a0\nentertainment or as self-betterment.\u00a0"
  },
  {
    "start": 7821.28,
    "duration": 3.92,
    "text": "But it sounded like you had a vision\u00a0\nalso that this education is relevant to\u00a0\u00a0"
  },
  {
    "start": 7825.2,
    "duration": 5.6,
    "text": "keeping humanity in control of AI. That sounds\u00a0\ndifferent. Is it entertaining for some people,\u00a0\u00a0"
  },
  {
    "start": 7830.8,
    "duration": 2.0,
    "text": "but then empowerment for some others?\nHow do you think about that?\u00a0"
  },
  {
    "start": 7832.8,
    "duration": 7.28,
    "text": "I do think eventually it's a bit of\u00a0\na losing game, if that makes sense.\u00a0"
  },
  {
    "start": 7841.44,
    "duration": 2.72,
    "text": "It is in the long term.\nIn the long term, which\u00a0\u00a0"
  },
  {
    "start": 7844.16,
    "duration": 3.36,
    "text": "is longer than maybe most people in the\u00a0\nindustry think about, it's a losing game.\u00a0"
  },
  {
    "start": 7847.52,
    "duration": 5.92,
    "text": "I do think people can go so far and we've barely\u00a0\nscratched the surface of how much a person can go.\u00a0"
  },
  {
    "start": 7853.44,
    "duration": 3.2,
    "text": "That's just because people are bouncing off\u00a0\nof material that's too easy or too hard.\u00a0"
  },
  {
    "start": 7859.28,
    "duration": 3.76,
    "text": "People will be able to go much further.\nAnyone will speak five languages because\u00a0\u00a0"
  },
  {
    "start": 7863.04,
    "duration": 6.64,
    "text": "why not? Because it's so trivial. Anyone will know\u00a0\nall the basic curriculum of undergrad, et cetera.\u00a0"
  },
  {
    "start": 7869.68,
    "duration": 3.52,
    "text": "Now that I'm understanding the\u00a0\nvision, that's very interesting.\u00a0"
  },
  {
    "start": 7874.08,
    "duration": 3.92,
    "text": "It has a perfect analog in gym culture.\nI don't think 100 years\u00a0\u00a0"
  },
  {
    "start": 7878.0,
    "duration": 4.72,
    "text": "ago anybody would be ripped.\nNobody would have been able to just spontaneously\u00a0\u00a0"
  },
  {
    "start": 7882.72,
    "duration": 6.48,
    "text": "bench two plates or three plates or something.\nIt's very common now because of this idea of\u00a0\u00a0"
  },
  {
    "start": 7889.2,
    "duration": 4.32,
    "text": "systematically training and lifting weights in\u00a0\nthe gym, or systematically training to be able\u00a0\u00a0"
  },
  {
    "start": 7893.52,
    "duration": 4.96,
    "text": "to run a marathon, which is a capability\u00a0\nmost humans would not spontaneously have.\u00a0"
  },
  {
    "start": 7898.48,
    "duration": 5.04,
    "text": "You're imagining similar things for\u00a0\nlearning across many different domains,\u00a0\u00a0"
  },
  {
    "start": 7903.52,
    "duration": 4.56,
    "text": "much more intensely, deeply, faster.\nExactly. I am betting a bit implicitly\u00a0\u00a0"
  },
  {
    "start": 7908.08,
    "duration": 8.48,
    "text": "on some of the timelessness of human nature.\nIt will be desirable to do all these things,\u00a0\u00a0"
  },
  {
    "start": 7918.24,
    "duration": 3.84,
    "text": "and I think people will look up\u00a0\nto it as they have for millennia.\u00a0"
  },
  {
    "start": 7923.6,
    "duration": 3.76,
    "text": "This will continue to be true.\nThere's some evidence of that historically.\u00a0"
  },
  {
    "start": 7927.36,
    "duration": 3.92,
    "text": "If you look at, for example, aristocrats, or you\u00a0\nlook at ancient Greece or something like that,\u00a0\u00a0"
  },
  {
    "start": 7931.28,
    "duration": 4.08,
    "text": "whenever you had little pocket environments\u00a0\nthat were post-AGI in a certain sense, people\u00a0\u00a0"
  },
  {
    "start": 7935.36,
    "duration": 5.36,
    "text": "have spent a lot of their time flourishing in a\u00a0\ncertain way, either physically or cognitively.\u00a0"
  },
  {
    "start": 7942.16,
    "duration": 6.0,
    "text": "I feel okay about the prospects of that.\nIf this is false and I'm wrong and we end up in a\u00a0\u00a0"
  },
  {
    "start": 7949.12,
    "duration": 5.92,
    "text": "WALL-E or Idiocracy future, then I don't even care\u00a0\nif there are Dyson spheres. This is a terrible\u00a0\u00a0"
  },
  {
    "start": 7955.04,
    "duration": 6.32,
    "text": "outcome. I really do care about humanity.\nEveryone has to just be\u00a0\u00a0"
  },
  {
    "start": 7961.36,
    "duration": 5.2,
    "text": "superhuman in a certain sense.\nIt's still a world in which that is not enabling\u00a0\u00a0"
  },
  {
    "start": 7966.56,
    "duration": 5.04,
    "text": "us to\u2026 It's like the culture world, right?\nYou're not fundamentally going to be able\u00a0\u00a0"
  },
  {
    "start": 7971.6,
    "duration": 6.08,
    "text": "to transform the trajectory\u00a0\nof technology or influence\u00a0\u00a0"
  },
  {
    "start": 7977.68,
    "duration": 5.44,
    "text": "decisions by your own labor or cognition alone.\nMaybe you can influence decisions because the AI\u00a0\u00a0"
  },
  {
    "start": 7983.12,
    "duration": 6.96,
    "text": "is asking for your approval, but it's not because\u00a0\nI've invented something or I've come up with a new\u00a0\u00a0"
  },
  {
    "start": 7990.08,
    "duration": 5.84,
    "text": "design that I'm really influencing the future.\nMaybe. I think there will be a transitional\u00a0\u00a0"
  },
  {
    "start": 7995.92,
    "duration": 3.2,
    "text": "period where we are going to be\u00a0\nable to be in the loop and advance\u00a0\u00a0"
  },
  {
    "start": 7999.12,
    "duration": 4.32,
    "text": "things if we understand a lot of stuff.\nIn the long-term, that probably goes away.\u00a0"
  },
  {
    "start": 8005.04,
    "duration": 3.28,
    "text": "It might even become a sport.\nRight now you have powerlifters\u00a0\u00a0"
  },
  {
    "start": 8008.32,
    "duration": 5.2,
    "text": "who go extreme in this direction.\nWhat is powerlifting in a cognitive era?\u00a0"
  },
  {
    "start": 8013.52,
    "duration": 3.28,
    "text": "Maybe it's people who are really trying\u00a0\nto make Olympics out of knowing stuff.\u00a0"
  },
  {
    "start": 8019.6,
    "duration": 4.0,
    "text": "If you have a perfect AI tutor,\u00a0\nmaybe you can get extremely far.\u00a0"
  },
  {
    "start": 8023.6,
    "duration": 4.88,
    "text": "I feel that the geniuses of\u00a0\ntoday are barely scratching the\u00a0\u00a0"
  },
  {
    "start": 8028.48,
    "duration": 7.36,
    "text": "surface of what a human mind can do, I think.\nI love this vision. I also feel like the person\u00a0\u00a0"
  },
  {
    "start": 8035.84,
    "duration": 6.4,
    "text": "you have the most product-market fit with is me\u00a0\nbecause my job involves having to learn different\u00a0\u00a0"
  },
  {
    "start": 8042.24,
    "duration": 8.56,
    "text": "subjects every week, and I am very excited.\nI'm similar, for that matter. A lot of people,\u00a0\u00a0"
  },
  {
    "start": 8050.8,
    "duration": 4.24,
    "text": "for example, hate school and want to get\u00a0\nout of it. I really liked school. I loved\u00a0\u00a0"
  },
  {
    "start": 8055.04,
    "duration": 1.76,
    "text": "learning things, et cetera.\nI wanted to stay in school.\u00a0"
  },
  {
    "start": 8056.8,
    "duration": 2.64,
    "text": "I stayed all the way until Ph.D. and\u00a0\nthen they wouldn't let me stay longer,\u00a0\u00a0"
  },
  {
    "start": 8059.44,
    "duration": 5.6,
    "text": "so I went to the industry.\nRoughly speaking, I love learning,\u00a0\u00a0"
  },
  {
    "start": 8065.04,
    "duration": 4.16,
    "text": "even for the sake of learning, but I also love\u00a0\nlearning because it's a form of empowerment and\u00a0\u00a0"
  },
  {
    "start": 8069.2,
    "duration": 2.8,
    "text": "being useful and productive.\nYou also made a point that\u00a0\u00a0"
  },
  {
    "start": 8072.8,
    "duration": 3.84,
    "text": "was subtle and I want to spell it out.\nWith what\u2019s happened so far with online\u00a0\u00a0"
  },
  {
    "start": 8076.64,
    "duration": 7.52,
    "text": "courses, why haven't they already enabled us to\u00a0\nenable every single human to know everything?\u00a0"
  },
  {
    "start": 8084.72,
    "duration": 6.64,
    "text": "They're just so motivation-laden because there are\u00a0\nno obvious on-ramps and it's so easy to get stuck.\u00a0"
  },
  {
    "start": 8092.64,
    "duration": 7.36,
    "text": "If you had this thing instead\u2014like a really\u00a0\ngood human tutor\u2014it would just be such an\u00a0\u00a0"
  },
  {
    "start": 8100.0,
    "duration": 4.24,
    "text": "unlock from a motivation perspective.\nI think so. It feels bad to bounce from\u00a0\u00a0"
  },
  {
    "start": 8104.24,
    "duration": 5.28,
    "text": "material. It feels bad. You get negative reward\u00a0\nfrom sinking an amount of time in something and it\u00a0\u00a0"
  },
  {
    "start": 8109.52,
    "duration": 4.56,
    "text": "doesn't pan out, or being completely bored because\u00a0\nwhat you're getting is too easy or too hard.\u00a0"
  },
  {
    "start": 8116.16,
    "duration": 4.48,
    "text": "When you do it properly, learning feels good.\nIt's a technical problem to get there.\u00a0"
  },
  {
    "start": 8121.92,
    "duration": 4.0,
    "text": "For a while, it's going to be AI plus human\u00a0\ncollab, and at some point, maybe it's just AI.\u00a0"
  },
  {
    "start": 8127.12,
    "duration": 5.76,
    "text": "Can I ask some questions about teaching well?\nIf you had to give advice to another educator\u00a0\u00a0"
  },
  {
    "start": 8132.88,
    "duration": 6.8,
    "text": "in another field that you're curious about to\u00a0\nmake the kinds of YouTube tutorials you've made.\u00a0"
  },
  {
    "start": 8140.4,
    "duration": 2.72,
    "text": "Maybe it might be especially interesting\u00a0\nto talk about domains where you can't\u00a0\u00a0"
  },
  {
    "start": 8143.92,
    "duration": 3.68,
    "text": "test someone's technical understanding by\u00a0\nhaving them code something up or something.\u00a0"
  },
  {
    "start": 8147.6,
    "duration": 6.88,
    "text": "What advice would you give them?\nThat's a pretty broad topic. There are 10\u201320 tips\u00a0\u00a0"
  },
  {
    "start": 8154.48,
    "duration": 8.88,
    "text": "and tricks that I semi-consciously do probably.\nBut a lot of this comes\u00a0\u00a0"
  },
  {
    "start": 8163.36,
    "duration": 2.72,
    "text": "from my physics background.\nI really, really did enjoy my physics background.\u00a0"
  },
  {
    "start": 8166.08,
    "duration": 4.88,
    "text": "I have a whole rant on how everyone\u00a0\nshould learn physics in early school\u00a0\u00a0"
  },
  {
    "start": 8170.96,
    "duration": 4.88,
    "text": "education because early school education is\u00a0\nnot about accumulating knowledge or memory\u00a0\u00a0"
  },
  {
    "start": 8175.84,
    "duration": 3.12,
    "text": "for tasks later in the industry.\nIt's about booting up a brain.\u00a0"
  },
  {
    "start": 8178.96,
    "duration": 4.0,
    "text": "Physics uniquely boots up the brain the\u00a0\nbest because some of the things that they\u00a0\u00a0"
  },
  {
    "start": 8182.96,
    "duration": 3.52,
    "text": "get you to do in your brain during\u00a0\nphysics is extremely valuable later.\u00a0"
  },
  {
    "start": 8186.48,
    "duration": 4.88,
    "text": "The idea of building models and abstractions\u00a0\nand understanding that there's a first-order\u00a0\u00a0"
  },
  {
    "start": 8191.36,
    "duration": 3.44,
    "text": "approximation that describes most of the system,\u00a0\nbut then there're second-order, third-order,\u00a0\u00a0"
  },
  {
    "start": 8194.8,
    "duration": 4.48,
    "text": "fourth-order terms that may or may not be present.\nThe idea that you're observing a very noisy\u00a0\u00a0"
  },
  {
    "start": 8199.28,
    "duration": 4.0,
    "text": "system, but there are these fundamental\u00a0\nfrequencies that you can abstract away.\u00a0"
  },
  {
    "start": 8203.28,
    "duration": 4.479,
    "text": "When a physicist walks into the class and\u00a0\nthey say, \"Assume there's a spherical cow,\"\u00a0\u00a0"
  },
  {
    "start": 8208.72,
    "duration": 4.32,
    "text": "everyone laughs at that, but this is brilliant.\nIt's brilliant thinking that's very generalizable\u00a0\u00a0"
  },
  {
    "start": 8213.04,
    "duration": 5.04,
    "text": "across the industry because a cow can be\u00a0\napproximated as a sphere in a bunch of ways.\u00a0"
  },
  {
    "start": 8218.8,
    "duration": 5.44,
    "text": "There's a really good book, for example, Scale.\nIt's from a physicist talking about biology.\u00a0"
  },
  {
    "start": 8224.24,
    "duration": 1.76,
    "text": "Maybe this is also a book\u00a0\nI would recommend reading.\u00a0"
  },
  {
    "start": 8226.56,
    "duration": 4.8,
    "text": "You can get a lot of really interesting\u00a0\napproximations and chart scaling laws of animals.\u00a0"
  },
  {
    "start": 8231.359,
    "duration": 4.08,
    "text": "You can look at their heartbeats and\u00a0\nthings like that, and they line up with\u00a0\u00a0"
  },
  {
    "start": 8235.439,
    "duration": 3.521,
    "text": "the size of the animal and things like that.\nYou can talk about an animal as a volume.\u00a0"
  },
  {
    "start": 8240.88,
    "duration": 4.88,
    "text": "You can talk about the heat dissipation of that,\u00a0\nbecause your heat dissipation grows as the surface\u00a0\u00a0"
  },
  {
    "start": 8245.76,
    "duration": 4.08,
    "text": "area, which is growing as a square.\nBut your heat creation or generation\u00a0\u00a0"
  },
  {
    "start": 8249.84,
    "duration": 3.28,
    "text": "is growing as a cube.\nSo I just feel like physicists\u00a0\u00a0"
  },
  {
    "start": 8253.12,
    "duration": 2.96,
    "text": "have all the right cognitive tools to\u00a0\napproach problem solving in the world.\u00a0"
  },
  {
    "start": 8256.08,
    "duration": 3.2,
    "text": "So because of that training, I\u00a0\nalways try to find the first-order\u00a0\u00a0"
  },
  {
    "start": 8259.28,
    "duration": 4.159,
    "text": "terms or the second-order terms of everything.\nWhen I'm observing a system or a thing, I have a\u00a0\u00a0"
  },
  {
    "start": 8263.439,
    "duration": 5.521,
    "text": "tangle of a web of ideas or knowledge in my mind.\nI'm trying to find, what is the thing that\u00a0\u00a0"
  },
  {
    "start": 8268.96,
    "duration": 3.6,
    "text": "matters? What is the first-order component?\u00a0\nHow can I simplify it? How can I have a\u00a0\u00a0"
  },
  {
    "start": 8272.56,
    "duration": 4.721,
    "text": "simplest thing that shows that thing, shows it in\u00a0\naction, and then I can tack on the other terms?\u00a0"
  },
  {
    "start": 8278.319,
    "duration": 4.721,
    "text": "Maybe an example from one of my repos that I\u00a0\nthink illustrates it well is called micrograd.\u00a0"
  },
  {
    "start": 8283.04,
    "duration": 3.68,
    "text": "I don't know if you're familiar with this.\nSo micrograd is 100 lines of code\u00a0\u00a0"
  },
  {
    "start": 8286.72,
    "duration": 3.6,
    "text": "that shows backpropagation.\nYou can create neural networks\u00a0\u00a0"
  },
  {
    "start": 8290.319,
    "duration": 4.561,
    "text": "out of simple operations like plus and times, et\u00a0\ncetera. Lego blocks of neural networks. You build\u00a0\u00a0"
  },
  {
    "start": 8294.88,
    "duration": 4.24,
    "text": "up a computational graph and you do a forward\u00a0\npass and a backward pass to get the gradients.\u00a0"
  },
  {
    "start": 8299.12,
    "duration": 2.64,
    "text": "Now, this is at the heart of\u00a0\nall neural network learning.\u00a0"
  },
  {
    "start": 8301.76,
    "duration": 3.6,
    "text": "So micrograd is a 100 lines of\u00a0\npretty interpretable Python code,\u00a0\u00a0"
  },
  {
    "start": 8305.359,
    "duration": 3.921,
    "text": "and it can do forward and backward arbitrary\u00a0\nneural networks, but not efficiently.\u00a0"
  },
  {
    "start": 8309.28,
    "duration": 3.439,
    "text": "So micrograd, these 100 lines of Python,\u00a0\nare everything you need to understand how\u00a0\u00a0"
  },
  {
    "start": 8312.72,
    "duration": 5.44,
    "text": "neural networks train. Everything else is just\u00a0\nefficiency. Everything else is efficiency. There's\u00a0\u00a0"
  },
  {
    "start": 8318.16,
    "duration": 3.199,
    "text": "a huge amount of work to get efficiency.\nYou need your tensors, you lay them out,\u00a0\u00a0"
  },
  {
    "start": 8321.359,
    "duration": 2.16,
    "text": "you stride them, you make sure\u00a0\nyour kernels, orchestrating\u00a0\u00a0"
  },
  {
    "start": 8323.52,
    "duration": 3.601,
    "text": "memory movement correctly, et cetera.\nIt's all just efficiency, roughly speaking.\u00a0"
  },
  {
    "start": 8327.12,
    "duration": 3.76,
    "text": "But the core intellectual piece of neural\u00a0\nnetwork training is micrograd. It's 100 lines.\u00a0\u00a0"
  },
  {
    "start": 8330.88,
    "duration": 4.479,
    "text": "You can easily understand it. It's a recursive\u00a0\napplication of chain rule to derive the gradient,\u00a0\u00a0"
  },
  {
    "start": 8335.359,
    "duration": 2.721,
    "text": "which allows you to optimize any\u00a0\narbitrary differentiable function.\u00a0"
  },
  {
    "start": 8338.08,
    "duration": 8.88,
    "text": "So I love finding these small-order terms and\u00a0\nserving them on a platter and discovering them.\u00a0"
  },
  {
    "start": 8346.96,
    "duration": 4.8,
    "text": "I feel like education is the most intellectually\u00a0\ninteresting thing because you have a tangle\u00a0\u00a0"
  },
  {
    "start": 8351.76,
    "duration": 5.12,
    "text": "of understanding and you're trying to lay\u00a0\nit out in a way that creates a ramp where\u00a0\u00a0"
  },
  {
    "start": 8356.88,
    "duration": 5.04,
    "text": "everything only depends on the thing before it.\nI find that this untangling of knowledge is just\u00a0\u00a0"
  },
  {
    "start": 8361.92,
    "duration": 5.28,
    "text": "so intellectually interesting as a cognitive task.\nI love doing it personally, but I just\u00a0\u00a0"
  },
  {
    "start": 8367.2,
    "duration": 4.0,
    "text": "have a fascination with trying to lay things\u00a0\nout in a certain way. Maybe that helps me.\u00a0"
  },
  {
    "start": 8371.2,
    "duration": 4.239,
    "text": "It also makes the learning\u00a0\nexperience so much more motivated.\u00a0"
  },
  {
    "start": 8375.439,
    "duration": 7.12,
    "text": "Your tutorial on the transformer begins\u00a0\nwith bigrams, literally a lookup table from,\u00a0\u00a0"
  },
  {
    "start": 8382.56,
    "duration": 4.0,
    "text": "\"Here's the word right now, or here's\u00a0\nthe previous word, here's the next word.\"\u00a0"
  },
  {
    "start": 8386.56,
    "duration": 2.4,
    "text": "It's literally just a lookup table.\nThat\u2019s the essence of it, yeah.\u00a0"
  },
  {
    "start": 8388.96,
    "duration": 4.64,
    "text": "It\u2019s such a brilliant way, starting with a\u00a0\nlookup table and then going to a transformer.\u00a0\u00a0"
  },
  {
    "start": 8393.6,
    "duration": 3.44,
    "text": "Each piece is motivated. Why would you add\u00a0\nthat? Why would you add the next thing?\u00a0"
  },
  {
    "start": 8397.04,
    "duration": 4.399,
    "text": "You could memorize the attention formula,\u00a0\nbut having an understanding of why every\u00a0\u00a0"
  },
  {
    "start": 8401.439,
    "duration": 4.561,
    "text": "single piece is relevant, what problem it solves.\nYou're presenting the pain before you present a\u00a0\u00a0"
  },
  {
    "start": 8406.0,
    "duration": 2.479,
    "text": "solution, and how clever is that?\nYou want to take the student\u00a0\u00a0"
  },
  {
    "start": 8408.479,
    "duration": 3.36,
    "text": "through that progression.\nThere are a lot of other small\u00a0\u00a0"
  },
  {
    "start": 8411.84,
    "duration": 5.76,
    "text": "things that make it nice and engaging and\u00a0\ninteresting. Always prompting the student.\u00a0\u00a0"
  },
  {
    "start": 8417.6,
    "duration": 4.4,
    "text": "There's a lot of small things like that are\u00a0\nimportant and a lot of good educators will do\u00a0\u00a0"
  },
  {
    "start": 8422.0,
    "duration": 5.68,
    "text": "this. How would you solve this? I'm not going to\u00a0\npresent the solution before you guess. That would\u00a0\u00a0"
  },
  {
    "start": 8427.68,
    "duration": 6.561,
    "text": "be wasteful. That's a little bit of a\u2026I don\u2019t\u00a0\nwant to swear but it\u2019s a dick move towards you\u00a0\u00a0"
  },
  {
    "start": 8434.24,
    "duration": 3.909,
    "text": "to present you with the solution before I give\u00a0\nyou a shot to try to come up with it yourself.\u00a0"
  },
  {
    "start": 8438.149,
    "duration": 8.49,
    "text": "Because if you try to come up with it yourself,\u00a0\nyou get a better understanding of what the action\u00a0\u00a0"
  },
  {
    "start": 8446.64,
    "duration": 5.84,
    "text": "space is, what the objective is, and then\u00a0\nwhy only this action fulfills that objective.\u00a0"
  },
  {
    "start": 8453.439,
    "duration": 4.48,
    "text": "You have a chance to try it yourself, and you\u00a0\nhave an appreciation when I give you the solution.\u00a0"
  },
  {
    "start": 8458.64,
    "duration": 2.72,
    "text": "It maximizes the amount of\u00a0\nknowledge per new fact added.\u00a0"
  },
  {
    "start": 8462.88,
    "duration": 7.92,
    "text": "Why do you think, by default, people who are\u00a0\ngenuine experts in their field are often bad\u00a0\u00a0"
  },
  {
    "start": 8470.8,
    "duration": 5.6,
    "text": "at explaining it to somebody ramping up?\nIt's the curse of knowledge and expertise.\u00a0"
  },
  {
    "start": 8476.399,
    "duration": 3.841,
    "text": "This is a real phenomenon, and I suffered\u00a0\nfrom it myself as much as I try not to.\u00a0"
  },
  {
    "start": 8481.04,
    "duration": 2.96,
    "text": "But you take certain things for granted,\u00a0\nand you can't put yourself in the shoes\u00a0\u00a0"
  },
  {
    "start": 8484.0,
    "duration": 4.08,
    "text": "of new people who are just starting out.\nThis is pervasive and happens to me as\u00a0\u00a0"
  },
  {
    "start": 8488.08,
    "duration": 4.16,
    "text": "well. One thing that's extremely helpful.\u00a0\nAs an example, someone was trying to show\u00a0\u00a0"
  },
  {
    "start": 8492.24,
    "duration": 4.88,
    "text": "me a paper in biology recently, and I just\u00a0\ninstantly had so many terrible questions.\u00a0"
  },
  {
    "start": 8498.08,
    "duration": 4.64,
    "text": "What I did was I used ChatGPT to ask the\u00a0\nquestions with the paper in the context window.\u00a0"
  },
  {
    "start": 8503.92,
    "duration": 3.84,
    "text": "It worked through some of the simple things.\nThen I shared the thread to the person who\u00a0\u00a0"
  },
  {
    "start": 8509.12,
    "duration": 5.68,
    "text": "wrote that paper or worked on that work.\nI felt like if they could see the dumb\u00a0\u00a0"
  },
  {
    "start": 8514.8,
    "duration": 2.88,
    "text": "questions I had, it might help\u00a0\nthem explain better in the future.\u00a0"
  },
  {
    "start": 8520.16,
    "duration": 4.72,
    "text": "For my material, I would love it if people\u00a0\nshared their dumb conversations with ChatGPT\u00a0\u00a0"
  },
  {
    "start": 8524.88,
    "duration": 2.8,
    "text": "about the stuff that I've created\u00a0\nbecause it really helps me put myself\u00a0\u00a0"
  },
  {
    "start": 8527.68,
    "duration": 7.2,
    "text": "again in the shoes of someone who's starting out.\nAnother trick that just works astoundingly well.\u00a0"
  },
  {
    "start": 8536.24,
    "duration": 8.96,
    "text": "If somebody writes a paper or a blog post or an\u00a0\nannouncement, it is in 100% of cases that just\u00a0\u00a0"
  },
  {
    "start": 8545.2,
    "duration": 7.92,
    "text": "the narration or the transcription of how they\u00a0\nwould explain it to you over lunch is way more,\u00a0\u00a0"
  },
  {
    "start": 8553.12,
    "duration": 6.16,
    "text": "not only understandable, but actually\u00a0\nalso more accurate and scientific,\u00a0\u00a0"
  },
  {
    "start": 8559.28,
    "duration": 4.8,
    "text": "in the sense that people have a bias\u00a0\nto explain things in the most abstract,\u00a0\u00a0"
  },
  {
    "start": 8564.88,
    "duration": 3.6,
    "text": "jargon-filled way possible and to clear\u00a0\ntheir throat for four paragraphs before\u00a0\u00a0"
  },
  {
    "start": 8568.479,
    "duration": 2.96,
    "text": "they explain the central idea.\nBut there's something about\u00a0\u00a0"
  },
  {
    "start": 8571.439,
    "duration": 6.24,
    "text": "communicating one-on-one with a person\u00a0\nwhich compels you to just say the thing.\u00a0"
  },
  {
    "start": 8577.68,
    "duration": 2.96,
    "text": "Just say the thing. I saw that\u00a0\ntweet, I thought it was really good.\u00a0"
  },
  {
    "start": 8580.64,
    "duration": 4.48,
    "text": "I shared it with a bunch of people.\nI noticed this many, many times.\u00a0"
  },
  {
    "start": 8586.16,
    "duration": 3.84,
    "text": "The most prominent example is that I\u00a0\nremember back in my PhD days doing research.\u00a0"
  },
  {
    "start": 8591.04,
    "duration": 3.52,
    "text": "You read someone's paper, and you\u00a0\nwork to understand what it's doing.\u00a0"
  },
  {
    "start": 8595.359,
    "duration": 3.28,
    "text": "Then you catch them, you're having beers\u00a0\nat the conference later, and you ask them,\u00a0\u00a0"
  },
  {
    "start": 8598.64,
    "duration": 4.561,
    "text": "\"So this paper, what were you doing? What is the\u00a0\npaper about?\" They will just tell you these three\u00a0\u00a0"
  },
  {
    "start": 8603.2,
    "duration": 3.52,
    "text": "sentences that perfectly captured the essence\u00a0\nof that paper and totally give you the idea.\u00a0"
  },
  {
    "start": 8606.72,
    "duration": 4.0,
    "text": "And you didn't have to read the paper.\nIt's only when you're sitting at the table\u00a0\u00a0"
  },
  {
    "start": 8610.72,
    "duration": 2.64,
    "text": "with a beer or something, and they're\u00a0\nlike, \"Oh yeah, the paper is just,\u00a0\u00a0"
  },
  {
    "start": 8613.359,
    "duration": 3.841,
    "text": "you take this idea, you take that idea and try\u00a0\nthis experiment and you try out this thing.\"\u00a0"
  },
  {
    "start": 8617.2,
    "duration": 3.6,
    "text": "They have a way of just putting it\u00a0\nconversationally just perfectly.\u00a0\u00a0"
  },
  {
    "start": 8620.8,
    "duration": 6.24,
    "text": "Why isn't that the abstract?\nExactly. This is coming from the\u00a0\u00a0"
  },
  {
    "start": 8627.04,
    "duration": 4.16,
    "text": "perspective of how somebody who's trying to\u00a0\nexplain an idea should formulate it better.\u00a0"
  },
  {
    "start": 8631.2,
    "duration": 5.84,
    "text": "What is your advice as a student to other\u00a0\nstudents, if you don't have a Karpathy\u00a0\u00a0"
  },
  {
    "start": 8637.04,
    "duration": 3.84,
    "text": "who is doing the exposition of an idea?\nIf you're reading a paper from somebody\u00a0\u00a0"
  },
  {
    "start": 8640.88,
    "duration": 6.24,
    "text": "or reading a book, what strategies do\u00a0\nyou employ to learn material you're\u00a0\u00a0"
  },
  {
    "start": 8647.12,
    "duration": 6.8,
    "text": "interested in in fields you're not an expert at?\nI don't know that I have unique tips and tricks,\u00a0\u00a0"
  },
  {
    "start": 8653.92,
    "duration": 11.12,
    "text": "to be honest. It's a painful process. One thing\u00a0\nthat has always helped me quite a bit is\u2014I\u00a0\u00a0"
  },
  {
    "start": 8666.8,
    "duration": 4.88,
    "text": "had a small tweet about this\u2014learning things\u00a0\non demand is pretty nice. Learning depth-wise.\u00a0\u00a0"
  },
  {
    "start": 8671.68,
    "duration": 3.44,
    "text": "I do feel you need a bit of alternation of\u00a0\nlearning depth-wise, on demand\u2014you're trying\u00a0\u00a0"
  },
  {
    "start": 8675.12,
    "duration": 3.359,
    "text": "to achieve a certain project that you're going\u00a0\nto get a reward from\u2014and learning breadth-wise,\u00a0\u00a0"
  },
  {
    "start": 8678.479,
    "duration": 3.84,
    "text": "which is just, \"Oh, let's do whatever 101,\u00a0\nand here's all the things you might need.\"\u00a0"
  },
  {
    "start": 8682.319,
    "duration": 3.04,
    "text": "Which is a lot of school\u2014does breadth-wise\u00a0\nlearning, like, \"Oh, trust me, you'll need\u00a0\u00a0"
  },
  {
    "start": 8685.359,
    "duration": 5.601,
    "text": "this later,\" that kind of stuff. Okay, I trust\u00a0\nyou. I'll learn it because I guess I need it.\u00a0"
  },
  {
    "start": 8690.96,
    "duration": 2.88,
    "text": "But I love the kind of learning\u00a0\nwhere you'll get a reward out of\u00a0\u00a0"
  },
  {
    "start": 8693.84,
    "duration": 4.16,
    "text": "doing something, and you're learning on demand.\nThe other thing that I've found extremely helpful.\u00a0"
  },
  {
    "start": 8699.84,
    "duration": 5.04,
    "text": "This is an aspect where education is a bit more\u00a0\nselfless, but explaining things to people is a\u00a0\u00a0"
  },
  {
    "start": 8704.88,
    "duration": 4.16,
    "text": "beautiful way to learn something more deeply.\nThis happens to me all the time.\u00a0"
  },
  {
    "start": 8709.04,
    "duration": 4.319,
    "text": "It probably happens to other people too because\u00a0\nI realize if I don't really understand something,\u00a0\u00a0"
  },
  {
    "start": 8713.359,
    "duration": 3.841,
    "text": "I can't explain it.\nI'm trying and I'm like,\u00a0\u00a0"
  },
  {
    "start": 8717.2,
    "duration": 4.08,
    "text": "\"Oh, I don't understand this.\"\nIt's so annoying to come to terms with that.\u00a0"
  },
  {
    "start": 8721.28,
    "duration": 4.079,
    "text": "You can go back and make sure you understood it.\nIt fills these gaps of your understanding.\u00a0"
  },
  {
    "start": 8725.359,
    "duration": 3.12,
    "text": "It forces you to come to terms\u00a0\nwith them and to reconcile them.\u00a0"
  },
  {
    "start": 8728.479,
    "duration": 4.801,
    "text": "I love to re-explain things and people\u00a0\nshould be doing that more as well.\u00a0"
  },
  {
    "start": 8733.28,
    "duration": 2.96,
    "text": "That forces you to manipulate the knowledge\u00a0\nand make sure that you know what you're\u00a0\u00a0"
  },
  {
    "start": 8736.24,
    "duration": 4.159,
    "text": "talking about when you're explaining it.\nThat's an excellent note to close on. Andrej,\u00a0\u00a0"
  },
  {
    "start": 8740.399,
    "duration": 1.761,
    "text": "that was great.\nThank you."
  }
]